---
layout: post
title: "(P0) QWen-VL"
date: 2024-05-22 10:05:04 +0800
labels: [qwen,llm,vlm]
---

## 动机、参考资料、涉及内容

- Qwen-VL 的模型结构及推理流程
- Qwen-VL 微调

## 使用

直接摘抄自 Qwen-VL 的 README

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)
model: "QWenLMHeadModel" = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat-int4",
    device_map="auto",
    trust_remote_code=True,
    fp16=True,
    offload_folder="./offload"
).eval()

# 1st dialogue turn
query: str = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},
    {'text': '这是什么'},
])
# query = 'Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\n这是什么'
response, history = model.chat(tokenizer, query=query, history=None)
print(response)  # 图中是一名年轻女子在沙滩上和她的狗玩耍，狗的品种可能是拉布拉多。她们坐在沙滩上，狗的前腿抬起来，似乎在和人类击掌。两人之间充满了信任和爱。
print(history)
# history:
# [
#     (
#         'Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\n这是什么',
#         '图中是一名年轻女子在沙滩上和她的狗玩耍，狗的品种可能是拉布拉多。她们坐在沙滩上，狗的前腿抬起来，似乎在和人类击掌。两人之间充满了信任和爱。'
#     ),
# ]

# 2nd dialogue turn
response, history = model.chat(tokenizer, '输出"击掌"的检测框', history=history)
print(response)  # <ref>击掌</ref><box>(517,508),(589,611)</box>
# 这里: (x1, y1), (x2, y2) = (517/1000*w, 508/1000*h), (589/1000*w, 611/1000*h)
# 坐标原点位于图像左上角, 水平方向为 x 轴, 竖直方向为 y 轴
# (x1, y1) 是左上角, (x2, y2) 是右下角, x1 表示图片左侧到矩形左边的距离, y1 表示图片上侧到矩形上边的距离

print(history)
# history
# [
#     (
#         'Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\n这是什么',
#         '图中是一名年轻女子在沙滩上和她的狗玩耍，狗的品种可能是拉布拉多。她们坐在沙滩上，狗的前腿抬起来，似乎在和人类击掌。两人之间充满了信任和爱。'
#     ),
#     (
#         '输出"击掌"的检测框',
#         '<ref>击掌</ref><box>(517,508),(589,611)</box>'
#     ),
# ]

image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('1.jpg')
else:
  print("no box")
```

关于坐标的进一步说明 (QWen-VL 的原始实现不是用 opencv 的, 经核验与下面的做法等价):

```python
import cv2
image = cv2.imread("demo.jpeg")
h, w = image.shape[:2]  # 1365, 2048
x1, y1, x2, y2 = 536, 509, 588, 602
x1, y1, x2, y2 = int(x1 / 1000 * w), int(y1 / 1000 * h), int(x2 / 1000 * w), int(y2 / 1000 * h)
image = cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255))
cv2.imwrite("box.jpeg", image)
```

下文的源码解析将围绕上面这个使用例子展开

## tokenizer

### `QWenTokenizer.__init__`: 特殊 token

**源码**

```python
PAT_STR = r"""(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"""
ENDOFTEXT = "<|endoftext|>"
IMSTART = "<|im_start|>"
IMEND = "<|im_end|>"
EXTRAS = tuple((f"<|extra_{i}|>" for i in range(205)))
SPECIAL_TOKENS = (
    ENDOFTEXT,
    IMSTART,
    IMEND,
) + EXTRAS

class QWenTokenizer(PreTrainedTokenizer):
    def __init__(
        ...,
        image_start_tag='<img>',
        image_end_tag='</img>',
        image_pad_tag='<imgpad>',
        ref_start_tag='<ref>',
        ref_end_tag='</ref>',
        box_start_tag='<box>',
        box_end_tag='</box>',
        quad_start_tag='<quad>',
        quad_end_tag='</quad>',
    ):
    # 这一部分将用于后续的 tokenize 部分作为 allowed_special
    # IMAGE_ST: image special tokens
    self.IMAGE_ST = (
        ref_start_tag, ref_end_tag,
        box_start_tag, box_end_tag,
        quad_start_tag, quad_end_tag,
        image_start_tag, image_end_tag,
        image_pad_tag
    )
    self.mergeable_ranks = _load_tiktoken_bpe(vocab_file)  # type: dict[bytes, int]
    self.special_tokens = {
        token: index
        for index, token in enumerate(
            SPECIAL_TOKENS + self.IMAGE_ST, start=len(self.mergeable_ranks)
        )
    }
    self.tokenizer = tiktoken.Encoding(
        "Qwen",
        pat_str=PAT_STR,
        mergeable_ranks=self.mergeable_ranks,
        special_tokens=self.special_tokens,
    )
```

**说明**

从源码中可以看出, Qwen-VL 的 tokenizer 采用的是 tiktoken, 其中普通字符一共 151643 个 (也就是 `qwen.tiktoken` 文件的行数), 特殊字符一共 217 (3+205+9) 个

### `QWenTokenizer.from_list_format`

**源码**

```python
class QWenTokenizer(PreTrainedTokenizer):
    def from_list_format(self, list_format: List[Dict]) -> str:
        text = ''
        num_images = 0
        for ele in list_format:
            if 'image' in ele:
                num_images += 1
                text += f'Picture {num_images}: '
                text += self.image_start_tag + ele['image'] + self.image_end_tag
                text += '\n'
            elif 'text' in ele:
                text += ele['text']
            elif 'box' in ele:
                if 'ref' in ele:
                    text += self.ref_start_tag + ele['ref'] + self.ref_end_tag
                for box in ele['box']:
                    text += self.box_start_tag + '(%d,%d),(%d,%d)' % (box[0], box[1], box[2], box[3]) + self.box_end_tag
            else:
                raise ValueError("Unsupport element: " + str(ele))
        return text
```

**说明**

可以看出 `list_format` 是一个列表, 每一项都是字典类型, 但其实只有这几种情况是有效的

```python
item_1 = {"image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"}
item_2 = {"text": "hello world"}
# 注意 box 是相对坐标, 也就是 (10/1000*w, 20/1000*h, 30/1000*w, 200/1000*h)
item_3 = {"box": [10, 100, 30, 200]}  # (x1, y1), (x2, y2) = (10, 100), (30, 200) 表示一个竖直方向较长, 水平方向较短的矩形
item_4 = {"box": [10, 100, 30, 200], "ref": "香蕉"}

# 对应的 string 如下
item_1_str = "Picture {i}: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\n"
item_2_str = "hello world"
item_3_str = "<box>(10, 100),(30,200)</box>"
item_4_str = "<ref>香蕉</ref><box>(10, 100),(30,200)</box>"

# 注意: query 是代表一次询问, 也就是说一次询问里可以包含多段文本, 多张图片
query = tokenizer.from_list_format(
    [item_1, item_2, item_3, item_4]
)
# query = "".join([item_1_str, item_2_str, item_3_str, item_4_str])
```

### `QWenLMHeadModel.chat`

```python
# class QWenPreTrainedModel(transformers.model_utils.PreTrainedModel)
class QWenLMHeadModel(QWenPreTrainedModel):
    def chat(
        self,
        tokenizer: PreTrainedTokenizer,
        query: str,
        history: Optional[HistoryType],
        system: str = "You are a helpful assistant.",  # 注意: 这里有个默认的 system prompt
        append_history: bool = True,
        stream: Optional[bool] = _SENTINEL,
        stop_words_ids: Optional[List[List[int]]] = None,
        generation_config: Optional[GenerationConfig] = None,
        **kwargs,
    ) -> Tuple[str, HistoryType]:
        generation_config = generation_config if generation_config is not None else self.generation_config

        # stream 参数不起作用, 如果需要启用流式, 则需要使用 QWenLMHeadModel.chat_stream
        assert stream is _SENTINEL, _ERROR_STREAM_IN_CHAT
        # Qwen 目前只支持 chat_format=="chatml"
        assert generation_config.chat_format == 'chatml', _ERROR_BAD_CHAT_FORMAT
        if history is None:
            history = []
        # stop_words_ids 是 generate 时的停止生成序列
        if stop_words_ids is None:
            stop_words_ids = []

        max_window_size = kwargs.get('max_window_size', None)
        if max_window_size is None:
            max_window_size = generation_config.max_window_size
        
        # make_context 函数是本节继续深入的部分
        raw_text, context_tokens = make_context(
            tokenizer,
            query,
            history=history,
            system=system,
            max_window_size=max_window_size,
            chat_format=generation_config.chat_format,
        )

        # 追加了一些默认的 stop_words_ids:
        # [[tokenizer.im_end_id], [tokenizer.im_start_id]]
        # 也就是 '<|im_end|>', '<|im_start|>' 对应的 token id
        stop_words_ids.extend(get_stop_words_ids(
            generation_config.chat_format, tokenizer
        ))

        input_ids = torch.tensor([context_tokens]).to(self.device)
        
        # ============== 模型推理流程 ==========
        outputs = self.generate(
                    input_ids,
                    stop_words_ids=stop_words_ids,
                    return_dict_in_generate=False,
                    generation_config=generation_config,
                    **kwargs,
                )
        # =====================================

        response = decode_tokens(
            outputs[0],
            tokenizer,
            raw_text_len=len(raw_text),
            context_length=len(context_tokens),
            chat_format=generation_config.chat_format,
            verbose=False,
            errors='replace'
        )

        if append_history:
            history.append((query, response))

        return response, history
```

本小节重点关注模型推理之前的 tokenizer 的过程, 因此需要继续深入 `make_context` 函数, 由于 `chat` 方法限制了 `chat_format='chatml'`, 所以下面关于 `make_context` 的源码就只关注 `chat_format='chatml'` 这一情况:

```python
def make_context(
    tokenizer: PreTrainedTokenizer,
    query: str,
    history: List[Tuple[str, str]] = None,
    system: str = "",
    max_window_size: int = 6144,  # 这个用于限制 system+history 占用的最大 token 数
    chat_format: str = "chatml",
):
    if history is None:
        history = []

    im_start, im_end = "<|im_start|>", "<|im_end|>"
    im_start_tokens = [tokenizer.im_start_id]
    im_end_tokens = [tokenizer.im_end_id]
    nl_tokens = tokenizer.encode("\n")

    # tokenizer.IMAGE_ST 就是与图片相关的特殊 token:
    # ["<img>", "</img>", "<ref>", "</ref>", "<box>", "</box>", "<imgpad>", "<quad>", "</quad>"]
    def _tokenize_str(role, content):
        return f"{role}\n{content}", tokenizer.encode(
            role, allowed_special=set(tokenizer.IMAGE_ST)
        ) + nl_tokens + tokenizer.encode(content, allowed_special=set(tokenizer.IMAGE_ST))

    system_text, system_tokens_part = _tokenize_str("system", system)
    system_tokens = im_start_tokens + system_tokens_part + im_end_tokens

    raw_text = ""
    context_tokens = []

    # 这里不要被 reversed 迷惑, 实际的 token 序列还是按照
    # system, turn-{i}, turn-{i+1}, ..., turn-{n}
    # 在历史对话合在一起都不超过最大长度的前提下 i=1, 否则会截取最近的 history
    for turn_query, turn_response in reversed(history):
        query_text, query_tokens_part = _tokenize_str("user", turn_query)
        query_tokens = im_start_tokens + query_tokens_part + im_end_tokens
        if turn_response is not None:
            response_text, response_tokens_part = _tokenize_str("assistant", turn_response)
            response_tokens = im_start_tokens + response_tokens_part + im_end_tokens
            next_context_tokens = nl_tokens + query_tokens + nl_tokens + response_tokens
            prev_chat = f"\n{im_start}{query_text}{im_end}\n{im_start}{response_text}{im_end}"
        else:
            next_context_tokens = nl_tokens + query_tokens + nl_tokens
            prev_chat = f"\n{im_start}{query_text}{im_end}\n"

        current_context_size = (
            len(system_tokens) + len(next_context_tokens) + len(context_tokens)
        )
        if current_context_size < max_window_size:
            context_tokens = next_context_tokens + context_tokens
            raw_text = prev_chat + raw_text
        else:
            break

    context_tokens = system_tokens + context_tokens
    raw_text = f"{im_start}{system_text}{im_end}" + raw_text
    context_tokens += (
        nl_tokens
        + im_start_tokens
        + _tokenize_str("user", query)[1]
        + im_end_tokens
        + nl_tokens
        + im_start_tokens
        + tokenizer.encode("assistant")
        + nl_tokens
    )
    raw_text += f"\n{im_start}user\n{query}{im_end}\n{im_start}assistant\n"

    return raw_text, context_tokens
```

`tokenizer.encode` 请参考下节

**TL;DR***, chatml 的格式是:

```python
query = "how about 2+2?"
history = [
    ("1+1=?", "1+1=2")
]
system = "you are a helpful assistant"

# 换行符显式给出
"""
<|im_start|><"system" token_ids>\nyou are a helpful assistant<|im_end|>
\n<|im_start|><"user" token_ids>\n1+1=?<|im_end|>\n
<|im_start|><"assistant" token_ids>\n1+1=2<|im_end|>
\n<|im_start|><"user" token_ids>\nhow about 2+2?<|im_end|>\n
<|im_start|><"assistant" token_ids>\n
"""


# 换行符隐式给出
"""
<|im_start|>system
you are a helpful assistant<|im_end|>
<|im_start|>user
1+1=?<|im_end|>
<|im_start|>assistant
1+1=2<|im_end|>
<|im_start|>user
how about 2+2<|im_end|>
<|im_start|>assistant

"""
```

因此, 可以做 token 数量的计算, 首先我们发现这些“特殊”字符串在 QWen-VL 的 tokenizer 里都是单个 token

```python
tokenizer.encode("system")    # [8948]
tokenizer.encode("assistant") # [77091]
tokenizer.encode("user")      # [872]
tokenizer.encode("\n")        # [198]

tokenizer.encode("ai")        # [2143]
tokenizer.encode("tool")      # [14172]
tokenizer.encode("function")  # [1688]
```

- 所以每句历史 (system/user/assistant) 都有 5 个 token 的 overhead (2 个 `\n`, 1 个角色,  1 个 `<|im_start|>` 以及 1 个 `<|im_end|>`)
- 用户问有 3 个 token 的 overhead, (1 个 `<|im_start|>`, 1 个 `"user"` 以及一个跟在 `"user"` 后面的 `\n`)


### `QWenTokenizer.tokenize`

根据前一节关于 `QWenLMHeadModel.chat` 的分析, 在将 `history` 和 `query` 转换为模型的输入时, 会经历 `tokenizer.encode` 的过程, 而 huggingface transformers 中的 `PreTrainedTokenizer` 的封装, `tokenizer.encode` 的实际流程是:

```python
def encode(self, ...):
    first_tokens = self.tokenize(text, **kwargs)
    # QWenTokenizer 对 convert_tokens_to_ids 的实现记为简单, 直接按字典查即可
    first_ids = self.convert_tokens_to_ids(first_tokens)
    # prepare_for_model 主要是做一些 padding 和 truncate, 以及补充一些特殊 token 的过程
    # 其中 padding 和 truncate 是标准化流程, 而 QWenTokenizer 没有重载补充特殊 token 的相关方法, 因此相当于只有 padding 和 truncate 的过程
    # 总之, prepare_for_model 在此处不予讨论
    return self.prepare_for_model(first_ids, second_ids, **kwargs)
```

总之, 只需关注 `QWenTokenizer.tokenize` 方法即可

**例子**

```python
img_url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"

# 仅用作对照
# 将<img>xxx</img> 以文本的方式 encode
token_ids = tokenizer.tokenizer.encode(
    f"Picture 1: <img>{img_url}</img>\n这是什么",
    allowed_special=set(["<img>", "</img>", "<ref>", "</ref>", "<box>", "</box>", "<imgpad>", "<quad>", "</quad>"])
)
tokens = tokenizer.convert_ids_to_tokens(token_ids)

# 这种方式是 QWenLMHeadModel 中的处理逻辑
# 将<img>xxx</img> 以图片的方式 encode
tokens_plus = tokenizer.tokenize(
    f"Picture 1: <img>{img_url}</img>\n这是什么",
    allowed_special=set(["<img>", "</img>", "<ref>", "</ref>", "<box>", "</box>", "<imgpad>", "<quad>", "</quad>"])
)
token_ids_plus = tokenizer.convert_tokens_to_ids(tokens_plus)

# 可以观察到两种方式对 <img>xxx</img> 中的 xxx 处理方式不一样
print(tokens)
print(tokens_plus)

# 验证其余部分完全一致
i, j = tokens.index("<img>"), tokens.index("</img>")
other_tokens = tokens[:i] + tokens[j+1:]

i_plus, j_plus = tokens_plus.index("<img>"), tokens_plus.index("</img>")
other_tokens_plus = tokens_plus[:i_plus] + tokens_plus[j_plus+1:]

print(all([a==b for a, b in zip(other_tokens, other_tokens_plus)]))  #  True
len(tokens_plus[i_plus+1:j_plus])  # 256, 一张图片转换为 256 个 token, 注意这 256 个 token 中可能有 <imgpad>
# 256 的含义实际上就是限制了 xxx 不能超过 256 个字节长度, 同时后续每张图片也占据这 256 个 token 位置? TODO: 确认此点

img_url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
bytes([x for x in token_ids_plus[i_plus+1:j_plus] if x != tokenizer.img_pad_id]).decode("utf-8") == img_url
```

**源码(TODO)**

```python
class QWenTokenizer(PreTrainedTokenizer)
    def tokenize(
        self,
        text: str,
        allowed_special: Union[Set, str] = "all",
        disallowed_special: Union[Collection, str] = (),
        **kwargs,
    ) -> List[Union[bytes, str]]:
        tokens = []
        text = unicodedata.normalize("NFC", text)

        # (源码注释) this implementation takes a detour: text -> token id -> token surface forms
        # 根据 huggingface PreTrainedTokenizer 的接口约定:
        #     tokenize 用于将原始字符串切分为 token 序列 (字符串序列)
        # PreTrainedTokenizer 最上层的对外接口是 __call__:
        #     而 __call__ 的执行逻辑是先调用 tokenize 方法, 再调用 convert_tokens_to_ids 方法
        # 这里由于 self.tokenizer 是 tiktoken.core.Encoding, 它只提供了 text -> token id 的接口, 所以只好迂回实现 tokenize 方法
        for t in self.tokenizer.encode(
            text, allowed_special=allowed_special, disallowed_special=disallowed_special
        ):
            tokens.append(self.decoder[t])

        def _encode_imgurl(img_tokens):
            assert img_tokens[0] == self.image_start_tag and img_tokens[-1] == self.image_end_tag
            img_tokens = img_tokens[1:-1]
            img_url = b''.join(img_tokens)
            out_img_tokens = list(map(self.decoder.get, img_url))
            if len(out_img_tokens) > IMG_TOKEN_SPAN:
                raise ValueError("The content in {}..{} is too long".format(
                    self.image_start_tag, self.image_end_tag))
            # IMG_TOKEN_SPAN = 256
            out_img_tokens.extend([self.image_pad_tag] * (IMG_TOKEN_SPAN - len(out_img_tokens)))
            out_img_tokens = [self.image_start_tag] + out_img_tokens + [self.image_end_tag]
            return out_img_tokens

        return _replace_closed_tag(tokens, self.image_start_tag, self.image_end_tag, _encode_imgurl)

def _replace_closed_tag(
    input_tokens: List[Any],
    start_tags: Union[Any, Tuple[Any]],
    end_tags: Union[Any, Tuple[Any]],
    inclusive_replace_func: Callable,
    exclusive_replace_func: Callable = lambda x: x,
):
    if isinstance(start_tags, (str, int)):
        start_tags = (start_tags,)
    if isinstance(end_tags, (str, int)):
        end_tags = (end_tags,)
    assert len(start_tags) == len(end_tags)

    output_tokens = []
    end = 0
    while True:
        start = _list_find(input_tokens, start_tags, end)
        if start == -1:
            break
        output_tokens.extend(exclusive_replace_func(input_tokens[end : start]))
        tag_idx = start_tags.index(input_tokens[start])
        end = _list_find(input_tokens, (end_tags[tag_idx],), start)
        if end == -1:
            raise ValueError("Unclosed image token")
        output_tokens.extend(inclusive_replace_func(input_tokens[start : end + 1]))
        end += 1
    output_tokens.extend(exclusive_replace_func(input_tokens[end : ]))
    return output_tokens
```

### `QWenTokenizer.decode`: (TODO)

可先参考下面的小节 `QWenLMHeadModel.generate` 的输出

```python
```

## model

### `QWenLMHeadModel.generate`: Overview

我们回到 `QWenLMHeadModel.chat` 的内部, 先看一下 `QWenLMHeadModel.generate` 的最终输出:

```python
# s, context_tokens = make_context(
#     tokenizer=tokenizer,
#     query="how about 2+2",
#     history=[("1+1=?", "1+1=2")],
#     system="you are a helpful assistant",
#     max_window_size=6144,
#     chat_format="chatml",
# )

# context_tokens 由上面得来
context_tokens = [151644, 8948, 198, 9330, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 16, 10, 16, 19884, 151645, 198, 151644, 77091, 198, 16, 10, 16, 28, 17, 151645, 198, 151644, 872, 198, 5158, 911, 220, 17, 10, 17, 151645, 198, 151644, 77091, 198]
input_ids = torch.tensor([context_tokens]).to(model.device)
stop_words_ids = [[tokenizer.im_end_id], [tokenizer.im_start_id]]
generation_config = model.generation_config

outputs = model.generate(
    input_ids,
    stop_words_ids=stop_words_ids,
    return_dict_in_generate=False,
    generation_config=generation_config,
    # **kwargs,
)
```

我们继续观察 `outputs`

```python
print(outputs)
# 输出:
# tensor([[151644,   8948,    198,   9330,    525,    264,  10950,  17847, 151645,
#             198, 151644,    872,    198,     16,     10,     16,  19884, 151645,
#             198, 151644,  77091,    198,     16,     10,     16,     28,     17,
#          151645,    198, 151644,    872,    198,   5158,    911,    220,     17,
#              10,     17, 151645,    198, 151644,  77091,    198,     17,     10,
#              17,     28,     19, 151645, 151643]], device='cuda:0')

new_tokens: List[int] = outputs.detach().cpu().tolist()[0][len(context_tokens):]
# tokenizer.decoder: Dict[int, Union[str, bytes]] 在 QWenTokenizer.__init__ 中构造(前面的小节略过这一点)
# 包含了所有 token 到 token_id 的映射 (含所有的 217 个特殊 token)
print([tokenizer.decoder[token_id] for token_id in new_tokens])

# 输出:
# [b'2', b'+', b'2', b'=', b'4', '<|im_end|>', '<|endoftext|>']
```

### `QWenLMHeadModel.transformers.encode`

```python
# model: QWenLMHeadModel
res = model.transformer.visual.encode(["https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"])
res.shape  # TODO: 估计是 [1, 256, C]
```
