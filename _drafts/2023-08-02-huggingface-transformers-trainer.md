---
layout: post
title: "(WIP) ğŸ¤— Transformers Trainer API"
date: 2023-08-02 22:00:04 +0800
labels: [huggingface]
---

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- ä½¿ç”¨ ğŸ¤— Transformers æ—¶æ¶‰åŠåˆ°æ¨¡å‹è®­ç»ƒçš„ä»£ç æ€ä¹ˆå†™æ‰æ˜¯ä¼˜é›…çš„æ–¹å¼
- ğŸ¤— Transformers Trainer çš„å®ç°é€»è¾‘

æ¶‰åŠå†…å®¹

- ğŸ¤— Transformers Trainer çš„å®ç°ç»†èŠ‚
- åº”è¯¥æ€æ ·æŒ‰éœ€åœ¨ Trainer çš„åŸºç¡€ä¸Šä¿®æ”¹/å¢åŠ åŠŸèƒ½

## Trainer ä½¿ç”¨å‚è€ƒ

ğŸ¤— Transformers GitHub é¡¹ç›®é‡ŒåŒ…å«äº†è®¸å¤šç«¯åˆ°ç«¯çš„ä¾‹å­, Trainer API çš„ä½¿ç”¨å¯ä»¥å€Ÿé‰´ [examples/pytorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch) åº•ä¸‹çš„å†…å®¹, ç²—ç•¥æ€»ç»“å¦‚ä¸‹:

```
# speed
benchmarking

# NLP
language-modeling
  - run_clm.py: Trainer
  - run_mlm.py: Trainer
  - run_plm.py: Trainer
question-answering
  - run_qa.py: QuestionAnsweringTrainer(è‡ªå®šä¹‰)
  - run_qa_beam_search.py: QuestionAnsweringTrainer(è‡ªå®šä¹‰)
  - run_seq2seq_qa.py: QuestionAnsweringSeq2SeqTrainer(è‡ªå®šä¹‰)
summarization
  - run_summarization.py: Seq2SeqTrainer
text-classification
  - run_glue.py: Trainer
  - run_xnli.py: Trainer
text-generation(ä»…å«æ¨ç†)
token-classification
  - run_ner.py: Trainer
translation
  - run_translation.py: Seq2SeqTrainer
multiple-choice(swag, é€‰æ‹©é¢˜)
  - run_swag.py: Trainer

# Audio
audio-classification
speech-pretraining
speech-recognition

# CV
contrastive-image-text
image-classification
image-pretraining
semantic-segmentation
```

## Trainer API è¯¦è§£

æœ¬èŠ‚é’ˆå¯¹å¦‚ä¸‹ç‰¹å®šç‰ˆæœ¬å¯¹ Trainer çš„ API è¿›è¡Œè§£é‡Š

```
accelerate==0.21.0
transformers==4.31.0
```

Trainer çš„**å…¨éƒ¨**æ–¹æ³•å¦‚ä¸‹:

```python
__init__

create_accelerator_and_postprocess

# ============ callback, state, control ==================
add_callback
pop_callback
remove_callback
call_model_init

# ======= train(train_dataset) ============
train
_inner_training_loop
training_step

compute_loss
compute_loss_context_manager
autocast_smart_context_manager


_load_best_model
_load_from_checkpoint
_load_optimizer_and_scheduler
_load_rng_state
_issue_warnings_after_load


_save
_save_checkpoint
_save_tpu
save_metrics
save_model
save_state
_rotate_checkpoints
_sorted_checkpoints
_get_output_dir

_maybe_log_save_evaluate

# ============ evaluate(eval_dataset) =============
evaluate

# ============ predict(test_dataset) ================
predict

# predictä¸evaluateéƒ½å¯èƒ½ä¼šè°ƒç”¨evaluation_loopæˆ–prediction_loop,è¿™ä¸¤ç§â€œloopâ€æœ€ç»ˆéƒ½è§¦å‘prediction_step
# é»˜è®¤ use_legacy_prediction_loop ä¸º False, æ­¤æ—¶ evaluate å’Œ predict éƒ½èµ° evaluation_loop

evaluation_loop
prediction_loop  # æºç ä¸­å°†è¿™éƒ¨åˆ†ä»£ç æ ‡è®°ä¸º deprecated code
prediction_step


# =========== train/evalauate/predict: dataset/dataloader ========
_get_eval_sampler
_get_train_sampler
get_test_dataloader
get_train_dataloader
get_eval_dataloader
_remove_unused_columns
_get_collator_with_removed_columns
_set_signature_columns_if_needed
_gather_and_numpify

# ========== optimizer/scheduler ===============================
create_optimizer
create_optimizer_and_scheduler
create_scheduler
get_optimizer_cls_and_kwargs
_get_learning_rate

# ============= others ==========
_move_model_to_device
_nested_gather
_prepare_input
_prepare_inputs
_wrap_model
ipex_optimize_model


store_flos
is_local_process_zero
is_world_process_zero
floating_point_ops
num_examples

_hp_search_setup
_tune_save_checkpoint


_push_from_checkpoint
create_model_card
_report_to_hp_search
_add_sm_patterns_to_gitignore
init_git_repo
push_to_hub


hyperparameter_search
log
log_metrics
metrics_format
torch_jit_model_eval
```

### æ ·ä¾‹

```python
# trainer.train çš„è¾“å‡º:
class TrainOutput(NamedTuple):
    global_step: int
    training_loss: float
    metrics: Dict[str, float]

# trainer.evaluation_loop/prediction_loop çš„è¾“å‡º:
class EvalLoopOutput(NamedTuple):
    predictions: Union[np.ndarray, Tuple[np.ndarray]]
    label_ids: Optional[Union[np.ndarray, Tuple[np.ndarray]]]
    metrics: Optional[Dict[str, float]]
    num_samples: Optional[int]
# trainer.evaluate çš„è¾“å‡ºæ˜¯ EvalLoopOutput.metrics

# trainer.predict çš„è¾“å‡º: å…¶å®å°±æ˜¯ EvalLoopOutput å»æ‰ num_samples å±æ€§
class PredictionOutput(NamedTuple):
    predictions: Union[np.ndarray, Tuple[np.ndarray]]
    label_ids: Optional[Union[np.ndarray, Tuple[np.ndarray]]]
    metrics: Optional[Dict[str, float]]

# text-classification/run_glue.py ç®€åŒ–åå¦‚ä¸‹
# Trainer å³å¯ä»¥ç”¨äºè®­ç»ƒ(å•å¡/å¤šå¡),ä¹Ÿå¯ä»¥ç”¨äºéªŒè¯(å¸¦æ ‡ç­¾,è·Ÿè®­ç»ƒä¸€è‡´,å•å¡/å¤šå¡),ä¹Ÿå¯ä»¥ç”¨äºå¯¹ä¸€ä¸ªæ•°æ®é›†åšæ¨ç†(æµ‹è¯•, ä¸å¸¦æ ‡ç­¾, å•å¡/å¤šå¡)
trainer = Trainer(
    model,
    training_args,
    data_collator,
    train_dataset,
    eval_dataset,
    tokenizer,
    model_init=None,
    compute_metrics=compute_metrics,  # Callable
    callbacks=None,
    optimizers=None,
    preprocess_logits_for_metrics=None,  # Callable
)

# training_args.resume_from_checkpoint: Optional[str]
checkpoint = None
if training_args.resume_from_checkpoint is not None:
    checkpoint = training_args.resume_from_checkpoint
train_result: "TrainOutput" = trainer.train(resume_from_checkpoint=checkpoint)
metrics = train_result.metrics

metrics: "EvalLoopOutput.metrics" = trainer.evaluate(eval_dataset=eval_dataset)

# predict_dataset = test_dataset
predict_dataset = predict_dataset.remove_columns("label")
predictions: "PredictionOutput" = trainer.predict(predict_dataset, metric_key_prefix="predict").predictions
predictions = np.squeeze(predictions) if is_regression else np.argmax(predictions, axis=1)
```

ä»¥ä¸‹æ˜¯è¯¦ç»†åˆ†æ

### å¯¹å¤– API

ä¸€èˆ¬æ¥è¯´, `Trainer` å¯¹å¤–çš„ API ä¸»è¦å°±æ˜¯ä¸Šé¢ä¾‹å­ä¸­æ‰€å±•ç¤ºçš„, é¦–å…ˆåˆå§‹åŒ–ä¸€ä¸ª `Trainer` å®ä¾‹, ç„¶åè°ƒç”¨ `train`, `evaluate`, `predict` å³å¯, é’ˆå¯¹æ¨ç†æ¥è¯´, ä½¿ç”¨ `evaluate` æˆ–è€… `predict` æ˜¯å¯¹ä¸€ä¸ª dataset åšæ¨ç†çš„, å¥½å¤„æ˜¯å®ƒä¹Ÿä¼šåˆ©ç”¨åˆ°å¤šå¼ å¡, è€Œå¦‚æœåªæƒ³å¯¹å•æ¡æ•°æ®/ä¸€ä¸ªbatchçš„æ•°æ®åšæ¨ç†çš„è¯, å¯ä»¥ä½¿ç”¨ `prediction_step`

### Trainer.train

- train å®é™…ä¸Šå°±æ˜¯ _inner_training_loop, å®Œæˆäº†æ•´ä¸ª(å¤šä¸ªepoch)çš„è®­ç»ƒè¿‡ç¨‹, çœŸæ­£å¹²æ´»çš„æ˜¯: training_step ä¸ compute_loss (å¯ä»¥é‡è½½).
- train çš„ hook çš„è°ƒç”¨ç‚¹æœ‰å‡ é¡¹: on_train_begin, on_epoch_begin, on_step_begin, on_step_end/on_substep_end, on_epoch_end, on_train_end, æ³¨æ„åœ¨ `Trainer` çš„è¯­å¢ƒé‡Œ, step æŒ‡çš„æ˜¯ä¸€æ¬¡æ¢¯åº¦æ›´æ–°, å¤§å¤šæ•°ä¸ step çš„æ¦‚å¿µéƒ½æ˜¯ä»¥ä¸€æ¬¡æ¢¯åº¦æ›´æ–°ä¸ºæœ€å°å•å…ƒçš„, è€Œ substep æ˜¯æŒ‡ä¸€æ¬¡æ¢¯åº¦æ›´æ–°æ‰€éœ€è¦çš„æ¢¯åº¦ç´¯ç§¯æ¬¡æ•°.
- trainçš„é»˜è®¤ callback æœ‰å¦‚ä¸‹:
    ```python
    # DefaultFlowCallback
    # on_step_end:
    # on_epoch_end:

    # ProgressCallback/NotebookProgressCallback/PrinterCallback
    # on_train_begin: åˆå§‹åŒ–è¿›åº¦æ¡
    # on_step_end: è¿›åº¦æ¡åŠ 1
    # on_train_end:
    # on_log:
    # on_prediction_step/on_evaluate/on_predict

    # TensorBoardCallback/WandbCallback/...
    # on_train_begin
    # on_train_end
    # on_log: åœ¨trainerä¸­stepç»“æŸåå¯èƒ½ä¼šé€šè¿‡ _maybe_log_save_evaluate åœ¨ DefaultFlowCallback.on_step_end è§¦å‘
    # å…·ä½“é€»è¾‘æ˜¯é¦–å…ˆ DefaultFlowCallback åœ¨ state.global_step % args.logging_steps == 0 æ—¶å°† control.should_log è®¾å®šä¸º True, ç„¶åè°ƒç”¨ trainer._maybe_log_save_evaluate æ—¶å†…éƒ¨ä¼šè§¦å‘ trainer.log, æœ€ç»ˆå½’ç»“ä¸º trainer.callback_handler.on_log
    ```
  æ³¨æ„: trainer.control.on_log å…ˆå°† control.should_log = False å†è§¦å‘ callbacks çš„ hook, trainer.control å¾ˆå¤š on_xxx éƒ½æœ‰ç±»ä¼¼çš„è¡Œä¸º. æ³¨æ„: TensorBoardCallback.on_log è§¦å‘æ—¶æ˜¯ä¸æ£€æŸ¥ control.should_log çš„. æ³¨æ„: DefaultFlowCallback ä¼šä¿®æ”¹ trainer.control å’Œ trainer.state, è€Œ ProgressCallback/TensorBoardCallback ä¸ä¿®æ”¹ trainer.control å’Œ trainer.state.


### Trainer.evaluate/predict

- evaluate/predict æ–¹æ³•å®é™…ä¸Šå°±æ˜¯ evaluation_loop, å®Œæˆäº†æ•´ä¸ªè¯„ä¼°, çœŸæ­£å¹²æ´»çš„æ˜¯: prediction_step. æ³¨æ„: `TrainingArguments` ä¸­åŒ…å« `use_legacy_prediction_loop` ä¸€é¡¹, å…¶é»˜è®¤å€¼ä¸º `False`, è¿™æ ·ä¼šå¯¼è‡´ evaluate/predict è¿›å…¥ evaluation_loop è€Œé prediction_loop, åè€…è¢«æ ‡è®°ä¸º deprecated code.
- evaluate ä¸ predict åŸºæœ¬ä¸Šæ˜¯ä¸€æ ·çš„: å› ä¸ºæœ¬è´¨ä¸Šéƒ½æ˜¯è°ƒç”¨ä¸€æ¬¡ `evaluation_loop`, å¾—åˆ°ä¸€ä¸ª `EvalLoopOutput` æ•°æ®ç»“æ„, å¤§ä½“é€»è¾‘å¦‚ä¸‹:
```python
# trainer.trainer.evaluation_loop/prediction_loop çš„è¾“å‡º:
class EvalLoopOutput(NamedTuple):
    predictions: Union[np.ndarray, Tuple[np.ndarray]]
    label_ids: Optional[Union[np.ndarray, Tuple[np.ndarray]]]
    metrics: Optional[Dict[str, float]]
    num_samples: Optional[int]
# trainer.evaluate çš„è¾“å‡ºæ˜¯ EvalLoopOutput.metrics

# trainer.predict çš„è¾“å‡º: å…¶å®å°±æ˜¯ EvalLoopOutput å»æ‰ num_samples å±æ€§
class PredictionOutput(NamedTuple):
    predictions: Union[np.ndarray, Tuple[np.ndarray]]
    label_ids: Optional[Union[np.ndarray, Tuple[np.ndarray]]]
    metrics: Optional[Dict[str, float]]

class Trainer:
    def predict(self, ...):
        output: "EvalLoopOutput" = self.evaluation_loop(...)
        return PredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=output.metrics)
    def evaluate(self, )
```


## è‡ªå®šä¹‰ Trainer æŒ‡å—

ä»å®ç°è§’åº¦, Trainer çš„ä¸»è¦é­”æ”¹æ–¹å¼æœ‰ä¸¤ç§: ä¸€ç§æ˜¯å†™ä¸€ä¸ªç±»ç»§æ‰¿ Trainer, é‡å†™æŸäº›æ–¹æ³•, å¦ä¸€ç§æ˜¯å®ä¾‹åŒ– Trainer æ—¶åŠ å…¥ä¸€äº› callback.

ä»åŠŸèƒ½è§’åº¦, æˆ‘ä»¬é€šå¸¸éœ€è¦é­”æ”¹éƒ¨åˆ†æœ‰è¿™äº›:

- è‡ªå®šä¹‰æ•°æ®é›†ä»¥åŠ dataloader
- è‡ªå®šä¹‰æŸå¤±è®¡ç®—é€»è¾‘
- è‡ªå®šä¹‰ optimizer ä¸ scheduler
- è‡ªå®šä¹‰æ—¥å¿—
- è‡ªå®šä¹‰æ¨¡å‹åŠ è½½
- è‡ªå®šä¹‰æ¨¡å‹ä¿å­˜

å…¶ä¸­å‰é¢ 4 é¡¹åœ¨ä¸‹ä¸€èŠ‚è®¨è®º, åé¢ 2 é¡¹åœ¨æœ€åè®¨è®º

### ç»§æ‰¿ Trainer å¹¶é‡è½½ä¸€äº›æ–¹æ³•

ç»§æ‰¿ Trainer è¿™ç§æ–¹å¼, å‚è€ƒå®˜æ–¹æ–‡æ¡£ [https://huggingface.co/docs/transformers/main_classes/trainer](https://huggingface.co/docs/transformers/main_classes/trainer), ä¸»è¦å…³æ³¨ä»¥ä¸‹æ–¹æ³•å³å¯: 

- `get_train_dataloader`, `get_eval_dataloader`, `get_test_dataloader`: æˆ‘ä»¬ç®€è¦çœ‹ä¸€ä¸‹å…¶ä¸­ä¸€ä¸ªçš„æºç 
    ```python
    def get_train_dataloader(self) -> DataLoader:
        if self.train_dataset is None:
            raise ValueError("Trainer: training requires a train_dataset.")

        train_dataset = self.train_dataset
        data_collator = self.data_collator
        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):
            train_dataset = self._remove_unused_columns(train_dataset, description="training")
        else:
            data_collator = self._get_collator_with_removed_columns(data_collator, description="training")

        dataloader_params = {
            "batch_size": self._train_batch_size,
            "collate_fn": data_collator,
            "num_workers": self.args.dataloader_num_workers,
            "pin_memory": self.args.dataloader_pin_memory,
        }

        if not isinstance(train_dataset, torch.utils.data.IterableDataset):
            dataloader_params["sampler"] = self._get_train_sampler()
            dataloader_params["drop_last"] = self.args.dataloader_drop_last
            dataloader_params["worker_init_fn"] = seed_worker

        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))
    ```
    è¿™ä¸‰ä¸ªå‡½æ•°é‡è½½èµ·æ¥æ¯”è¾ƒç®€å•, å®é™…ä¸Šæœ€ç»ˆåªæ˜¯å¾—åˆ°ä¸€ä¸ª dataloader, æ³¨æ„è¿™é‡Œä½¿ç”¨åˆ°çš„å‡ ä¸ªå†…éƒ¨æ–¹æ³•: `_remove_unused_columns`, `_get_collator_with_removed_columns`, `_get_train_sampler`, `_get_eval_sampler` ä»…åœ¨è¿™ä¸‰ä¸ªæ–¹æ³•ä¸­è¢«ä½¿ç”¨åˆ°, æ‰€ä»¥å¦‚æœé‡è½½æ—¶ä¸æ–¹ä¾¿æ“ä½œ, å¯ä»¥ä¸å»è°ƒç”¨è¿™å››ä¸ªå†…éƒ¨æ–¹æ³•, ä¸ä¼šå¼•å‘å…¶ä»–åœ°æ–¹çš„é€»è¾‘é—®é¢˜
- `log`: è¿™ä¸ªç›¸å¯¹æ¥è¯´æ˜¯æ¯”è¾ƒéœ€è¦é‡è½½çš„åœ°æ–¹, é¦–å…ˆçœ‹ä¸€ä¸‹ç›¸å…³çš„æºç 
    ```python
    def log(self, logs: Dict[str, float]) -> None:
        if self.state.epoch is not None:
            logs["epoch"] = round(self.state.epoch, 2)

        output = {**logs, **{"step": self.state.global_step}}
        self.state.log_history.append(output)
        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)

    def evaluate(self, ...):
        # åœ¨ evaluation_loop ç»“æŸä¹‹å
        # output: EvalLoopOutput
        self.log(output.metrics)
    def train(self, ...):
        # åœ¨æ•´ä¸ªè®­ç»ƒç»“æŸä¹‹å‰æœ‰ä¸€æ¬¡æ—¥å¿—è®°å½•
        self.log(metrics)
    # ä»¥ä¸‹ä¸º _maybe_log_save_evaluate æ–¹æ³•çš„å®Œæ•´æºç , æ­¤æ–¹æ³•åªåœ¨trainä¸­è¢«è°ƒç”¨: ä¸€å…±ä¸¤å¤„, ä¸€æ˜¯åœ¨æ¯æ¬¡æ¢¯åº¦æ›´æ–°ç»“æŸå, äºŒæ˜¯æ¯ä¸ªè®­ç»ƒepochç»“æŸåè¢«è°ƒç”¨
    # æ³¨æ„ train å‡½æ•°å¯¹ evaluate çš„è°ƒç”¨éƒ½æ˜¯é€è¿‡ _maybe_log_save_evaluate æ–¹æ³•çš„
    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval):
        if self.control.should_log:
            if is_torch_tpu_available():
                xm.mark_step()

            logs: Dict[str, float] = {}
            # all_gather + mean() to get average loss over all processes
            tr_loss_scalar = self._nested_gather(tr_loss).mean().item()
            # reset tr_loss to zero
            tr_loss -= tr_loss
            logs["loss"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)
            logs["learning_rate"] = self._get_learning_rate()
            self._total_loss_scalar += tr_loss_scalar
            self._globalstep_last_logged = self.state.global_step
            self.store_flos()
            self.log(logs)
        metrics = None
        if self.control.should_evaluate:  # æ³¨æ„, æ­¤å¤„å¯èƒ½è§¦å‘ trainer.evaluate çš„è°ƒç”¨
            if isinstance(self.eval_dataset, dict):
                metrics = {}
                for eval_dataset_name, eval_dataset in self.eval_dataset.items():
                    dataset_metrics = self.evaluate(
                        eval_dataset=eval_dataset,
                        ignore_keys=ignore_keys_for_eval,
                        metric_key_prefix=f"eval_{eval_dataset_name}",
                    )
                    metrics.update(dataset_metrics)
            else:
                metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
            self._report_to_hp_search(trial, self.state.global_step, metrics)

            # Run delayed LR scheduler now that metrics are populated
            if isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                metric_to_check = self.args.metric_for_best_model
                if not metric_to_check.startswith("eval_"):
                    metric_to_check = f"eval_{metric_to_check}"
                self.lr_scheduler.step(metrics[metric_to_check])

        if self.control.should_save:
            self._save_checkpoint(model, trial, metrics=metrics)
            self.control = self.callback_handler.on_save(self.args, self.state, self.control)
    ```
    æˆ‘ä»¬ä»æ—¥å¿—éœ€æ±‚çš„è§’åº¦æ¥ä¸¾ä¾‹çœ‹åº”è¯¥æ€ä¹ˆä¼˜é›…åœ°æ»¡è¶³:
    - éœ€è¦éš”å‡ ä¸ª step æ‰“å°ä¸€æ¬¡è¯¥ batch çš„è®­ç»ƒæ•°æ®, å³éœ€è¦ä½¿ç”¨ Tensorboard çš„ä¸€äº›ä¿å­˜æ–‡æœ¬çš„æ“ä½œ, è¿™ä¸ªå¯ä»¥é‡è½½ `training_step`, å†é‡è½½çš„æ–¹æ³•é‡Œè§¦å‘ `self.log` çš„è°ƒç”¨, å¹¶ä¸”é€‚å½“é‡è½½ `self.log` (ä»¥ Tensorboard ä¸¾ä¾‹, å¯èƒ½è¿˜éœ€è¦åœ¨é€‚å½“çš„åœ°æ–¹è°ƒç”¨ `add_text` æ–¹æ³•, å†…ç½®çš„ `TensorboardCallback` åªä¼šä½¿ç”¨åˆ° `add_scalar` åŠŸèƒ½)
    - éš”å‡ ä¸ª step/epoch è¿›è¡Œä¸€æ¬¡è®­ç»ƒé›†çš„æŸå¤±: å¯ä»¥é€šè¿‡ TrainingArguments é‡Œçš„å‚æ•°è¿›è¡Œç›¸åº”çš„è®¾ç½®é—´éš”æ•°

- `create_optimizer_and_scheduler`, `create_optimizer`, `create_scheduler`: æ³¨æ„åœ¨ train ä¸­, scheduler çš„æ›´æ–°é¢‘ç‡æ˜¯æ¯æ¬¡æ¢¯åº¦æ›´æ–°å°±æ›´æ–°ä¸€æ¬¡
- `compute_loss`, `training_step`:
    ```python
    def train(self, ...):
        for epoch in range(epochs_trained, num_train_epochs):
            # ...
            for step, inputs in enumerate(epoch_iterator):
                # ...
                with self.accelerator.accumulate(model):
                    tr_loss_step = self.training_step(model, inputs)
                # ...

    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
        model.train()
        inputs = self._prepare_inputs(inputs)
        if is_sagemaker_mp_enabled():
            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)
            return loss_mb.reduce_mean().detach().to(self.args.device)
        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)  # æ³¨æ„ compute_loss åœ¨ prediction_step ä¸­ä¹Ÿæœ‰å¯èƒ½è¢«è°ƒç”¨
        if self.args.n_gpu > 1:
            # è¿™é‡Œä¸æ˜¯å¾ˆç†è§£, ä¼šè§¦å‘å¤šGPUä¹‹é—´çš„é€šè®¯å—?
            loss = loss.mean()  # mean() to average on multi-gpu parallel training
        if self.do_grad_scaling:
            self.scaler.scale(loss).backward()
        elif self.use_apex:
            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                scaled_loss.backward()
        else:
            self.accelerator.backward(loss)
        return loss.detach() / self.args.gradient_accumulation_steps
    
    def compute_loss(self, model, inputs, return_outputs=False):
        if self.label_smoother is not None and "labels" in inputs:
            labels = inputs.pop("labels")
        else:
            labels = None
        outputs = model(**inputs)
        if self.args.past_index >= 0:
            self._past = outputs[self.args.past_index]
        if labels is not None:
            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():
                loss = self.label_smoother(outputs, labels, shift_labels=True)
            else:
                loss = self.label_smoother(outputs, labels)
        else:
            if isinstance(outputs, dict) and "loss" not in outputs:
                raise ValueError(
                    "The model did not return a loss from the inputs, only the following keys: "
                    f"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}."
                )
            # We don't use .loss here since the model may return tuples instead of ModelOutput.
            loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]
        return (loss, outputs) if return_outputs else loss
    ```

å…³äºè®­ç»ƒæŸå¤±, è¿™é‡Œåšå‡ ç‚¹è¯´æ˜ (ä»¥ DDP ä¸¾ä¾‹):
- é¦–å…ˆä» `training_step` å‡ºæ¥çš„æŸå¤±æ˜¯ä¸€å¼ å¡ä¸Šè¿™ä¸€ä¸ªbatchçš„å¹³å‡æŸå¤±
- å‡è®¾æ¯éš” 10 ä¸ª step è¿›è¡Œä¸€æ¬¡æ—¥å¿—æ‰“å°, æ¯å¼ å¡ä¸Šä¼šå°†è¿™ 10 ä¸ª step çš„æŸå¤±è¿›è¡ŒåŠ å’Œå¾—åˆ° `tr_loss` (`_maybe_log_save_evaluate` çš„å…¥å‚ä¹‹ä¸€)
- åœ¨ `_maybe_log_save_evaluate` å†…éƒ¨:
    ```python
    # å‡è®¾æœ‰ 4 å¼  GPU, é¦–å…ˆå°† 4 ä¸ª tr_loss æ±‡æ€»èµ·æ¥æ±‚å¹³å‡
    tr_loss_scalar = self._nested_gather(tr_loss).mean().item()
    # ç„¶ååœ¨é™¤ä»¥ 10, å¾—åˆ°å¹³å‡æŸå¤±, æ€»çš„æ¥è¯´è¿™é‡Œçš„ logs["loss"] æ˜¯è¿™ 10 ä¸ª step é‡Œå¹³å‡åˆ°æ¯ä¸ªæ ·æœ¬çš„å¹³å‡æŸå¤±
    logs["loss"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)
    ```
- `prediction_step`, `predict`, `evaluate`: Seq2SeqTrainer ä¸»è¦å°±æ˜¯é‡è½½äº†è¿™ä¸‰ä¸ªæ–¹æ³•, å…·ä½“å¯å‚è€ƒä¸‹ä¸€èŠ‚çš„ç¤ºä¾‹

### ä¾‹å­: Seq2SeqTrainer

```python
class Seq2SeqTrainer(Trainer):
    def __init__(self, ...):
        super().__init__(self, model, args, ...)
        if self.args.generation_config is not None:
            gen_config = self.load_generation_config(self.args.generation_config)
            self.model.generation_config = gen_config
    # predict ç±»ä¼¼, ä¹Ÿæ˜¯åŒæ ·çš„é‡è½½æ–¹å¼
    def evaluate(
        self,
        eval_dataset: Optional[Dataset] = None,
        ignore_keys: Optional[List[str]] = None,
        metric_key_prefix: str = "eval",
        **gen_kwargs,  # æ³¨æ„çˆ¶ç±» Trainer å¹¶ä¸å« gen_kwargs è¿™ä¸ªå…¥å‚
    ):
        gen_kwargs = gen_kwargs.copy()
        if gen_kwargs.get("max_length") is None and gen_kwargs.get("max_new_tokens") is None:
            gen_kwargs["max_length"] = self.args.generation_max_length
        gen_kwargs["num_beams"] = (
            gen_kwargs["num_beams"] if gen_kwargs.get("num_beams") is not None else self.args.generation_num_beams
        )
        self._gen_kwargs = gen_kwargs
        return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
    
    def prediction_step(
        self,
        model: nn.Module,
        inputs: Dict[str, Union[torch.Tensor, Any]],
        prediction_loss_only: bool,
        ignore_keys: Optional[List[str]] = None,
        **gen_kwargs,  # æ³¨æ„çˆ¶ç±» Trainer å¹¶ä¸å« gen_kwargs è¿™ä¸ªå…¥å‚
    ):
        # ä½¿ç”¨åˆ° self._gen_kwargs
        # ...
        # çˆ¶ç±»è¿™é‡Œæ˜¯ self.model(**inputs)
        generated_tokens = self.model.generate(**inputs, **gen_kwargs)
        # å¦‚æœéœ€è¦è®¡ç®—æŸå¤±, ä¼šå†è°ƒç”¨ä¸€æ¬¡ self.model(**inputs)
        return loss, generated_tokens, labels
```

æ³¨æ„ `Seq2SeqTrainer.train` æ–¹æ³•æ²¿ç”¨çˆ¶ç±»çš„ `Trainer.train`, å› æ­¤åœ¨ `train` ä¸­å¯¹ `Seq2SeqTrainer.evaluate` çš„è°ƒç”¨ä¸ä¼šä¼ å…¥ `gen_kwargs` å‚æ•°, å› æ­¤åœ¨è®­ç»ƒè¿‡ç¨‹é‡Œçš„éªŒè¯æ­¥éª¤ (å³æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹é‡Œ `generate` å‡½æ•°çš„æ§åˆ¶å‚æ•°) ä¾èµ–äºå®ä¾‹åŒ–æ—¶çš„ä¼ å‚ `args`, è€Œå•ç‹¬è°ƒç”¨ `evaluate` æˆ– `predict` æ—¶, å¯ä»¥é€šè¿‡ä¼ å…¥ `gen_kwargs` æ§åˆ¶æ–‡æœ¬ç”Ÿæˆçš„å‚æ•°.


### å¢åŠ  callback

è‡³äºåŠ  callback è¿™ç§åšæ³•, ğŸ¤— Transformers æœ¬èº«å†…ç½®çš„ callback å¹¶ä¸å¤š, å®é™…ä¸Šä¹Ÿè¶³å¤Ÿä½¿ç”¨äº†, æ„Ÿè§‰ä¸€èˆ¬ä¹Ÿä¸éœ€è¦å†æ–°å¢ä»€ä¹ˆäº†, åŒ…æ‹¬:

```python
DefaultFlowCallback
ProgressCallback/NotebookProgressCallback/PrinterCallback
TensorBoardCallback/WandbCallback/...
# è¿™ä¸ªéœ€è¦åœ¨ Trainer å®ä¾‹åŒ–æ—¶ä¼ å…¥
EarlyStoppingCallback
```

### Trainer çš„æ¨¡å‹åŠ è½½é€»è¾‘

æ¶‰åŠçš„è°ƒç”¨å…³ç³»å¦‚ä¸‹, ä¸»å…¥å£å¦‚ä¸‹:

- train å‡½æ•°ä¼ å…¥ `resume_from_checkpoint` æ—¶éœ€è¦å…³æ³¨: `_load_from_checkpoint`, `_load_optimizer_and_scheduler`, `_load_rng_state` å³å¯
- Trainer å®ä¾‹çš„å‚æ•° `args` ä¸­è®¾ç½®äº† `load_best_model_at_end=True` æ—¶, è¿˜éœ€è¦å…³æ³¨ `_load_best_model`

```python
# è®­ç»ƒå¼€å§‹å¯èƒ½ä¼šåŠ è½½æ¨¡å‹, è°ƒç”¨ _load_from_checkpoint
_load_from_checkpoint
# æ ¹æ®ä¸åŒçš„æƒ…å½¢, å¯èƒ½ä¼šåœ¨å†…éƒ¨è§¦å‘å¦‚ä¸‹:
# deepspeed_load_checkpoint: å¯ç”¨ deepspeed æ—¶
# load_sharded_checkpoint: å¤šä¸ªæ¨¡å‹åˆ‡ç‰‡æ—¶
# load_fsdp_model: å¯ç”¨ FSDP æ—¶
# model.load_state_dict

_load_optimizer_and_scheduler
_load_rng_state
_issue_warnings_after_load

# è®­ç»ƒç»“æŸæ—¶, æ ¹æ®åˆå§‹åŒ– Trainer æ—¶çš„å‚æ•°è®¾ç½®, å¯èƒ½ä¼šåŠ è½½æœ€ä¼˜çš„æ¨¡å‹
_load_best_model
```

### Trainer çš„æ¨¡å‹ä¿å­˜é€»è¾‘

æ¶‰åŠçš„è°ƒç”¨å…³ç³»å¦‚ä¸‹, ä¸»å…¥å£å¦‚ä¸‹:

- train å‡½æ•°ä¸­åªä¼šé€è¿‡ `_maybe_log_save_evaluate` è§¦å‘æ¨¡å‹ä¿å­˜, è€Œå®ƒåªç›´æ¥è§¦å‘ `_save_checkpoint` (ä¼šä¿å­˜æ¨¡å‹, ä¼˜åŒ–å™¨çŠ¶æ€, éšæœºç§å­ç­‰), è€Œä¿å­˜æ¨¡å‹çš„éƒ¨åˆ†æ˜¯ç”± `save_model` æ¥å®Œæˆçš„, è€Œå®ƒæ ¹æ®ä¸åŒçš„æƒ…å†µ, ä¸€èˆ¬ä¼šé€è¿‡ `_save` æ¥åšä¿å­˜.
- åœ¨ Trainer å®ä¾‹çš„å‚æ•° `args` ä¸­è®¾ç½®äº† `args.save_total_limit: int` æ—¶, ä¼šè§¦å‘ä¸€äº›åˆ é™¤æ¨¡å‹æ–‡ä»¶çš„æ“ä½œ, æœ€åº•å±‚æ¶‰åŠåˆ° `_rotate_checkpoints`
- train å‡½æ•°åœ¨åªä¿ç•™ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶çš„è®¾å®šæ—¶, è¿˜ä¼šåœ¨è®­ç»ƒç»“æŸæ—¶åšä¸€äº›åˆ é™¤æ¨¡å‹æ–‡ä»¶çš„æ“ä½œ (åˆ©ç”¨ `_sorted_checkpoints`)

```python
_maybe_log_save_evaluate  # åŒ…å«äº†å¯¹ _save_checkpoint çš„è°ƒç”¨
_save_checkpoint  # åŒ…å«äº†ä¿å­˜æƒé‡, ä¼˜åŒ–å™¨çŠ¶æ€, éšæœºç§å­ç­‰
  _get_output_dir   # è¢« _save_checkpoint è°ƒç”¨, ç”¨äºç¡®å®šä¿å­˜è·¯å¾„
  save_model        # è¢« _save_checkpoint è°ƒç”¨, ç”¨äºä¿å­˜æƒé‡, æ ¹æ®ä¸åŒçš„è®­ç»ƒè®¾ç½®åˆ†åˆ«è°ƒç”¨å¦‚ä¸‹
    _save
    _save_tpu
    # save_fsdp_model
  save_metrics
  save_state

# åªåœ¨ _save_checkpoint ç»“æŸæ—¶è¢«è°ƒç”¨
_rotate_checkpoints
_sorted_checkpoints
```

å¤„äºè°ƒç”¨é“¾æœ€ä½ç«¯çš„ `_save` å‡½æ•°å®Œæ•´æºç å¦‚ä¸‹: å®ƒéœ€è¦è´Ÿè´£ä¿å­˜ `model`, `tokenizer` å’Œè®­ç»ƒå‚æ•° `args`

```python
def _save(self, output_dir: Optional[str] = None, state_dict=None):
    # If we are executing this function, we are the process zero, so we don't check for that.
    output_dir = output_dir if output_dir is not None else self.args.output_dir
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Saving model checkpoint to {output_dir}")

    supported_classes = (PreTrainedModel,) if not is_peft_available() else (PreTrainedModel, PeftModel)
    # Save a trained model and configuration using `save_pretrained()`.
    # They can then be reloaded using `from_pretrained()`
    if not isinstance(self.model, supported_classes):
        if state_dict is None:
            state_dict = self.model.state_dict()

        if isinstance(unwrap_model(self.model), supported_classes):
            unwrap_model(self.model).save_pretrained(
                output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors
            )
        else:
            logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
            if self.args.save_safetensors:
                safetensors.torch.save_file(state_dict, os.path.join(output_dir, SAFE_WEIGHTS_NAME))
            else:
                torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))
    else:
        self.model.save_pretrained(
            output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors
        )

    if self.tokenizer is not None:
        self.tokenizer.save_pretrained(output_dir)

    # Good practice: save your training arguments together with the trained model
    torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))
```

## æ¡ˆä¾‹åˆ†æ 1: run_glue.py

## æ¡ˆä¾‹åˆ†æ 2: chatglm2