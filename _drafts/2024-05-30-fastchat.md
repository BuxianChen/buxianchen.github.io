---
layout: post
title: "(P1) FastChat 简介"
date: 2024-05-30 00:05:04 +0800
labels: [llm,fastchat]
---

## 动机、参考资料、涉及内容

先暂时记录启动方式, 后续再看要不要深入

## 使用方法

**注册多个模型的方法**

依赖

```bash
# python 3.11, cuda 12.1
pip install fschat[model_worker,webui]
pip install vllm
```

分别用 4 个终端启动 (参考: [https://github.com/lm-sys/FastChat/blob/main/README.md](https://github.com/lm-sys/FastChat/blob/main/README.md) 和 [https://github.com/lm-sys/FastChat/blob/main/playground/FastChat_API_GoogleColab.ipynb](https://github.com/lm-sys/FastChat/blob/main/playground/FastChat_API_GoogleColab.ipynb))

TODO:

下面这种方案能成功运行, 但 seed 参数似乎不起作用, 猜测可能需要用这个方案: `vllm.entrypoints.openai.api_server` 启动 + `fastchat.serve.llm_worker` 启动, 每个模型需要 2 个终端

```bash
python3 -m fastchat.serve.controller  # 默认端口号是 21002

# 启动模型 1:
# (方法1) 使用 fastchat 对 vllm 的集成
# CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.vllm_worker --model-path /home/buxian/wsl2-test/gpt-2 --model-names gpt-2 --controller http://localhost:21001 --port 31000 --worker-address http://localhost:31000
# (方法2) 使用 huggingface transformers 的方式推理
CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.model_worker --model-path /home/buxian/wsl2-test/gpt-2 --model-names gpt-2 --controller http://localhost:21001 --port 31000 --worker http://localhost:31000

# 启动模型 2:
# (方法1) 使用 fastchat 对 vllm 的集成
# CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.vllm_worker --model-path /home/buxian/wsl2-test/opt-125m --model-names opt-125m,facebook/opt-125m --controller http://localhost:21001 --port 31001 --worker-address http://localhost:31001
# (方法2) 使用 huggingface transformers 的方式推理
-port 31001 --worker-address http://localhost:31001
CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.model_worker --model-path /home/buxian/wsl2-test/opt-125m --model-names opt-125m,facebook/opt-125m --controller http://localhost:21001 --port 31001 --worker http://localhost:31001


# 转换为 openai api 接口: 打开 http://127.0.0.1:8000/docs 观察文档, 似乎不包含 seed 参数
# 而 http://127.0.0.1:31001/docs 和 http://127.0.0.1:31000/docs 和 http://127.0.0.1:21001/docs 暂时没观察到跟模型推理有关的信息, 有点奇怪
python3 -m fastchat.serve.openai_api_server --host 0.0.0.0 --controller-address http://127.0.0.1:21001 --port 8000
```

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123",
)
xs = []
for i in range(2):
    completion = client.chat.completions.create(
        model="gpt-2",
        messages=[
            {"role": "user", "content": "Hello!"}
        ],
        seed=1,
        max_tokens=200,
    )
    xs.append(completion.choices[0].message.content)
xs[0] == xs[1]  # False
```

**vllm 的 entrypoint 启动方式**

可以允许传入随机种子 seed 参数, 且确实输出值是相同的

备注: 假设遇到类似这种错误

```
The client socket has failed to connect to any network address of (172.19.16.1, 53929)
```

请参考: [issue](https://github.com/vllm-project/vllm/issues/2427)

```bash
# 似乎无法指定 model-name
python -m vllm.entrypoints.openai.api_server --model /home/buxian/wsl2-test/gpt-2 --tokenizer /home/buxian/wsl2-test/gpt-2 --dtype auto --port 8000 --host 0.0.0.0
```

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123",
)
xs = []
for i in range(2):
    completion = client.chat.completions.create(
        model="/home/buxian/wsl2-test/gpt-2",
        messages=[
            {"role": "user", "content": "Hello!"}
        ],
        seed=1,
        max_tokens=200,
    )
    xs.append(completion.choices[0].message.content)
xs[0] == xs[1]  # True
```
