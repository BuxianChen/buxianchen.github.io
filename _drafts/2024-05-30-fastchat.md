---
layout: post
title: "(P1) FastChat 简介"
date: 2024-05-30 00:05:04 +0800
labels: [llm,fastchat]
---

## 动机、参考资料、涉及内容

先暂时记录启动方式, 后续再看要不要深入

## 使用方法

**注册多个模型的方法**

依赖

```bash
# python 3.11, cuda 12.1
pip install fschat[model_worker,webui]
pip install vllm
```

分别用 4 个终端启动 (参考: [https://github.com/lm-sys/FastChat/blob/main/README.md](https://github.com/lm-sys/FastChat/blob/main/README.md) 和 [https://github.com/lm-sys/FastChat/blob/main/playground/FastChat_API_GoogleColab.ipynb](https://github.com/lm-sys/FastChat/blob/main/playground/FastChat_API_GoogleColab.ipynb))

TODO:

- 怎么利用 vllm 来启动? 两种方案都还没成功: (1) `vllm.entrypoints.openai.api_server` 启动 + `fastchat.serve.llm_worker` 启动, 每个模型需要 2 个终端 (2) `fastchat.serve.vllm_worker` 启动, 每个模型需要一个终端
- 下面这种方案能成功运行, 但 seed 参数似乎不起作用 (官方说要用两个不同的 GPU 卡, 笔者只有 1 张卡, 不太确定是不是这个原因?)

```bash
python3 -m fastchat.serve.controller  # 默认端口号是 21002

# 怎么用 fastchat.serve.vllm_worker 或 vllm.entrypoints.openai.api_server 还不确定
CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.model_worker --model-path /home/buxian/wsl2-test/gpt-2 --model-names gpt-2 --controller http://localhost:21001 --port 31000 --worker http://localhost:31000

CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.model_worker --model-path /home/buxian/wsl2-test/opt-125m --model-names opt-125m,facebook/opt-125m --controller http://localhost:21001 --port 31001 --worker http://localhost:31001

python3 -m fastchat.serve.openai_api_server --host 0.0.0.0 --controller-address http://127.0.0.1:21001 --port 8000
```

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123",
)
xs = []
for i in range(2):
    completion = client.chat.completions.create(
        model="gpt-2",
        messages=[
            {"role": "user", "content": "Hello!"}
        ],
        seed=1,
        max_tokens=200,
    )
    xs.append(completion.choices[0].message.content)
xs[0] == xs[1]  # False
```

**vllm 的 entrypoint 启动方式**

可以允许传入随机种子 seed 参数, 且确实输出值是相同的

```bash
# 似乎无法指定 model-name
python -m vllm.entrypoints.openai.api_server --model /home/buxian/wsl2-test/gpt-2 --tokenizer /home/buxian/wsl2-test/gpt-2 --dtype auto --port 8000 --host 0.0.0.0
```

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123",
)
xs = []
for i in range(2):
    completion = client.chat.completions.create(
        model="/home/buxian/wsl2-test/gpt-2",
        messages=[
            {"role": "user", "content": "Hello!"}
        ],
        seed=1,
        max_tokens=200,
    )
    xs.append(completion.choices[0].message.content)
xs[0] == xs[1]  # True
```
