---
layout: post
title: "(WIP) T5 è¯¦è§£"
date: 2023-03-10 14:31:04 +0800
labels: [paper]
---

<style>
h2:after {
  content: "# ";
  color: gray;
}
h3:after {
  content: "## ";
  color: gray;
}
h4:after {
  content: "### ";
  color: gray;
}
h5:after {
  content: "#### ";
  color: gray;
}
.alert-red {
    padding: 1em;
    border: 1px solid #f44336;
    background-color: #ffebee;
    color: #f44336;
    /* font-weight: bold; */
    margin-top: 1em;
    margin-bottom: 1em
}
</style>

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- ç†Ÿæ‚‰ ğŸ¤— Transformers çš„ç›¸å…³ API ä¸æºç 
- ç†Ÿæ‚‰ ğŸ¤— Tokenizers çš„ç›¸å…³ API ä¸æºç 
- æ·±å…¥ç†è§£ T5 çš„è®­ç»ƒä¸æ¨ç†æ­¥éª¤ï¼ŒåŒ…æ‹¬æ¯ä¸€æ­¥çš„è®¡ç®—è¿‡ç¨‹
- é€‚å½“è¡¥å……ç›¸å…³çŸ¥è¯†

å‚è€ƒèµ„æ–™

- ğŸ¤— Transformers 4.26.1 æºä»£ç 
- ğŸ¤— Transformers å®˜æ–¹æ–‡æ¡£
- T5åŸå§‹è®ºæ–‡
  - è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/pdf/1910.10683.pdf
  - æ ‡é¢˜ï¼šExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
  - æœºæ„ï¼šGoogle

æ³¨æ„äº‹é¡¹

ä¸»è¦ä»ä¸¤ä¸ªè§†è§’æ¥å†™è¿™ç¯‡åšå®¢ï¼š

- åŸç†è§†è§’ï¼šä¸»è¦æ˜¯è®ºæ–‡é‡Œæè¿°ä¸ºä¸»ï¼Œä½†ç¼ºç‚¹æ˜¯æŸäº›åœ°æ–¹å¯èƒ½ä¼šæœ‰ä¸€å®šçš„æ¨¡ç³Š
- å®ç°è§†è§’ï¼šä»¥ ğŸ¤— Transformers çš„å®é™…å®ç°ä¸ºå‡†

## Overview: T5

T5 æ¨¡å‹å°è¯•å°†æ‰€æœ‰çš„ NLP ä»»åŠ¡åšäº†ä¸€ä¸ªç»Ÿä¸€å¤„ç†ï¼Œå³ï¼šå°†æ‰€æœ‰çš„ NLP ä»»åŠ¡éƒ½è½¬åŒ–ä¸º Text-to-Text ä»»åŠ¡ã€‚å¦‚åŸè®ºæ–‡ä¸‹å›¾æ‰€ç¤ºï¼š
![](../assets/figures/t5/text-to-text.png)

ç»¿è‰²çš„æ¡†æ˜¯ä¸€ä¸ªç¿»è¯‘ä»»åŠ¡ï¼ˆè‹±æ–‡ç¿»è¯‘ä¸ºå¾·æ–‡ï¼‰ï¼ŒæŒ‰ç…§ä»¥å¾€æ ‡å‡†çš„ç¿»è¯‘æ¨¡å‹çš„åšæ³•ï¼Œæ¨¡å‹çš„è¾“å…¥ä¸ºï¼š`That is good.`ï¼ŒæœŸæœ›æ¨¡å‹çš„è¾“å‡ºä¸ºï¼š`Das ist gut.`ï¼Œè€Œ T5 çš„åšæ³•æ˜¯å°†è¾“å…¥è½¬åŒ–ä¸ºï¼š`translate English to German: That is good.`ï¼ŒæœŸæœ›çš„è¾“å‡ºä¾ç„¶ç»´æŒåŸæ ·ã€‚ä¹Ÿå°±æ˜¯å°† NLP ä»»åŠ¡çš„æè¿°ä¹ŸåŠ åœ¨äº†æ¨¡å‹è¾“å…¥é‡Œã€‚åŸæ–‡ä¸­é™„å½• D ä¸­ç»™å‡ºäº†æ›´å¤šçš„ä¾‹å­ã€‚

åœ¨æ¨¡å‹ç»“æ„ä¸Šï¼ŒT5 æ¨¡å‹é‡‡ç”¨äº† Encoder-Decoder çš„æ¶æ„ï¼Œä»å¤§ä½“ä¸Šè¯´ï¼Œå¯¹äºè®­ç»ƒè¿‡ç¨‹ï¼Œä¼ªä»£ç å¦‚ä¸‹ï¼š

```python
x, y = "translate English to German: That is good.", "Das ist gut."
x = tokenizer(x)  # [0, 23, 45, 89, 1230, 4, 9], å…¶ä¸­0ä»£è¡¨<BOS>, åœ¨å®ç°ä¸­<PAD>ä¹Ÿæ˜¯0
y = tokenizer(y)  # [0, 44, 156, 4, 1], å…¶ä¸­1ä»£è¡¨<EOS>
x_embedding = encoder.embed_layer(x)  # å°†tokenè½¬æ¢ä¸ºembedding, x_embeddingçš„å½¢çŠ¶ä¸º(7, 768)
encoded_x = encoder.other_layer(x_embedding)  # ç»è¿‡encoderåencoded_xçš„å½¢çŠ¶ä¸º(7, 768)

input_y = y[:-1]  # [0,  44,  156, 4]
# å°†tokenè½¬åŒ–ä¸ºemdedding, input_y_emdeddingçš„å½¢çŠ¶ä¸º(4, 768)
input_y_emdedding = decoder.embed_layer(input_y)  # åœ¨T5çš„è®¾è®¡ä¸­ï¼Œencoder.embed_layerä¸decoder.embed_layerå…±äº«å‚æ•°
target_y = y[1:]  # [44, 156, 4,   1]

# decoder_outputçš„å½¢çŠ¶ä¸º(4, 768)
decoder_output = decoder.other_layer(encoded_x, input_y_emdedding)

# logits çš„å½¢çŠ¶ä¸º(4, vocab_size=32128)
logits = linear_layer(decoder_output)  # åœ¨T5çš„è®¾è®¡ä¸­ï¼Œdecoder.embed_layerä¸linear_layerå…±äº«å‚æ•°

# æ¥ä¸‹æ¥ä½¿ç”¨ softmax ä¸æ™®é€šçš„äº¤å‰ç†µè®¡ç®—æŸå¤±
loss = loss_fn(logits, target_y)
```


## Overview: ğŸ¤— Transformers
å¯¹äº ğŸ¤— Transformers çš„æºç é˜…è¯»è€Œè¨€ï¼Œæœ¬æ–‡ä¸»è¦çš„å…³æ³¨ç‚¹åœ¨äºä»¥ä¸‹éƒ¨åˆ†ï¼Œé¦–å…ˆ ğŸ¤— Transformers github é¡¹ç›®çš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼ˆèŠ‚é€‰ï¼‰

```
examples                         # ä¸€äº›ç¤ºä¾‹ä»£ç , å¯ä¾›å­¦ä¹ , ä½†ä¸ç¡®ä¿èƒ½ä¸å½“å‰ç‰ˆæœ¬å…¼å®¹
  - flax/language-modeling/t5_tokenizer_model.py  # t5 tokenizer è®­ç»ƒå‚è€ƒ
  - flax/language-modeling/run_t5_mlm_flax.py     # t5 mask-LM é¢„è®­ç»ƒå‚è€ƒ
  - pytorch/summarization                         # t5 ç”Ÿæˆå¼æ¨¡å‹è®­ç»ƒå‚è€ƒ
src/transformers
  - generation/
    - beam_constraints.py        # constraint_beam_search è¾…åŠ©æ–¹æ³•/ç±»: Constraint, ConstraintListState
    - beam_search.py             # beam_search è¾…åŠ©æ–¹æ³•/ç±»: BeamSearchScorer, ConstrainedBeamSearchScorer, BeamHypotheses
    - configuration_utils.py     # ç”Ÿæˆå¼æ¨¡å‹çš„ç»Ÿä¸€é…ç½®æ–‡ä»¶, ç”¨æ¥æ§åˆ¶ç”Ÿæˆç®—æ³•åŠå„ç±»è¶…å‚æ•°, ä¾‹å¦‚ç”Ÿæˆé•¿åº¦æƒ©ç½š
    - logit_process.py           # ç”Ÿæˆè¿‡ç¨‹æ—¶å¯¹log-softmax scoreçš„åå¤„ç†ï¼šLogitsProcessor, LogitsWarpper
    - stopping_criteria.py       # ç”Ÿæˆä¸­æ­¢æ¡ä»¶ï¼šStoppingCriteria
    - streamer.py                # transformers 4.28.0 ç‰ˆæœ¬æ–°å¢, ç”¨äºç”Ÿæˆå­—ç¬¦æ—¶æµå¼é€è¯è¾“å‡º
    - utils.py                   # GenerationMixin çš„å®ç°
    ...
  - models/  # æ¯ä¸ªæ¨¡å‹ä¸ºä¸€ä¸ªå•ç‹¬çš„æ–‡ä»¶å¤¹, æ¯ä¸ªæ–‡ä»¶å¤¹çš„æ–‡ä»¶ç»“æ„æ¯”è¾ƒå›ºå®š, å‚è€ƒt5å­æ–‡ä»¶å¤¹
    - t5/
      - __init__.py
      - convert_t5_original_tf_checkpoint_to_pytorch.py  # æœ‰äº›æ¨¡å‹åŸå§‹å®˜æ–¹ä»“åº“çš„æƒé‡éœ€è¦é€šè¿‡è½¬æ¢å¾—åˆ° ğŸ¤— Transformers ä¸­æ¨¡å‹å®šä¹‰ä¸‹æ¨¡å‹è½½å…¥çš„æ ¼å¼, è¿™ç§æƒ…å†µä¸‹ä¼šç»´æŠ¤ä¸€ä¸ªè½¬æ¢è„šæœ¬
      - modeling_flax_t5.py      # flaxç‰ˆæœ¬çš„æ¨¡å‹ç»“æ„ä»£ç , æœ¬æ–‡ä¸æ¶‰åŠ
      - modeling_tf_t5.py        # tensorflowç‰ˆæœ¬çš„æ¨¡å‹ç»“æ„ä»£ç , æœ¬æ–‡ä¸æ¶‰åŠ
      - modeling_t5.py           # pytorchç‰ˆæœ¬çš„æ¨¡å‹ç»“æ„ä»£ç 
      - configuration_t5.py      # æ¯ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªè‡ªå·±çš„æ¨¡å‹ç»“æ„å‚æ•°é…ç½®æ–‡ä»¶
      - tokenization_t5.py       # æ¯ä¸ªæ¨¡å‹çš„slow/pythonç‰ˆæœ¬çš„tokenizerå®ç°, é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢
      - tokenization_t5_fast.py  # æ¯ä¸ªæ¨¡å‹çš„fastç‰ˆæœ¬çš„tokenizerå®ç°, é€Ÿåº¦è¾ƒå¿«, ä¾èµ–äº ğŸ¤— Tokenizers
      - ...
    - ...
  - pipelines/                   # å°è£…tokenizerä¸model, ç®€åŒ–ä½¿ç”¨, æœ¬æ–‡ä¸æ¶‰åŠ
  - modeling_outputs.py          # æ¨¡å‹è¾“å‡ºç»“æœçš„æ•°æ®ç»“æ„
  - modeling_utils.py            # æ‰€æœ‰æ¨¡å‹çš„åŸºç±»: PreTrainedModel
  - tokenization_utils_base.py   # æ‰€æœ‰tokenizerçš„åŸºç±»: PreTrainedTokenizerBase
  - tokenization_utils.py        # æ‰€æœ‰slowç‰ˆæœ¬tokenizerçš„åŸºç±»: PreTrainedTokenizer
  - tokenization_utils_fast.py   # æ‰€æœ‰fastç‰ˆæœ¬tokenizerçš„åŸºç±»: PreTrainedTokenizerFast
  - trainer.py                   # Trainerç±», æœ¬æ–‡ä¸æ¶‰åŠ
  - trainer_callback.py          # Trainerç±»ä¸­ä½¿ç”¨åˆ°çš„ TrainerCallback/TrainerState/TrainerControl/CallbackHandler
  - integrations.py              # é«˜çº§æ—¥å¿—è®°å½•å·¥å…·, ä¾‹å¦‚: TensorBoardCallback
  - ...
```

## åŸç†è§£æï¼šTokenizer

å–å†³äºä¸åŒçš„ tokenizer å®ç°, ğŸ¤— Tokenizers ä¸­çš„ Tokenizer åœ¨encodeé˜¶æ®µé€šå¸¸ä¼šè¿›è¡Œå¦‚ä¸‹å‡ ä¸ªæ­¥éª¤ï¼Œå…·ä½“å®ç°ç»†èŠ‚è§æºç è§£æéƒ¨åˆ†

```
# ä»¥bert-base-uncasedçš„fastç‰ˆæœ¬ä¸ºä¾‹
How are U today?
# Normalization
how are u today?
# Pre-tokenization
[how, are, u, today, ?]
# tokenize
[how, are, u, to, ##day, ?]
# Postprocess
[CLS, how, are, u, to, ##day, ?, SEP]
```

<div class="alert-red">
æ³¨æ„: æœ¬èŠ‚å‰©ä½™éƒ¨åˆ†çš„ç®—æ³•æè¿°ä¸ä¿è¯ä¸ ğŸ¤— Tokenizers æˆ– ğŸ¤— Transformers ä¸­çš„ slow/fast ç‰ˆä¸­çš„å®ç°å®Œå…¨å»åˆã€‚åŸå› æ˜¯ï¼š
ï¼ˆ1ï¼‰ğŸ¤— Tokenizers çš„ç¡®å®ç°äº†ä»¥ä¸‹çš„å‡ ç§ç®—æ³•[å‚è€ƒå®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/tokenizers/api/models)ï¼Œä½†ç”±äºğŸ¤— Tokenizersé‡‡ç”¨äº† Rust è¿›è¡Œå®ç°ï¼Œç¬”è€…æš‚æ—¶æ— åŠ›ç†æ¸…å‡†ç¡®çš„æºç ï¼Œæ‰€ä»¥æ²¡æœ‰æ·±ç©¶
ï¼ˆ2ï¼‰ğŸ¤— Transformers ä¸­çš„ slow/fast ç‰ˆçš„ tokenizer æ˜¯ä¸ºäº†å¯¹é½ç›¸åº”æ¨¡å‹çš„åŸå§‹å®ç°ï¼Œå› æ­¤å¯¹äºä¸€ä¸ªä¸ªå…·ä½“çš„æ¨¡å‹çš„ Tokenizerï¼Œæœ‰å¯èƒ½ä¼šå¯¹æ ‡å‡†çš„ BPE/WordPiece/Unigramç®—æ³•åšäº›å°æ”¹åŠ¨ã€‚
</div>



### BPE

ä¸€ä¸ªå¸¦æœ‰å®Œæ•´å®ç°çš„æ•™ç¨‹ï¼š[ğŸ¤— NLP Course](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)

BPE (Byte Pairwise Encoding) ç®—æ³•çš„è®­ç»ƒæµç¨‹å¦‚ä¸‹ï¼š
```
è¾“å…¥ï¼šå¥å­åˆ—è¡¨ï¼Œè¯è¡¨æ•°é‡ä¸Šé™
ï¼ˆä¾‹å­ï¼‰ï¼š[" ".join(["hug"]*10), " ".join(["pug"]*5), " ".join(["pun"]*12)]
å‰å¤„ç†ï¼šå°†å¥å­åˆ—è¡¨è½¬æ¢ä¸ºè¯åˆ—è¡¨ï¼Œå¹¶ç»Ÿè®¡è¯é¢‘ã€‚åŒæ—¶è®°å½•æ‰€æœ‰å‡ºç°çš„å­—ç¬¦ä½œä¸ºbase_vocabï¼šæœ€ç»ˆå¾—åˆ°çš„ç»“æœä¸ºï¼š[(è¯è¯­1, è¯é¢‘1), ..., (è¯è¯­N, è¯é¢‘N)], base_vocab: [å­—ç¬¦1, ..., å­—ç¬¦K]
ï¼ˆä¾‹å­ï¼‰ï¼šä»¥ç©ºæ ¼ä½œä¸ºåˆ†éš”ç¬¦è¿›è¡Œåˆ‡è¯ï¼Œå¾—åˆ°[("hug": 10), ("pug", 5), ("pun", 12)], base_vocab: ["h", "u", "g", "p", "n"]
è®­ç»ƒæµç¨‹ï¼š
  é¦–å…ˆåˆå§‹åŒ–æ‰€æœ‰è¯è¯­çš„å½“å‰æ‹†è§£æ–¹å¼ï¼š{è¯è¯­1: ([å­—ç¬¦1,...,å­—ç¬¦k_1], è¯é¢‘1), ..., è¯è¯­N: ([å­—ç¬¦1,...,å­—ç¬¦k_N], è¯é¢‘N)}, å½“å‰mergeåˆ—è¡¨ä¸º: []
  ï¼ˆä¾‹å­ï¼‰ï¼š{hug: ([h, u, g], 10), pug: ([p, u, g], 5), pun: ([p, u, n], 12)}

  While True:
    æ ¹æ®å½“å‰è¯çš„æ‹†è§£æ–¹å¼è®¡ç®—å€™é€‰çš„mergeåˆ—è¡¨åŠå¯¹åº”çš„é¢‘æ•°, å€™é€‰çš„mergeåˆ—è¡¨æŒ‡çš„æ˜¯æ‰€æœ‰è¯è¯­å½“å‰æ‹†è§£æ–¹å¼
    ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå€™é€‰mergeåˆ—è¡¨ä¸ºï¼š[(h, u): 10, (u, g): 15, (p, u): 17, (u, n): 12]
    ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå€™é€‰mergeåˆ—è¡¨ä¸ºï¼š[(h, u): 10, (u, g): 10, (pu, g): 5, (pu, n): 12]
    é€‰å‡ºè¯é¢‘æœ€å¤§çš„mergeæ–¹å¼, åŠ å…¥è‡³mergeåˆ—è¡¨, å¹¶å¯¹æ‰€æœ‰è¯è¯­çš„æ‹†è§£æ–¹å¼åšæ›´æ–°
    ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå°†åŸå§‹çš„è¯è¯­æ‹†è§£æ–¹å¼ç”¨ [p, u] -> puæ›´æ–°ï¼Œå¾—åˆ°ï¼š{hug: ([h, u, g], 10), pug: ([pu, g], 5), pun: ([pu, n], 12)}, mergeåˆ—è¡¨ä¸º: [(p, u)]
    ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå°†åŸå§‹çš„è¯è¯­æ‹†è§£æ–¹å¼ç”¨ [pu, n] -> punæ›´æ–°ï¼Œå¾—åˆ°ï¼š{hug: ([h, u, g], 10), pug: ([pu, g], 5), pun: ([pun], 12)}, mergeåˆ—è¡¨ä¸º: [(p, u),(pu, n)]
    å¾ªç¯ç›´è‡³ï¼ˆmergeåˆ—è¡¨é•¿åº¦+base_vocabé•¿åº¦ï¼‰è¾¾åˆ°è¯è¡¨æ•°é‡ä¸Šé™
```

æ¨ç†æµç¨‹å¦‚ä¸‹
```
è¾“å…¥ï¼šå¥å­ï¼Œbase_vocabä¸åˆå¹¶è§„åˆ™
ï¼ˆä¾‹å­ï¼‰ï¼šbase_vocabä¸åˆå¹¶è§„åˆ™ï¼š[h, u, g, p, n, (p, u), (h, u), (hu, g)]
å‰å¤„ç†ï¼šå°†å¥å­æ‹†è§£ä¸ºè¯è¯­åˆ—è¡¨
æ¨ç†æµç¨‹ï¼š
  tokens = []
  for word in sentence:
    word_split = [å­—ç¬¦1, ..., å­—ç¬¦k]
    ï¼ˆä¾‹å­ï¼‰ï¼šword_split = [h,u,g,i,h,u]
    for merge in merges:
      å°è¯•å°†mergeåº”ç”¨äºwordä¸Š, å¹¶æ›´æ–°word_split
      ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå°è¯•ä½¿ç”¨(p, u)åˆå¹¶ï¼Œword_splitä¸å˜
      ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå°è¯•ä½¿ç”¨(h, u)åˆå¹¶ï¼Œword_splitå˜ä¸º[hu, g, g, i, hu]
      ï¼ˆä¾‹å­-ç¬¬3è½®ï¼‰ï¼šå°è¯•ä½¿ç”¨(hu, g)åˆå¹¶ï¼Œword_splitå˜ä¸º[hug, g, i, hu]
    tokens.extend(word_split)
```

### WordPiece

ä¸€ä¸ªå¸¦æœ‰å®Œæ•´å®ç°çš„æ•™ç¨‹ï¼š[ğŸ¤— NLP Course](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)

WordPiece ç®—æ³•æ˜¯ Bert æ‰€ç”¨çš„ tokenize ç®—æ³•

<div class="alert-red">
æ­£å¦‚[ğŸ¤— NLP Course](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)æŒ‡å‡ºçš„é‚£æ ·, Google å¹¶æœªå°† WordPiece çš„è®­ç»ƒç®—æ³•è¿›è¡Œå¼€æºï¼Œä½†æ¨ç†ç®—æ³•æ˜¯å¼€æºçš„ï¼Œæ¨ç†ç®—æ³•å¯å‚è€ƒ[bertæºç ](https://github.com/google-research/bert/blob/master/tokenization.py)ã€‚å› æ­¤ä¸¥æ ¼åœ°è¯´ï¼ŒWordPiece çš„è®­ç»ƒç®—æ³•åªæ˜¯çŒœæµ‹ã€‚
</div>

ä¸ BPE ç®—æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼š
- ä¸­é—´å­—ç¬¦é‡‡ç”¨ "##" å¼€å¤´è¡¨ç¤º
- è®­ç»ƒé˜¶æ®µ
  - é€‰å–mergeæ—¶ï¼Œåˆ¤æ–­æœ€å¤§å€¼çš„æ ‡å‡†å˜ä¸º: åˆå¹¶å‡ºç°çš„æ¬¡æ•°/(piece1çš„æ¬¡æ•°*piece2çš„æ¬¡æ•°)
  - ä¸ä¿ç•™mergeçš„äºŒå…ƒç»„, åªä¿ç•™æœ€ç»ˆç»“æœ
- æ¨ç†é˜¶æ®µ
  - è´ªå¿ƒç®—æ³•åŒ¹é…æ¯ä¸ªè¯çš„å‰©ä½™å­—ç¬¦

WordPiece ç®—æ³•çš„è®­ç»ƒæµç¨‹å¦‚ä¸‹ï¼š

```
è¾“å…¥ï¼šå¥å­åˆ—è¡¨ï¼Œè¯è¡¨æ•°é‡ä¸Šé™
ï¼ˆä¾‹å­ï¼‰ï¼š[" ".join(["hug"]*10), " ".join(["pug"]*5), " ".join(["pun"]*12)]
å‰å¤„ç†ï¼šå°†å¥å­åˆ—è¡¨è½¬æ¢ä¸ºè¯åˆ—è¡¨ï¼Œå¹¶ç»Ÿè®¡è¯é¢‘ã€‚åŒæ—¶è®°å½•æ‰€æœ‰å‡ºç°çš„å­—ç¬¦ä½œä¸ºvocabï¼ŒåŒ…æ‹¬å‡ºç°åœ¨å¼€å¤´çš„å­—ç¬¦ä¸å‡ºç°åœ¨ä¸­é—´çš„å­—ç¬¦ï¼šæœ€ç»ˆå¾—åˆ°çš„ç»“æœä¸ºï¼š[(è¯è¯­1, è¯é¢‘1), ..., (è¯è¯­N, è¯é¢‘N)], vocab: [å­—ç¬¦1, ..., å­—ç¬¦K]
ï¼ˆä¾‹å­ï¼‰ï¼šä»¥ç©ºæ ¼ä½œä¸ºåˆ†éš”ç¬¦è¿›è¡Œåˆ‡è¯ï¼Œå¾—åˆ°[("hug": 10), ("pug", 5), ("pun", 12)], vocab: ["h", "##u", "##g", "p", "##n"]
è®­ç»ƒæµç¨‹ï¼š
  é¦–å…ˆåˆå§‹åŒ–æ‰€æœ‰è¯è¯­çš„å½“å‰æ‹†è§£æ–¹å¼ï¼š{è¯è¯­1: ([å­—ç¬¦1,##å­—ç¬¦k_2...,##å­—ç¬¦k_1], è¯é¢‘1), ..., è¯è¯­N: ([å­—ç¬¦1,##å­—ç¬¦2...,##å­—ç¬¦k_N], è¯é¢‘N)}
  ï¼ˆä¾‹å­ï¼‰ï¼š{hug: ([h, ##u, ##g], 10), pug: ([p, ##u, ##g], 5), pun: ([p, ##u, ##n], 12)}
  While True:
    æ ¹æ®å½“å‰è¯çš„æ‹†è§£æ–¹å¼è®¡ç®—å€™é€‰çš„mergeåˆ—è¡¨åŠå¯¹åº”çš„åˆ†æ•°(åˆå¹¶åå‡ºç°çš„é¢‘æ•°/åˆå¹¶å‰çš„é¢‘æ•°ä¹‹ç§¯), å€™é€‰çš„mergeåˆ—è¡¨æŒ‡çš„æ˜¯æ‰€æœ‰è¯è¯­å½“å‰æ‹†è§£æ–¹å¼
    ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå€™é€‰mergeåˆ—è¡¨ä¸ºï¼š[(h, ##u): 10/(10*27), (##u, ##g): 15/(27*15), (p, ##u): 17/(17*27), (##u, ##n): 12/(27*12)]ï¼Œè¿™ä¸ªä¾‹å­æ¯”è¾ƒç‰¹åˆ«ï¼Œåˆ†æ•°å…¨éƒ¨ç›¸åŒ
    ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå€™é€‰mergeåˆ—è¡¨ä¸ºï¼š[(hu, ##g): 10/(10*15) , (p, ##u): 17/(17*17), (##u, ##g): 5/(17*15), (##u, ##n): 17/(17*12)]ï¼Œæœ€å¤§åˆ†æ•°çš„åˆå¹¶æ–¹å¼ä¸º(##u, ##n)
    é€‰å‡ºè¯é¢‘æœ€å¤§çš„mergeæ–¹å¼, vocabåˆ—è¡¨, å¹¶å¯¹æ‰€æœ‰è¯è¯­çš„æ‹†è§£æ–¹å¼åšæ›´æ–°
    ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå°†åŸå§‹çš„è¯è¯­æ‹†è§£æ–¹å¼ç”¨ [h, ##u] -> huæ›´æ–°ï¼Œå¾—åˆ°ï¼š{hug: ([hu, ##g], 10), pug: ([p, ##u, ##g], 5), pun: ([p, ##u, ##n], 12)}ã€‚vocab.append("hu")
    ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå°†åŸå§‹çš„è¯è¯­æ‹†è§£æ–¹å¼ç”¨ [##u, ##n] -> ##unæ›´æ–°ï¼Œå¾—åˆ°ï¼š{hug: ([hu, ##g], 10), pug: ([p, ##u, ##g], 5), pun: ([p, ##un], 12)}ã€‚vocab.append("##un")
    å¾ªç¯ç›´è‡³vocabè¾¾åˆ°è¯è¡¨æ•°é‡ä¸Šé™
```

æ¨ç†æµç¨‹å¦‚ä¸‹ï¼ˆå…¶å®æ˜¯ç®€å•çš„è´ªå¿ƒç­–ç•¥ï¼Œå°½é‡åŒ¹é…è¯è¡¨é‡Œæœ€é•¿çš„å­—ä¸²ï¼Œå¦‚æœæŸä¸€æ­¥ç¢°åˆ°OOVï¼Œåˆ™è¿™ä¸ªè¯çš„å‰©ä½™éƒ¨åˆ†è¢«æ ‡è®°ä¸ºUNKï¼‰ï¼Œå‡†ç¡®ä»£ç å¯ç›´æ¥å‚è€ƒ[BertåŸå§‹ä»£ç ](https://github.com/google-research/bert/blob/master/tokenization.py)
```
è¾“å…¥ï¼šå¥å­ï¼Œvocab
å‰å¤„ç†ï¼šå°†å¥å­æ‹†è§£ä¸ºè¯è¯­åˆ—è¡¨
æ¨ç†æµç¨‹ï¼š
  tokens = []
  for word in sentence:
    start=0, end=len(word)
    while start < len(word):
      while end > start:
        if word[start:end] in vocab:
          tokens.append(word[start:end])
          end -= 1
          start = end
          break
        if end == start:
          è¿™ç§æƒ…å†µä¸‹æŠŠæ•´ä¸ªåç»­tokenéƒ½ä½œä¸º[unk]ï¼Œä¸å†è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†è¯
```

### Unigram

è¿™é‡ŒæŒ‰ç…§ [ğŸ¤— nlp course](https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt) ä¸­çš„æè¿°å¯¹ç®—æ³•è¿›è¡Œç®€è¦ä»‹ç»ã€‚

è®­ç»ƒæµç¨‹ï¼š

- å‰å¤„ç†ï¼šå°†å¥å­åˆ†å‰²ä¸ºè¯
- é¦–å…ˆå°†æ‰€æœ‰å‡ºç°çš„å•ä¸ªå­—ç¬¦ä½œä¸ºbase-vocabï¼Œç„¶åä½¿ç”¨ä¸€äº›æ–¹æ³•è·å–åˆ°ä¸€ä¸ªç›¸å¯¹æ¯”è¾ƒå¤§çš„è¯è¡¨vocabï¼ˆæ•™ç¨‹çš„ä»£ç é‡Œé‡‡ç”¨çš„æ˜¯æ‰€æœ‰å‡ºç°çš„è¯çš„å­åºåˆ—ï¼Œå¹¶æŒ‡å‡ºå®é™…ä½¿ç”¨æ—¶å¯ä»¥é‡‡ç”¨BPEç®—æ³•ï¼‰ï¼Œå…¶ä¸­vocabåŒ…å«base-vocabã€‚å¹¶ä¸”è®¡ç®—vocabä¸­æ¯ä¸ªtokenå‡ºç°çš„é¢‘ç‡ï¼Œä¾›åç»­è®¡ç®—æŸå¤±æ—¶ä½¿ç”¨ã€‚
- å¯¹äºvocabä¸­çš„æ¯ä¸ªébase-vocabä¸­çš„è¯ï¼Œè®¡ç®—è¿™ä¸ªè¯ä»è¯è¡¨ä¸­æ’é™¤åï¼Œæ•´ä½“æŸå¤±çš„å¢é•¿é‡ï¼Œä¸¢å¼ƒå¢é•¿é‡æœ€å¤§çš„å‰20%çš„vocabã€‚é‡å¤æ­¤æ­¥éª¤ç›´è‡³è¯è¡¨å¤§å°æ»¡è¶³è¦æ±‚

ç»™å®šä¸€ä¸ªè¯è¡¨ï¼Œè¿™ä¸ªè¯è¡¨åœ¨æ•°æ®é›†ä¸Šçš„æŸå¤±å®šä¹‰ä¸ºæ•°æ®é›†ä¸­æ‰€æœ‰è¯çš„æŸå¤±æŒ‰è¯é¢‘åŠ æƒå¹³å‡ï¼Œè€Œæ¯ä¸ªè¯çš„æŸå¤±ä¸ºï¼š

$$
L(word)=\max_{\bold{x}\in S(word)}[-\sum_{i}log(p(x_i))]
$$

è¿™é‡Œ $S(word)$ è¡¨ç¤ºçš„æ˜¯æŒ‰ç…§ vocabï¼Œæ‰€æœ‰èƒ½æ‹¼å‡‘æˆ $word$ çš„ subword åºåˆ—ã€‚

æ¨ç†æµç¨‹ï¼š

(1) one-best-decoding: å³è®¡ç®—æ¯ä¸ªè¯çš„æŸå¤±æ—¶æ‰¾åˆ°çš„æœ€ä¼˜ subword åºåˆ—ï¼Œè¿™å¯ä»¥ç”¨åŠ¨æ€è§„åˆ’ï¼ˆç»´ç‰¹æ¯”ç®—æ³•ï¼‰æ¥è§£å†³ï¼Œå…·ä½“è¿‡ç¨‹ä»ç•¥ã€‚
(2) k-best-decoding: [ğŸ¤— nlp course](https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt) æ²¡æœ‰æ¶‰åŠåˆ°ï¼Œä½†åŸå§‹è®ºæ–‡ä¸­æŒ‡å‡ºå¯ä»¥ä½¿ç”¨ Forward-DP Backward-A* ç®—æ³•å¾—åˆ°æœ€ä¼˜çš„ k ç§subword åºåˆ—, ä½¿ç”¨ Forward-Filtering and Backward-Sampling algorithm(FFBS) å¯ä»¥æŒ‰æ¦‚ç‡é‡‡æ ·åˆ° k ç§ subword åºåˆ—


åŸå§‹è®ºæ–‡[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959)ä¸­å¯¹ Unigram ç®—æ³•çš„æè¿°ä¸ä¸Šè¿°åŸºæœ¬ä¸€è‡´ï¼Œç¨æœ‰ä¸åŒçš„æ˜¯åœ¨ $p(x_i)$ çš„è®¡ç®—ä¸Šï¼Œè®ºæ–‡ä¸­æè¿°ç”¨ EM ç®—æ³•å¾—åˆ°ï¼Œè€Œä¸Šè¿°æè¿°é‡Œç›´æ¥ä½¿ç”¨é¢‘ç‡å¾—åˆ°ã€‚

å¯¹åŸå§‹è®ºæ–‡çš„ç†è§£ä»¥åŠä¸€äº›å®ç°ç»†èŠ‚å¯ä»¥å‚è€ƒè¿™ç¯‡[åšå®¢](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)

### SentencePiece

<div class="alert-red">
æœ¬å°èŠ‚çš„æè¿°å¯èƒ½ä¸å‡†ç¡®ï¼Œéœ€è¿›ä¸€æ­¥åˆ†è¾¨
</div>

åœ¨ ğŸ¤— Transformers ä¸­,  æŒ‰[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/tokenizer_summary)æè¿°ï¼šSentencePiece ç®—æ³•æ€»æ˜¯å’Œ Unigram é…åˆä½¿ç”¨, å› æ­¤å¯ä»¥è®¤ä¸ºåœ¨ ğŸ¤— Transformers ä¸­, è¿™ä¸¤è€…åŸºæœ¬ä¸Šå¯ä»¥åˆ’ç­‰å·ã€‚ï¼ˆğŸ¤— Transformers ä¸­çš„ SentencePiece = ä¸€äº›é¢„å¤„ç† + Unigramï¼‰

åœ¨å®ç°ç»†èŠ‚ä¸Šï¼ŒğŸ¤— Transformers ä¸­ fast tokenizer ä¾èµ–äº ğŸ¤— Tokenizersï¼Œè€Œ ğŸ¤— Tokenizers ä¸­å¯¹ sentencepiece çš„å¤„ç†æ–¹å¼æ˜¯ä½¿ç”¨ protobuf è§£æ sentencepiece çš„è¯è¡¨å­˜å‚¨æ ¼å¼, ç„¶åå†ç»„åˆä¸Š ğŸ¤— Tokenizers è‡ªèº«å®ç°çš„ Unigram, è¯¦ç»†å†…å®¹å¯ä»¥å‚è€ƒ[tokenizers/implementations/sentencepiece_unigram.py](https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py)ã€‚å³ç›¸å½“äº ğŸ¤— Tokenizers é‡æ–°å®ç°äº† sentencepieceã€‚ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼š ğŸ¤— Tokenizers ä¹Ÿå®ç°äº† SentencePieceBPETokenizer ï¼Œä½†å¹¶æœªåœ¨ ğŸ¤— Transformers è¢«ä½¿ç”¨åˆ°ã€‚

ğŸ¤— Transformers ä¸­ slow tokenizer åˆ™ä¸€èˆ¬ä¾èµ–äº sentencepiece åŒ…


### T5 ä½¿ç”¨çš„ tokenizer

T5 ä½¿ç”¨ SentencePiece ä½œä¸º tokenizerï¼Œç»†èŠ‚å‚è€ƒå®ç°éƒ¨åˆ†

## æºç è§£æ: ğŸ¤— Tokenizers

æœ¬èŠ‚åªä»‹ç» ğŸ¤— Tokenizers æœ¬èº«çš„ä½¿ç”¨ï¼Œä¸æ¶‰åŠ ğŸ¤— Transformers ä¸­ fast tokenizer å¯¹ ğŸ¤— Tokenizers çš„è¿›ä¸€æ­¥å°è£…

ğŸ¤— Tokenizers çš„[å®˜æ–¹æ–‡æ¡£-Getting Started](https://huggingface.co/docs/tokenizers/index)å¯¹ä½¿ç”¨çš„ä»‹ç»å·²ç»è¶³å¤Ÿå……åˆ†ï¼Œæ­¤å¤„ä»…èµ·ä¸€ä¸ªæµ“ç¼©çš„ä½œç”¨ã€‚

ğŸ¤— Tokenizers ä»£ç åº“çš„æ ¸å¿ƒç±»ä¸º `tokenizers.Tokenizer`ã€‚

### ç»„æˆ

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
# (component-3: model): å°†è¯tokenizeä¸ºtokenåˆ—è¡¨: List(str) -> List(Token)
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

# (component-1: normalizer): å¯¹åŸå§‹å¥å­è¿›è¡Œé¢„å¤„ç†: str -> str
from tokenizers.normalizers import NFD, StripAccents
tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents()])

# (component-2: pre_tokenizer): å°†å¥å­æ‹†åˆ†ä¸ºè¯åˆ—è¡¨: str -> List(str)
from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

# (component-4: post-processor): å¯¹tokenåˆ—è¡¨è¿›è¡Œåå¤„ç†, ä¾‹å¦‚å¢åŠ EOS: List(Token) -> List(Token)
from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",  # è¿™é‡Œçš„:1æŒ‡çš„æ˜¯å°†è¿™éƒ¨åˆ†çš„token_type_idæ ‡è®°ä¸º1
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

# (component-5: decoder): å°†tokenåˆ—è¡¨è½¬æ¢ä¸ºå¥å­: List(str) -> str
from tokenizers import decoders
tokenizer.decoder = decoders.WordPiece()
tokenizer.decode(output.ids)

# trainer
from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)

# tokenizer çš„ä¿å­˜æ ¼å¼ä¸ºä¸€ä¸ªå•ä¸€çš„ json æ–‡ä»¶
tokenizer.save("data/tokenizer-wiki.json")
tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")
tokenizer = Tokenizer.from_pretrained("bert-base-uncased")  # ä¸ ğŸ¤— Transformers ä¸­ fast tokenizer çš„ä½¿ç”¨ç±»ä¼¼

# è¿™ç§ç”¨æ³•å¯èƒ½ä¸å¸¸ç”¨? BertWordPieceTokenizer çš„åŸºç±»æ˜¯BaseTokenizer, è€ŒBaseTokenizerä¸Tokenizerç±»æ— å…³
from tokenizers import BertWordPieceTokenizer
tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)
```

ä»¥ gpt2 ä¸ºä¾‹ç®€è¦çœ‹ä¸€ä¸‹å„ä¸ªç»„æˆéƒ¨åˆ†æ€ä¹ˆå•ç‹¬è¢«è°ƒç”¨

<div class="alert-red">
æ³¨æ„: ä¸€èˆ¬æƒ…å†µä¸‹, ä¸è¦å•ç‹¬ä½¿ç”¨å„ä¸ªç»„æˆéƒ¨åˆ†
</div>

```python
from tokenizers import Tokenizer
tokenizer = Tokenizer.from_pretrained("gpt2")

# (component-1: normalizer): å¯¹åŸå§‹å¥å­è¿›è¡Œé¢„å¤„ç†: str -> str
tokenizer.normalizer                              # None
# ä¸å·§çš„æ˜¯, gpt2å¹¶æ²¡æœ‰normalizer, æ‰€ä»¥è¿™é‡Œåªå¥½å¦å¤–é€ ä¸€ä¸ªä¾‹å­
from tokenizers.normalizers import StripAccents, NFD, NFC, Sequence
normalizer = Sequence([NFD(), StripAccents()])    # StripAccents éœ€è¦ä¸ NFD é…åˆä½¿ç”¨
normalizer.normalize_str("Ã©")                     # è¾“å‡º: 'e'

text = "ä¸­å›½"

# (component-2: pre_tokenizer): å°†å¥å­æ‹†åˆ†ä¸ºè¯åˆ—è¡¨: str -> List(str)
tokenizer.pre_tokenizer                           # tokenizers.pre_tokenizers.ByteLevel
word_with_pos = tokenizer.pre_tokenizer.pre_tokenize_str("ä¸­å›½")
# word_with_pos: [('Ã¤Â¸ÅƒÃ¥Ä½Â½', (0, 2))], åˆ‡è¯çš„ç»“æœ, è¿™ä¸ªçœ‹èµ·æ¥ä¹±ç çš„ä¸œè¥¿å®é™…ä¸Šé•¿åº¦ä¸º6(åœ¨utf-8ç¼–ç ä¸­æ±‰å­—ä¸€èˆ¬ç”±3ä¸ªå­—èŠ‚æ„æˆ)
print([ord(x) for x in word_with_pos[0][0]])      # [228, 184, 323, 229, 317, 189]
print(list(text.encode()))                        # [228, 184, 323, 229, 317, 189]

# (component-3: model): å°†è¯tokenizeä¸ºtokenåˆ—è¡¨: List(str) -> List(Token)
tokenizer.model                                   # tokenizers.models.ByteLevel
all_tokens = []
for word, (start, end) in word_with_pos:
    tokens = tokenizer.model.tokenize(word)       # tokens: List[tokenizers.Token]
    all_tokens.append(tokens)

# tokenizers.Token ä¸»è¦æ–¹æ³•ä¸º as_tuple(), ä¸»è¦å±æ€§æ˜¯ value, id
print([token.as_tuple() for token in all_tokens[0]])
# è¾“å‡ºä¸º: [(40792, 'Ã¤Â¸Åƒ', (0, 6)), (32368, 'Ã¥Ä½', (6, 10)), (121, 'Â½', (10, 12))]
# ä¸ºä»€ä¹ˆæ˜¯(0, 6), (6, 10), (10, 12)è€Œä¸æ˜¯(0, 3), (3, 5), (5, 6)ï¼Ÿ

# (component-4: post-processor): å¯¹tokenåˆ—è¡¨è¿›è¡Œåå¤„ç†, ä¾‹å¦‚å¢åŠ EOS: List(Token) -> List(Token)
tokenizer.post_processor                          # tokenizers.processors.ByteLevel

# ä¸å·§çš„æ˜¯, gpt2çš„post_processoræ²¡æœ‰è¿½åŠ ä»»ä½•token
# tokenizer = Tokenizer.from_pretrained("hfl/chinese-roberta-wwm-ext")
encoding = tokenizer.encode(text, add_special_tokens=False)
print(encoding.tokens)                            # ['ä¸­', 'å›½']
encoding = tokenizer.post_processor.process(encoding)
print(encoding.tokens)                            # ['[CLS]', 'ä¸­', 'å›½', '[SEP]']

# (component-5: decoder): å°†tokenåˆ—è¡¨è½¬æ¢ä¸ºå¥å­: List(str) -> str
tokenizer.decoder                                 # tokenizers.decoders.ByteLevel
token_strs = [token.value for tokens in all_tokens for token in tokens]  # ['Ã¤Â¸Åƒ', 'Ã¥Ä½', 'Â½']
tokenizer.decoder.decode(token_strs)              # "ä¸­å›½"
```

### ä½¿ç”¨ã€TODOã€‘

æœ¬èŠ‚ä¸ºå¸¸è§çš„ä½¿ç”¨æ–¹æ³•
```python
# æ„å»ºæ–¹æ³•1
from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
# tokenizer.normalizer = ...
# tokenizer.pre_tokenizer = ...
# tokenizer.post_processor = ...
# tokenizer.decoder = ...

# æ„å»ºæ–¹æ³•2: åœ¨ ğŸ¤— hub ä¸­ä¿å­˜çš„ tokenizer.json æ–‡ä»¶
tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")

# æ„å»ºæ–¹æ³•3: ç±»ä¼¼äº ğŸ¤— Transformers çš„ä½¿ç”¨
tokenizer = Tokenizer.from_pretrained("bert-base-uncased")

# ä¿å­˜
save_path = "tokenizer.json"
tokenizer.save(save_path)

# è¡¥å……: ğŸ¤— Transformers ä¸­ä½¿ç”¨ tokenizers.Tokenizer æ„å»º fast tokenizer
# æœ¬è´¨ä¸Š: (1) PreTrainedTokenizerFast çš„è¡Œä¸ºå®Œå…¨ç”± tokenizer_object å†³å®š
# (2) PreTrainedTokenizerFast.save_pretrained å®é™…ä¸Šè°ƒç”¨äº† Tokenizer.save
from transformers import PreTrainedTokenizerFast
fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=save_path)

ã€TODOã€‘
tokenizer.encode
tokenizer.token_to_id
tokenizer.encode_batch
tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")
tokenizer.add_special_tokens(tokens: List[Union[AddedToken, str]]) # -> int
tokenizer.add_tokens(tokens: List[Union[AddedToken, str]])
```

## æºç è§£æ: sentencepieceã€TODOï¼šæºç è§£æå¾…åç»­å¦èµ·ä¸€ç¯‡åšå®¢è¿›è¡Œä»‹ç»ã€‘

ğŸ¤— Transformers ä¸­æ¯ä¸ªå…·ä½“çš„ slow tokenizer çš„å®ç°é‡Œ, å¦‚æœ tokenizer çš„ç±»å‹ä¸º BPE æˆ–è€…æ˜¯ WordPiece, é‚£ä¹ˆä¸€èˆ¬æ˜¯åœ¨ç›¸åº”çš„ `xxx_tokenizer.py` ä¸­ä½¿ç”¨ python å®ç° BPE å’Œ WordPieceã€‚å› æ­¤ä¼šå‘ç°ä¸€äº›é‡å¤çš„ä»£ç ï¼Œä¾‹å¦‚[tokenization_bert.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py)ä¸[tokenization_distilbert.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/tokenization_distilbert.py)ï¼Œè¿™ç¬¦åˆ ğŸ¤— Transformers ä»£ç åº“çš„[å“²å­¦](https://huggingface.co/docs/transformers/philosophy)ã€‚è€Œ tokenizer çš„ç±»å‹ä¸º SentencePiece æ—¶ï¼Œç›¸åº”çš„ slow tokenizer çš„å®ç°ä¼šå€ŸåŠ© [sentencepiece](https://pypi.org/project/sentencepiece/) åŒ…ã€‚

sentencepieceåŒ…çš„ä½¿ç”¨æ–¹æ³•è¯·ç›´æ¥å‚è€ƒ: [å®˜æ–¹ç¤ºä¾‹](https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb)


## æºç è§£æ: ğŸ¤— Transformers ä¸­çš„ tokenizer

é¦–å…ˆè¯´æ˜ä¸€ä¸‹ ğŸ¤— Transformers ä¸ ğŸ¤— Tokenizers ä¹‹é—´çš„å…³ç³»ï¼šğŸ¤— Transformers 4.x ç‰ˆæœ¬ä¸­æ¯ä¸ªæ¨¡å‹éƒ½ä¼šå°½é‡æ”¯æŒä¸¤ç§ tokenizer çš„å®ç°, slowç‰ˆæœ¬çš„å®ç°ä¸fastç‰ˆæœ¬çš„å®ç°, åè€…ä¾èµ–äº ğŸ¤— Tokenzers åŒ…, è€Œå‰è€…ä¸ä¾èµ–, ä¸”ä¸ºçº¯ python å®ç°ï¼Œæ‰€ä»¥ slow ç‰ˆæœ¬çš„ tokenizer æ›´æ–¹ä¾¿é˜…è¯»ã€‚

å…·ä½“æ¥è¯´ï¼ŒğŸ¤— Transformers ä¸­ fast tokenizer åœ¨å®ä¾‹åˆå§‹åŒ–æ—¶æœ‰å¦‚ä¸‹ä»£ç æ®µï¼š
```python
from .convert_slow_tokenizer import convert_slow_tokenizer
from tokenizers import Tokenizer as TokenizerFast

class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
  vocab_files_names = VOCAB_FILES_NAMES
  slow_tokenizer_class: PreTrainedTokenizer = None
  can_save_slow_tokenizer: bool = True
  
  # èŠ‚é€‰äº†ä¸€éƒ¨åˆ†
  def __init__(self, *args, **kwargs):
    tokenizer_object = kwargs.pop("tokenizer_object", None)
    slow_tokenizer = kwargs.pop("__slow_tokenizer", None)
    fast_tokenizer_file = kwargs.pop("tokenizer_file", None)
    from_slow = kwargs.pop("from_slow", False)
    if from_slow and slow_tokenizer is None and self.slow_tokenizer_class is None:
        raise ValueError("...")
    if tokenizer_object is not None:
        fast_tokenizer = copy.deepcopy(tokenizer_object)
    elif fast_tokenizer_file is not None and not from_slow:
        # We have a serialization from tokenizers which let us directly build the backend
        fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
    elif slow_tokenizer is not None:
        # We need to convert a slow tokenizer to build the backend
        fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
    elif self.slow_tokenizer_class is not None:
        # We need to create and convert a slow tokenizer to build the backend
        slow_tokenizer = self.slow_tokenizer_class(*args, **kwargs)
        fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
    else:
        raise ValueError("...")
    self._tokenizer = fast_tokenizer
    # ...
```
fast tokenizer çš„å„ç±»æ–¹æ³•ä¾‹å¦‚ï¼š`tokenize`ã€`convert_tokens_to_ids`ã€`get_vocab`ã€`decode` æœ€ç»ˆéƒ½ä¼šç›´æ¥è½¬æ¢ä¸ºå¯¹ `self._tokenizer` çš„ç›¸åº”æ–¹æ³•çš„è°ƒç”¨ã€‚ä»å‰é¢å¯¹äº ğŸ¤— Tokenizers çš„ä»‹ç»å¯ä»¥çŸ¥é“ï¼Œ`self._tokenizer` å°è£…äº†è¿™äº›ç»„æˆéƒ¨åˆ†ï¼š`normalizer`ã€`pre_tokenizer`ã€`tokenizer`ã€`post_processor`ã€`decoder`ã€‚

ğŸ¤— Transformers ä¸­çš„æ¯ä¸ª slow tokenizer éœ€è¦é€ä¸€ç”¨pythonå®ç° `normalizer`ã€`pre_tokenizer`ã€`tokenizer`ã€`post_processor`ã€`decoder` è¿™äº›ç»„æˆéƒ¨åˆ†ï¼Œå…¶ä¸­ `tokenizer` æ˜¯ BPE æˆ–æ˜¯ WordPiece æ—¶ï¼Œåˆ™éœ€æ‰‹åŠ¨å®ç° encode çš„è¿‡ç¨‹ï¼Œå¦‚æœæ˜¯ SentencePiece æ—¶ï¼Œåˆ™ä¸€èˆ¬å€ŸåŠ© sentencepiece åŒ…æ¥å®ç°ä¸»è¦é€»è¾‘ã€‚


ğŸ¤— Transformers ä¸­, æ¯ä¸ªæ¨¡å‹éƒ½ä¼šå¯¹åº”äºå…¶ç‰¹æœ‰çš„ tokenizer, ä¾‹å¦‚: t5 æ¨¡å‹çš„ tokenizer ä¸º `T5Tokenizer` å’Œ `T5TokenizerFast`ã€‚ç»§æ‰¿å…³ç³»å¦‚ä¸‹ï¼š

![](../assets/figures/t5/tokenizer.png)

<div class="alert-red">
æ³¨æ„: slow ç‰ˆæœ¬çš„ tokenizer ä¸ fast ç‰ˆæœ¬çš„ tokenizer çš„è¡Œä¸ºæœªå¿…èƒ½å®Œå…¨ä¸€è‡´
</div>

### ä½¿ç”¨ã€TODO: éœ€è°ƒæ•´ã€‘

ä»ä¸€ä¸ªç–‘æƒ‘å¼•å…¥ï¼š[issue](https://github.com/huggingface/transformers/issues/5087)

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer
pretrained_name_or_path = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(pretrained_name_or_path)

print(tokenizer.special_tokens_map_extended)
# {"eos_token": "</s>", "unk_token": "<unk>", "pad_token": "<pad>", "additional_special_tokens": ['<extra_id_0>', ..., '<extra_id_99>']}
print(tokenizer.eos_token, tokenizer.unk_token, tokenizer.pad_token)
# eos_token: 1, unk_token: 2, pad_token: 0

text = "abc __"
tokens = tokenizer.tokenize(text)  # ["__ab", "c", "__", "_", "_"]
ids = tokenizer.convert_tokens_to_ids(tokens)  # [703, 75, 3, 834, 834]

ids = tokenizer.encode(text)  # [703, 75, 3, 834, 834, 1]
```

ç”±æ­¤å¯è§ï¼Œåœ¨ `T5Tokenizer` çš„å®ç°é‡Œï¼Œæ²¡æœ‰ `bos_token` è¿™ä¸ªå±æ€§ï¼Œå¹¶ä¸”æ¯ä¸ª word èµ·å§‹çš„ subword ä¼šåŠ ä¸Š `__` çš„å‰ç¼€ã€‚æ³¨æ„è¯è¡¨ä¸­æ—¢æœ‰ä»¥ `__` å¼€å¤´çš„ tokenï¼Œä¾‹å¦‚ `__ab`ï¼Œè€Œ `__` æœ¬èº«ä¹Ÿåœ¨è¯è¡¨ä¸­ã€‚è¿™ç§å¤„ç†æ–¹å¼æ˜¯å› ä¸º `T5Tokenizer` ä½¿ç”¨äº† SentencePiece Tokenizerã€‚

tokenizer çš„å¸¸ç”¨æ–¹æ³•å¦‚ä¸‹å‚è€ƒ[ç¬”è®°](https://buxianchen.gitbook.io/notes/note/dl/huggingface#pretrainedtokenizerbase)ã€åç»­è€ƒè™‘æ€ä¹ˆåˆå¹¶/åˆ å‡ã€‘ï¼Œè¿™é‡Œé‡ç‚¹ä»‹ç» `__call__` æ–¹æ³•çš„è¿”å›ç»“æœåšäº›ç‰¹æ®Šè¯´æ˜

### `BatchEncoding`

é¦–å…ˆçœ‹ä¸€ä¸ªä½¿ç”¨ç¤ºä¾‹ï¼š

```python
from transformers import AutoTokenizer  # AutoTokenizeræ€»æ˜¯å°è¯•åŠ è½½fastç‰ˆæœ¬çš„tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")
encodings = tokenizer(["This is a sentence"])
```

æ­¤å¤„çš„ `encodings` çš„ç±»å‹ä¸º `BatchEncoding` ç±»å‹, å®ƒç»§æ‰¿è‡ª `UserDict`ï¼Œå³ç»§æ‰¿è‡ªå­—å…¸ã€‚é™¤äº†å­—å…¸çš„æ–¹æ³•å¤–ï¼Œå®ƒè¿˜å…·å¤‡ä»¥ä¸‹æ–¹æ³•

```python
inputs = encoding.convert_to_tensors("pt")  # inplaceæ“ä½œ, å°†å†…éƒ¨çš„valueä¾‹å¦‚input_idsç­‰è½¬æ¢ä¸ºtensor
inputs = encoding.to("cuda:0")  # inplaceæ“ä½œ, æ”¹å˜è®¾å¤‡
# æ³¨: ä»¥ä¸‹æ–¹æ³•ä»…ä½¿ç”¨äº fast ç‰ˆæœ¬çš„ tokenizer çš„æƒ…å½¢
# æ³¨: ä»¥ä¸‹æ–¹æ³•å¯¹äºç‰¹æ®Štokenä¾‹å¦‚[CLS], è¿”å›ç»“æœä¼šå¤„ç†æˆNone
encoding.tokens()                     # List[str], æ‰€æœ‰çš„tokenå­—ç¬¦ä¸², å‡è®¾é•¿åº¦ä¸ºN
# tokenizeä¸€ä¸ªbatchçš„æ•°æ®æ—¶, æ³¨æ„éœ€è¦è°ƒæ•´å…¥å‚, encoding.tokens(i), iä¸ºç¬¬å‡ ä¸ªæ ·æœ¬, ä¸‹åŒ
encoding.word_ids()                   # List[int], æ¯ä¸ªtokenæ‰€åœ¨çš„word_idx, æ³¨æ„è¿™é‡Œçš„wordçš„æ¦‚å¿µé€šå¸¸å–å†³äºpre-tokenizerçš„å®šä¹‰
encoding.sequence_ids()               # List[int], æ¯ä¸ªtokenæ‰€åœ¨çš„sequence_idx, è¿™é‡Œçš„sequenceçš„æ¦‚å¿µå–å†³äºpre-tokenizerçš„å®šä¹‰
encoding.token_to_word(token_idx)     # ç¬¬token_idxä¸ªtokenæ‰€åœ¨çš„word_idx
encoding.token_to_sequence(token_idx) # ç¬¬token_idxä¸ªtokenæ‰€åœ¨çš„sequence_idx
start, end = encoding.word_to_chars(word_idx)      # ç¬¬word_idxä¸ªwordå¯¹åº”çš„åŸå§‹stringçš„èµ·å§‹/ç»“æŸä½ç½®
start, end = encoding.word_to_tokens(word_idx)     # ç¬¬word_idxä¸ªwordå¯¹åº”çš„èµ·å§‹ä¸ç»“æŸçš„token_idx
start, end = encoding.token_to_chars(token_idx)    # ç¬¬token_idxä¸ªtokenå¯¹åº”çš„åŸå§‹stringçš„èµ·å§‹/ç»“æŸä½ç½®
word_idx = encoding.char_to_word(i)                # åŸå§‹stringä¸­ç¬¬iä¸ªå­—ç¬¦å¯¹åº”çš„word_idx
token_idx = encoding.char_to_token(i)              # åŸå§‹stringä¸­ç¬¬iä¸ªå­—ç¬¦å¯¹åº”çš„token_idx
```

ç®€å•æ¥è¯´, fast ç‰ˆæœ¬çš„ tokenizer çš„ encode è¿‡ç¨‹ä¿å­˜äº†åŸå§‹å­—ç¬¦ä¸²ä¸­æ¯ä¸ªå­—ç¬¦ä¸token, word, sequenceçš„å¯¹åº”å…³ç³», è€Œ slow ç‰ˆæœ¬ä¸å…·å¤‡


### add tokens / special tokensã€TODOã€‘

æºç åŠå®˜æ–¹æ–‡æ¡£ä¸­å¯¹ç‰¹æ®Štokenæœ‰è‹¥å¹²ä¸ªç›¸ä¼¼çš„æ–¹æ³•ä¸å‘½åï¼Œè®©äººéå¸¸å›°æƒ‘ã€‚å¦å¤–ï¼Œ`PretrainedTokenizerBase` çš„ `add_tokens` ä¸ `add_special_tokens` çš„è¿™ä¸¤ä¸ªæ–¹æ³•ä¹Ÿè®©äººå›°æƒ‘ã€‚å› æ­¤æœ‰å¿…è¦ç†æ¸…æ¥šã€‚

é¦–å…ˆï¼Œè¿™é‡Œå¼•ç”¨ [SentencePiece æ•™ç¨‹](https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb) ä¸­çš„å‡ ä¸ªæœ¯è¯­å¯¹ token è¿›è¡Œåˆ†ç±»ï¼ˆåœ¨ğŸ¤— Transformersçš„æ–‡æ¡£ä¸­ï¼Œç¬”è€…æ²¡æœ‰è§åˆ°ç±»ä¼¼çš„æœ¯è¯­ï¼‰ï¼š

- normal symbols: æ™®é€šçš„ token, sentencepiece tokenizer å¯èƒ½ä¼šå°†è¿™ç§ token åˆ‡åˆ†å¼€
- user defined symbols: ç”¨æˆ·å¢åŠ çš„ç‰¹æ®Š token, å¯ä»¥å‡ºç°åœ¨åŸå§‹æ–‡æœ¬ä¸­, sentencepiece tokenizer ä¿è¯ä¸ä¼šå¯¹è¿™ç§ token è¿›è¡Œåˆ‡åˆ†
- control symbols: å¯¹tokenizerçš„ç»“æœè¿›è¡Œåå¤„ç†æ—¶ä½¿ç”¨çš„ token, ä¾‹å¦‚ï¼šsentencepiece tokenizer å°†å¥å­ tokenize å, åå¤„ç†åŠ ä¸Š `"[CLS]"` å’Œ `"[SEP]"`, å¦‚æœåœ¨è¾“å…¥çš„å¥å­ä¸­å«æœ‰ `"[CLS]"`, sentencepiece tokenizer æœ‰å¯èƒ½ä¼šå°†è¿™ç§ token åˆ‡åˆ†å¼€

ä»ä¸Šé¢çš„ä¾‹å­æ¨å¹¿å¼€æ¥, å¯¹ tokenizer å¢åŠ  token åº”è¯¥è¦åŒ…å«è¿™å‡ ç§æƒ…å½¢:

- æ™®é€štoken,å‡ºç°åœ¨åŸå§‹æ–‡æœ¬, ä¸ä¿è¯å®ƒä¸è¢«åˆ‡åˆ†å¼€: ä¾‹å¦‚: å‡è®¾è¯è¡¨ä¸­å·²ç»æœ‰äº† `"ä¸­å›½"` å’Œ `"äºº"` è¿™ä¸¤ä¸ªtokenï¼Œç°åœ¨å¢åŠ  `"ä¸­å›½äºº"` åˆ°è¯è¡¨é‡Œ, ç›®çš„æ˜¯å¸Œæœ› tokenizer æœ‰å¯èƒ½ä¼šå°† `"ä¸­å›½äºº"` å½“ä½œä¸€ä¸ªæ•´ä½“, å½“ç„¶ä¹Ÿä¸æ’é™¤ tokenizer ä»ç„¶ä¼šè¢«åˆ‡åˆ†ä¸º `"ä¸­å›½"` å’Œ `"äºº"`ã€‚ç„¶è€Œåœ¨ BPEã€WordPieceã€Unigram è¿™ä¸‰ç±»ç®—æ³•ä¸­ï¼Œä¸ºäº†å¢åŠ è¿™ç§ç±»å‹çš„tokenï¼Œ
  - BPE éœ€è¦å¢åŠ çš„æ˜¯ merge è§„åˆ™, å³ `("ä¸­å›½", "äºº")`, ç”šè‡³äºéœ€è¦è°ƒæ•´è¿™ä¸ª merge çš„è§„åˆ™åˆ°åˆé€‚çš„ä½ç½®(ä¼˜å…ˆçº§)
  - WordPiece åªéœ€è¦å°† `"ä¸­å›½äºº"` åŠ å…¥åˆ°è¯è¡¨ä¸­å³å¯
  - Unigram éœ€è¦å°† `"ä¸­å›½äºº"` ä»¥åŠç›¸åº”çš„æ¦‚ç‡å€¼åŠ å…¥è‡³è¯è¡¨é‡Œ, ç”šè‡³äºéœ€è¦è°ƒæ•´å·²æœ‰è¯çš„æ¦‚ç‡å€¼
  å› æ­¤ ğŸ¤— Transformers ä¸­ä¸æ”¯æŒè¿™ç§æ·»åŠ æ–¹å¼(slow tokenizerä¸æ”¯æŒ, ä¸ç¡®å®š fast tokenizer çš„æƒ…å†µ)
- å‡ºç°åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„token, ä¿è¯å®ƒä¸ä¼šè¢«åˆ‡åˆ†å¼€ï¼ˆğŸ¤— Transformers æ”¯æŒï¼‰
- åå¤„ç†token, ä¸»è¦ç”¨é€”ç”¨äºåå¤„ç†æ—¶è¿½åŠ ã€‚å¹¶ä¸”å³ä½¿å®ƒå‡ºç°åœ¨åŸå§‹æ–‡æœ¬ä¸­, ä¹Ÿä¸ä¼šåˆ‡åˆ†å¼€ï¼ˆğŸ¤— Transformers çš„EOSç­‰éƒ½æœ‰æ­¤æ€§è´¨ï¼‰
- åå¤„ç†token, ä¸»è¦ç”¨é€”ç”¨äºåå¤„ç†æ—¶è¿½åŠ ã€‚ä½†å¦‚æœå®ƒå‡ºç°åœ¨åŸå§‹æ–‡æœ¬ä¸­, æœ‰å¯èƒ½ä¼šè¢«åˆ‡åˆ†å¼€ï¼ˆğŸ¤— Transformers ä¸æ”¯æŒï¼‰

ã€è¿˜éœ€è¦æ€»ç»“ä¸€ä¸‹è·Ÿspecial tokenç›¸å…³çš„å®ä¾‹å˜é‡åã€‘

æœ‰äº†ä¸Šè¿°è®¤çŸ¥ï¼Œä¸‹é¢å…·ä½“åˆ†ææºä»£ç 

`PrtrainedTokenizerBase` ä¸ `add_tokens` å’Œ `add_special_tokens` ä¸­æœ‰å…³çš„ä»£ç ç‰‡æ®µå¦‚ä¸‹
```python
class SpecialTokensMixin:
    SPECIAL_TOKENS_ATTRIBUTES = [
        "bos_token",
        "eos_token",
        "unk_token",
        "sep_token",
        "pad_token",
        "cls_token",
        "mask_token",
        "additional_special_tokens",
    ]
    def add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, AddedToken]], replace_additional_special_tokens=True) -> int:
        if not special_tokens_dict:
            return 0
        added_tokens = 0
        for key, value in special_tokens_dict.items():
            assert key in self.SPECIAL_TOKENS_ATTRIBUTES, f"Key {key} is not a special token"
            if key == "additional_special_tokens":
                assert isinstance(value, (list, tuple)) and all(isinstance(t, (str, AddedToken)) for t in value)
                if replace_additional_special_tokens:
                    setattr(self, key, value)
                else:
                    # This is a copy of `self._additional_special_tokens`
                    additional_special_tokens = getattr(self, key)
                    additional_special_tokens_set = set(additional_special_tokens)
                    to_add = []
                    for token in value:
                        if str(token) not in additional_special_tokens_set and str(token) not in to_add:
                            to_add.append(token)
                    # update the property
                    additional_special_tokens.extend(to_add)
                    self.additional_special_tokens = additional_special_tokens
                added_tokens += self.add_tokens(value, special_tokens=True)
            else:
                assert isinstance(value, (str, AddedToken))
                setattr(self, key, value)
                added_tokens += self.add_tokens([value], special_tokens=True)
        return added_tokens
    def add_tokens(self, new_tokens: Union[str, AddedToken, List[Union[str, AddedToken]]], special_tokens: bool = False) -> int:
        if not new_tokens:
            return 0
        if not isinstance(new_tokens, (list, tuple)):
            new_tokens = [new_tokens]
        return self._add_tokens(new_tokens, special_tokens=special_tokens)
    def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:
        raise NotImplementedError
```

ç”±æ­¤å¯è§:
- `add_special_tokens` å®é™…ä¸Šåªæ˜¯ç”¨æ¥æ“ä½œ `[bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token]` ä»¥åŠ `additional_special_tokens` è¿™å‡ ä¸ªå±æ€§çš„, ä»ã€å…¶ä»–ã€‘åˆ†æå¯ä»¥çŸ¥é“ `additional_special_tokens` è·Ÿå‰é¢ 6 ç§ token å¹¶æ²¡æœ‰æœ¬è´¨åŒºåˆ«ã€‚è€Œ `add_special_tokens` çš„è¡Œä¸ºæ˜¯ç»™è¿™ 7 ä¸ªå®ä¾‹å˜é‡èµ‹æ–°å€¼, ç„¶åå†è°ƒç”¨ `added_tokens`
- `add_tokens` çš„è¡Œä¸ºå®Œå…¨ç”± `_add_tokens` å†³å®š, ç”±åŸºç±»å®ç°

å¯¹äº slow tokenizer, `_add_tokens` çš„å®ç°å¦‚ä¸‹

```python
def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:
    new_tokens = [str(tok) for tok in new_tokens]
    tokens_to_add = []
    for token in new_tokens:
        if not isinstance(token, str):
            raise TypeError(f"Token {token} is not a string but a {type(token)}.")
        if not special_tokens and hasattr(self, "do_lower_case") and self.do_lower_case:
            token = token.lower()
        if (
            token != self.unk_token
            and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token)
            and token not in tokens_to_add
        ):
            tokens_to_add.append(token)
    added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(tokens_to_add))
    added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}
    self.added_tokens_encoder.update(added_tok_encoder)
    self.added_tokens_decoder.update(added_tok_decoder)

    # Make sure we don't split on any special tokens (even they were already in the vocab before e.g. for Albert)
    if special_tokens:
        if len(new_tokens) == 1:
            _insert_one_token_to_ordered_list(self.unique_no_split_tokens, new_tokens[0])
        else:
            self.unique_no_split_tokens = sorted(set(self.unique_no_split_tokens).union(set(new_tokens)))
    else:
        # Or on the newly added tokens
        if len(tokens_to_add) == 1:
            _insert_one_token_to_ordered_list(self.unique_no_split_tokens, tokens_to_add[0])
        else:
            self.unique_no_split_tokens = sorted(set(self.unique_no_split_tokens).union(set(tokens_to_add)))
    self._create_trie(self.unique_no_split_tokens)
    return len(tokens_to_add)
```

å› æ­¤ slow tokenizer `add_tokens` æ–¹æ³•çš„æµç¨‹å¦‚ä¸‹:

- é€šè¿‡ `convert_tokens_to_ids(token)==convert_tokens_to_ids(self.unk_token)` åˆ¤æ–­æ˜¯å¦ä¸ºæ–°å¢è¯, å¦‚æœæ˜¯, åˆ™åœ¨ `self.added_tokens_encoder` ä»¥åŠ `self.added_tokens_decoder` ä¸­è®°å½• token to idx å’Œ idx to token çš„æ˜ å°„å…³ç³», æ³¨æ„è¿™ä¸¤ä¸ªå®ä¾‹å˜é‡æ˜¯ slow tokenizer ç‹¬æœ‰çš„, fast tokenizer æ— æ­¤å®ä¾‹å˜é‡ã€è¿˜éœ€è¦åœ¨å…¶ä»–åœ°æ–¹ä»‹ç»è¿™ä¸¤ä¸ªå®ä¾‹å˜é‡ã€‘
- å…¥å‚ `special_tokens=True`, é‚£ä¹ˆå°±ä¸è¿›è¡Œå‰ä¸€æ­¥ç­›é€‰, ç›´æ¥å°†å…¥å‚ `new_tokens` ä½œä¸ºä¸å¯åˆ†å‰²çš„ token åŠ å…¥åˆ°è¯è¡¨ä¸­ã€è¿˜éœ€è¦åœ¨å…¶ä»–åœ°æ–¹ä»‹ç»self.unique_no_split_tokensã€‘ã€‚å…¥å‚ `special_tokens=False`, é‚£ä¹ˆå°±éœ€è¦ç»è¿‡å‰ä¸€æ­¥ç­›é€‰å†ä½œä¸ºä¸å¯åˆ†å‰²çš„ token åŠ å…¥åˆ°è¯è¡¨ä¸­
- è°ƒç”¨ `self._create_trie`, ä¾¿äºtokenizeçš„æ—¶å€™å…ˆä¿è¯ä¸å¯åˆ†å‰²çš„è¯ä¸è¢«åˆ‡å¼€

å› æ­¤, `add_tokens` æœ¬è´¨ä¸Šæ˜¯æ·»åŠ äº†ä¸å¯åˆ†å‰²çš„ token, `special_tokens` è¿™ä¸ªå…¥å‚æ˜¾å¾—ä¸å®¹æ˜“ç†è§£

å¯¹äº fast tokenizer, `_add_tokens` çš„å®ç°å¦‚ä¸‹
```python
def _add_tokens(self, new_tokens: List[Union[str, AddedToken]], special_tokens=False) -> int:
    if special_tokens:
        return self._tokenizer.add_special_tokens(new_tokens)
    return self._tokenizer.add_tokens(new_tokens)
```
æ‰€ä»¥æœ¬è´¨ä¸Šå›åˆ°äº† ğŸ¤— Tokenizers ä¸­ `tokenizers.Tokenizer` çš„ä¸¤ä¸ªæ–¹æ³•: `add_special_tokens`, `add_tokens`ã€éœ€è¿›ä¸€æ­¥ææ¸…æ¥šï¼Œå¹¶ç¡®å®šè¡Œä¸ºæ˜¯å¦ä¸slowç‰ˆæœ¬çš„ä¸€è‡´ã€‘

### `PretrainedTokenizerBase.__call__`ã€TODOã€‘

å®˜æ–¹å»ºè®®ä¸è¦ç›´æ¥è°ƒç”¨ `batch_encode_plus`ï¼Œ`encode_plus` æ–¹æ³•ï¼Œè€Œæ˜¯é€šè¿‡ `__call__` æ–¹æ³•æ¥è°ƒç”¨ã€‚è¿™ä¸€è¿‡ç¨‹å®é™…èµ·ä½œç”¨çš„â€œç»„ä»¶â€å‡½æ•°ä¸ºï¼š`encode`ã€`convert_tokens_to_ids`ã€`prepare_for_model`ã€‚

ä¸åšè¯´æ˜çš„æƒ…å†µä¸‹ï¼Œé»˜è®¤æŒ‡çš„æ˜¯`PretrainedTokenizerBase`çš„æ–¹æ³•ï¼Œé¦–å…ˆå¯¹ `__call__` æ–¹æ³•çš„é‡è¦çš„è¾“å…¥å‚æ•°åšä»‹ç»

```python
def __call__(
    self,
    text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,
    text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,
    text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,
    text_pair_target: Optional[
        Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]
    ] = None,
    add_special_tokens: bool = True,  # æ˜¯å¦éœ€è¦åœ¨tokenizeä¹‹åæ·»åŠ ä¸€äº›ç‰¹æ®Štoken(ä¾‹å¦‚èµ·å§‹ç»“æŸtoken), éœ€å…·ä½“çš„tokenizerå®ç°ï¼Œå…·ä½“è§åé¢è¯´æ˜
    padding: Union[bool, str, PaddingStrategy] = False,  # padding, truncation, max_lengthè§åé¢è¯´æ˜
    truncation: Union[bool, str, TruncationStrategy] = None,
    max_length: Optional[int] = None,
    is_split_into_words: bool = False,  # è§åé¢è¯´æ˜
    return_overflowing_tokens: bool = False,  # è¿”å›è¢«æˆªæ–­çš„éƒ¨åˆ†
    return_offsets_mapping: bool = False,  # è¿”å›æ¯ä¸ªtoken_idå¯¹åº”äºåŸå§‹æ–‡æœ¬çš„èµ·å§‹ä½ç½®(slow tokenizerä¸æ”¯æŒæ­¤ç‰¹æ€§)
    ...
) -> BatchEncoding:
    # ä»¥ä¸‹ä¸ºå¤§ä½“é€»è¾‘, æœ‰åˆ æ”¹
    if text is not None:
        if not self._in_target_context_manager:
            self._switch_to_input_mode()
        encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
    if text_target is not None:
        self._switch_to_target_mode()
        target_encodings = self._call_one(text=text_target, text_pair=text_pair_target, **all_kwargs)
    self._switch_to_input_mode()
    if text_target is None:
        return encodings
    elif text is None:
        return target_encodings
    else:  # sourceå’Œtargetéƒ½ç»™çš„æ—¶å€™, åªæŠŠtarget encodingsç»“æœä¸­çš„input_idsä½œä¸ºlabelsæ·»åŠ åˆ°sourceçš„encodingç»“æœé‡Œ
        encodings["labels"] = target_encodings["input_ids"]
    return encodings
# æ³¨æ„ text_pair æŒ‡çš„æ˜¯ç¬¬2ä¸ªå¥å­, è€Œéå¥å­å¯¹

# text/text_pair/text_target/text_pair_targetçš„æ•°æ®ç±»å‹ä¸ºä»¥ä¸‹4ç§æƒ…å†µ:
TextInput = str  # å³æ•´å¥è¯, è½¬ä¸ºtokenåºåˆ—
List[TextInput] = List[str] # batchç‰ˆæœ¬, å¤šå¥è¯åˆ†åˆ«è½¬ä¸ºtokenåºåˆ—
PreTokenizedInput = List[str]  # å·²ç»é¢„å…ˆåˆ‡å¥½"è¯"çš„åºåˆ—, è¿™ä¸ªæ—¶å€™ä¼šå¯¹æ¯ä¸ªå°æ®µè¿›è¡ŒtokenåŒ–, æœ€åæ‹¼æ¥åœ¨ä¸€èµ·
List[PreTokenizedInput] = List[List[str]]  # å¤šä¸ªå·²ç»ä¸”ä¸ºå°æ®µçš„å¥å­
# ä¸€ä¸ª PreTokenizedInput çš„ä½¿ç”¨ä¾‹å­æ˜¯: ["ç´«ç¦åŸ", "æ˜¯xxx,åè½äº", "åŒ—äº¬"]
# å¾—åˆ°çš„åºåˆ—ä¼šæ˜¯ List[int] = tokenize("ç´«ç¦åŸ") + tokenize(æ˜¯xxx,åè½äº) + tokenize("åŒ—äº¬")
# ä¿è¯å‘½åå®ä½“æœ¬èº«ä¸ä¼šè¢«åˆ‡åˆ†å¼€æ¥
```

å¯¹è¾“å…¥å‚æ•°åšç®€è¦è§£é‡Šå¦‚ä¸‹

**`text`, `text_pair`, `text_target`, `text_pair_target`**

éœ€è¦è¢«åºåˆ—åŒ–ä¸ºæ•´æ•°çš„â€œä¸œè¥¿â€ï¼ŒæŸäº› tokenizer å¯¹ source(è¾“å…¥) å’Œ target(è¾“å‡º) çš„åºåˆ—åŒ–æ–¹å¼å¯èƒ½æœ‰æ‰€ä¸åŒ, æ‰€ä»¥ç•™äº†

**`add_special_tokens`**

è¡¨ç¤ºæ˜¯å¦éœ€è¦åœ¨tokenizeä¹‹åæ·»åŠ ä¸€äº›ç‰¹æ®Štoken(ä¾‹å¦‚èµ·å§‹ç»“æŸtoken), é»˜è®¤å€¼ä¸ºTrueï¼Œè¿™ä¸ªå‚æ•°åœ¨ `prepare_for_model` ä¸­ç”¨åˆ°ï¼Œä¸åŒçš„ tokenizer éœ€è¦é€šè¿‡é‡è½½å¦‚ä¸‹å‡ ä¸ªæ–¹æ³•è¿›è¡Œå®ç°ï¼š
  ```python
  def prepare_for_model(self, ...):
      # å‰åºå¤„ç†çœç•¥, ä¸»è¦åŒ…æ‹¬truncate

      # Add special tokens
      if add_special_tokens:
          sequence = self.build_inputs_with_special_tokens(ids, pair_ids)  # è¿½åŠ ç‰¹æ®Štoken
          token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)  # é€šå¸¸ç¬¬1å¥è¯çš„ä½ç½®ä¸º0ï¼Œç¬¬2å¥çš„ä½ç½®ä¸º1
      else:
          sequence = ids + pair_ids if pair else ids
          token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])

      # Build output dictionary
      encoded_inputs["input_ids"] = sequence
      if return_token_type_ids:
          encoded_inputs["token_type_ids"] = token_type_ids
      if return_special_tokens_mask:
          if add_special_tokens:
              encoded_inputs["special_tokens_mask"] = self.get_special_tokens_mask(ids, pair_ids)
          else:
              encoded_inputs["special_tokens_mask"] = [0] * len(sequence)
      # åç»­å¤„ç†ä¸»è¦æ˜¯pad
  
  # ä¸Šé¢å‡ ä¸ªæ–¹æ³•çš„é»˜è®¤å®ç°å¦‚ä¸‹:
  def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1):
      if token_ids_1 is None:
          return token_ids_0
      return token_ids_0 + token_ids_1
  def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1):
      if token_ids_1 is None:
          return len(token_ids_0) * [0]
      return [0] * len(token_ids_0) + [1] * len(token_ids_1)
  def get_special_tokens_mask(self, token_ids_0, token_ids_1, already_has_special_tokens=False):
      # 1 ä»£è¡¨ special token, 0 ä»£è¡¨æ™®é€šçš„ token
      all_special_ids = self.all_special_ids  # cache the property
      special_tokens_mask = [1 if token in all_special_ids else 0 for token in token_ids_0]
      return special_tokens_mask

  ```
  ä¾‹å­: `BertTokenizer`ï¼Œ`T5Tokenizer` å‡å¯¹ `build_inputs_with_special_tokens`ã€`create_token_type_ids_from_sequences`ã€`get_special_tokens_mask`åšé‡è½½ã€‚å¹¶ä¸”ä¹Ÿæ˜¯è¿™ä¸¤ä¸ª tokenizer é™¤äº†å¿…é¡»å®ç°çš„5ä¸ªæ–¹æ³• `save_vocabulary`ã€`get_vocab`ã€`_tokenize`ã€`_convert_token_to_id`ã€`convert_id_to_token` ä»¥å¤–çš„å…¨éƒ¨é‡è½½æ–¹æ³•ã€‚ï¼ˆå¯¹äºdecodeè¿‡ç¨‹ï¼Œ`T5Tokenizer`è¿˜é‡è½½äº†`convert_tokens_to_string`ï¼‰

**`padding`ã€`truncate`ã€`max_length`ã€`is_split_into_words`**

is_split_into_words ä¸è°ƒç”¨ `_batch_encode_plus` è¿˜æ˜¯è°ƒç”¨ `_encode_plus` æ˜¯ç›¸å…³çš„ã€å¾…è¡¥å……ã€‘

å¤‡æ³¨ï¼š

- `truncation_side` å–å€¼ä¸º `"left"` è¡¨ç¤ºæˆªæ–­æ—¶å»æ‰å·¦è¾¹çš„å­—ç¬¦ï¼Œå–å€¼ä¸º `"right"` è¡¨ç¤ºæˆªæ–­æ—¶å»æ‰å³è¾¹çš„å­—ç¬¦

**`__call__`æ–¹æ³•çš„è°ƒç”¨æµç¨‹**

`__call__` æ–¹æ³•çš„å…·ä½“æµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆå°†éœ€è¦è½¬æ¢ä¸º token åºåˆ—çš„è¾“å…¥åˆ†ä¸ºä¸¤ç»„ `text, text_pair` å’Œ `text_target, text_pair_target`ï¼Œåˆ†åˆ«è°ƒç”¨ `_call_one` æ–¹æ³•ï¼Œç„¶åå°†ä¸¤éƒ¨åˆ†è¿›è¡Œåˆå¹¶ã€‚è€Œ `_call_one` æ–¹æ³•æ ¹æ® `text` æˆ– `text_target` çš„å˜é‡ç±»å‹ä»¥åŠ `is_split_into_words` å‚æ•°çš„å–å€¼ç¡®å®šè¿›ä¸€æ­¥è°ƒç”¨ä¸¤è€…ä¹‹ä¸€: `batch_encode_plus` æˆ–æ˜¯ `encode_plus`ï¼Œæ­¤æ—¶æ³¨æ„è¿™ä¸¤ä¸ªå‡½æ•°çš„å‡½æ•°ç­¾åå¦‚ä¸‹:

```python
EncodedInput=List[int]
EncodedInputPair=Tuple[List[int], List[int]]

def batch_encode_plus(
    self,
    batch_text_or_text_pairs: Union[
        List[TextInput],  # List[str]
        List[TextInputPair],  # List[Tuple[str, str]]
        List[PreTokenizedInput],  # List[List[str]]
        List[PreTokenizedInputPair],  # List[Tuple[List[str], List[str]]]
        List[EncodedInput],  # å¦‚æœåªçœ‹ __call__ æ–¹æ³•docstring, åœ¨è°ƒç”¨__call__æ–¹æ³•æ—¶, ä¸å¯èƒ½ä»¥è¿™ç§å˜é‡ç±»å‹è§¦å‘batch_encode_plusæ–¹æ³•
        List[EncodedInputPair],  # å¦‚æœåªçœ‹ __call__ æ–¹æ³•docstring, åœ¨è°ƒç”¨__call__æ–¹æ³•æ—¶, ä¸å¯èƒ½ä»¥è¿™ç§å˜é‡ç±»å‹è§¦å‘batch_encode_plusæ–¹æ³•
    ],
    ...
) -> BatchEncoding:
    ...

def encode_plus(
    self,
    text: Union[TextInput, PreTokenizedInput, EncodedInput],  # åŒç†EncodeInputè¿™ç§ç±»å‹æŒ‰ç†ä¹Ÿä¸ä¼šè§¦å‘
    text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,
    ...
) -> BatchEncoding:
    ...
```

è€Œè¿™ä¸¤ä¸ªæ–¹æ³•é¦–å…ˆæ ¹æ®è¾“å…¥å‚æ•° `padding`ã€`truncate`ã€`max_length` å¤„ç†å¥½ï¼ˆè½¬æ¢æˆç›¸åº”çš„æšä¸¾ç±»å‹ï¼Œç”¨æˆ·å¦‚æœä¸ä¼  max_lengthï¼Œè¿™ä¸€æ­¥ä¹Ÿä¼šå°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªç¡®ç¡®å®å®çš„æ•´æ•°ï¼‰ï¼Œç„¶åç»§ç»­è°ƒç”¨ `_batch_encode_plus` æˆ– `_encode_plus` (è¿™ä¸¤ä¸ªæ–¹æ³•åœ¨å­ç±» `PretrainedTokenizer` å’Œ `PretrainedTokenizerFast` ä¸­åˆ†åˆ«å®ç°) ï¼Œä»¥ slow ç‰ˆæœ¬çš„ä¸ºä¾‹ï¼Œå®ƒä»¬å®é™…åšçš„äº‹æƒ…å¯ä»¥å‚è€ƒå¦‚ä¸‹ç®€åŒ–ç‰ˆæœ¬çš„æºä»£ç å®ç°ï¼š

```python
# ç®€åŒ–ç‰ˆæœ¬(åªè€ƒè™‘textä¸ºstrç±»å‹, ä¸è€ƒè™‘List[str]ç±»å‹)
def _encode_plus(self, text, text_pair):
    # tokenizeæ–¹æ³•å†…éƒ¨ä¾æ¬¡è°ƒç”¨: ä¸€æ¬¡ prepare_for_tokenization å’Œå¤šæ¬¡ _tokenize å®Œæˆ
    # _tokenizeæ–¹æ³•å¿…é¡»åœ¨å…·ä½“çš„tokenizerä¸­å®ç°
    # tokenizeçš„å¤§ä½“é€»è¾‘ä¸º: æŠŠä¸å¯æ‹†åˆ†çš„tokenæŠ½å‡ºæ¥, å…¶ä½™çš„è°ƒç”¨_tokenizeæ¥å®Œæˆ
    # ä¾‹å¦‚: "æˆ‘åœ¨<extra_001>é©¬è·¯è¾¹" => ["æˆ‘åœ¨", "<extra_001>", "é©¬è·¯è¾¹"]
    # => [_tokenize("æˆ‘åœ¨"), 32001, _tokenize("é©¬è·¯è¾¹")] = [34, 567, 32001, 76, 98]
    first_tokens = self.tokenize(text, **kwargs)
    first_ids = self.convert_tokens_to_ids(first_tokens)
    second_tokens = self.tokenize(text_pair, **kwargs)
    second_ids = self.convert_tokens_to_ids(second_tokens)
    # prepare_for_model éœ€è¦åšåå¤„ç†: é¦–å°¾åŠ ç‰¹æ®Štoken, è·å–
    return self.prepare_for_model(first_ids, second_ids, **kwargs)  # è¿™ä¸ªæ–¹æ³•å®šä¹‰åœ¨çˆ¶ç±»æ–¹æ³•ä¸­, è§å‰é¢å…³äº__call__çš„å…¥å‚è§£é‡Šéƒ¨åˆ†

# ç®€åŒ–ç‰ˆæœ¬(åªè€ƒè™‘batch_text_or_text_pairsä¸ºList[tuple[str, str]]çš„æƒ…å†µ)
def _batch_encode_plus(self, batch_text_or_text_pairs):
    input_ids = []
    for text_or_text_pair in batch_text_or_text_pairs:
        text, text_pair = text_or_text_pair
        first_tokens: List[str] = self.tokenize(text, **kwargs)
        first_ids: List[int] = self.convert_tokens_to_ids(first_tokens)
        second_tokens = self.tokenize(text_pair, **kwargs)
        second_ids = self.convert_tokens_to_ids(second_tokens)
        input_ids.append((first_ids, second_ids))
    return self._batch_prepare_for_model(input_ids, **kwargs)

def _batch_prepare_for_model(input_ids):
    batch_out = defaultdict(list)
    for first_ids, second_ids in input_ids:
        outputs: BatchEncoding = self.prepare_for_model(first_ids, second_ids)
        for key, value in outputs.items():
            batch_out[key].append(value)
    return BatchEncoding(self.pad(batch_out))
```

ä¸Šé¢çš„æºç ä¸­, æ¶‰åŠåˆ° `PretrainedTokenizer` çš„ `tokenize`ã€`convert_tokens_to_ids`ã€`prepare_for_model` æ–¹æ³•ï¼Œæ­¤å¤„å†åšä¸€äº›å±•å¼€è¯´æ˜ï¼š
```python

```


### T5Tokenizerã€TODOã€‘

æœ¬èŠ‚ä»¥ `T5Tokenizer` ä¸ºä¾‹, ä»‹ç»å¦‚ä½•å†™ä¸€ä¸ª slow tokenizer

### T5TokenizerFastã€TODOã€‘

æœ¬èŠ‚ä»¥ `T5TokenizerFast` ä¸ºä¾‹, ä»‹ç»å¦‚ä½•å†™ä¸€ä¸ª fast tokenizer

### è®­ç»ƒä¸€ä¸ª Tokenizer

åœ¨ ğŸ¤— Transformers åº“ä¸­, fast ç‰ˆæœ¬çš„ tokenizer å®é™…ä¸Šåˆ©ç”¨äº† ğŸ¤— Tokenizers çš„ä¸€äº›å†…å®¹, å› æ­¤ `PretrainedTokenizerFast` æ˜¯å¯ä»¥è®­ç»ƒçš„ï¼Œè€Œ slow ç‰ˆæœ¬çš„ tokenizer ä¸æ”¯æŒè®­ç»ƒã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```python
from transformers import AutoTokenizer
old_tokenizer = AutoTokenizer.from_pretrained("t5-small")
training_corpus: List[str] = ["sentence one", "sentence one"]
training_corpus = ([training_corpus[i*32: (i+1)*32]] for i in range(100))  # è¿­ä»£å™¨å³å¯
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
tokenizer.save_pretrained("save-dir")
```

æ³¨æ„ï¼šè¿™ç§åšæ³•é€‚ç”¨äºä¸ç°æœ‰çš„ä¸€ä¸ª tokenizer ä¸€è‡´çš„è®¾å®š, ä¾‹å¦‚ï¼šBOS tokenç­‰, ç»å¤§å¤šæ•°æƒ…å†µä¸‹, å·²ç»è¶³å¤Ÿä½¿ç”¨ã€‚å¦‚æœç¡®å®éœ€è¦åšæ¯”è¾ƒå¤§çš„è°ƒæ•´ï¼Œåˆ™éœ€è¦å€ŸåŠ© ğŸ¤— Tokenizers åŒ…ï¼ˆè§å‰æ–‡ä»‹ç»ï¼‰ã€‚


### æ‚é¡¹ã€TODO: éœ€è°ƒæ•´ã€‘

fast ç‰ˆæœ¬çš„ tokenizer å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼æŸ¥çœ‹å…¶èƒŒåçš„ tokenizer ç±»å‹ï¼š

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("chinese-roberta-wwm-ext", use_fast=False)
json.loads(tokenizer._tokenizer.to_str())["model"]["type"]
```

## åŸç†è§£æï¼šT5 è®­ç»ƒè¿‡ç¨‹çš„å‰å‘è®¡ç®—æµç¨‹

### encoder

é¦–å…ˆç»™å‡ºæ€»ä½“çš„ç»“æ„å›¾

![](../assets/figures/t5/T5_encoder.png)

T5 æ¨¡å‹çš„ Encoder éƒ¨åˆ†ç”±è‹¥å¹²ä¸ª Block æ„æˆï¼Œæ¯ä¸ª Block éƒ½å…·æœ‰ç›¸åŒçš„ç»“æ„ï¼šä¸€ä¸ª Self-Attention Layer å’Œä¸€ä¸ª Feed-Forward Layerã€‚è¿™é‡Œä¹Ÿé¦–å…ˆç»™å‡ºä¼ªä»£ç ï¼š

```python
class Encoder:
    def forward(self, x_token, x_attention_mask):
        # x_token: (B, L=512), long
        # x_attention: (B, L), 0/1 mask
        x_embedding = embedding_layer(x_token)
        hidden = dropout(x_embedding)  # (B, L, C=768)
        
        positional_bias = None
        for block in blocks:
            hidden_1 = block.layernorm_layer(hidden)  # LayerNormå±‚, hidden_1: (B, L, C)
            # Self-Attentionå±‚, attention_hidden: (B, L, C), postional_bias: (1, n_heads, L, L)
            # postional_biasåœ¨ç¬¬ä¸€å±‚è¢«äº§ç”Ÿ, åé¢æ¯ä¸€å±‚éƒ½ä½¿ç”¨å®ƒ(å…±äº«å‚æ•°)
            attention_hidden, positional_bias = block.attention_layer(hidden_1, x_attention_mask, positional_bias)
            hidden = block.dropout(attention_hidden) + hidden  # æ®‹å·®è¿æ¥: hidden: (B, L, C)
            
            hidden = block.ff_layer(hidden)  # Feed-Forwardå±‚: hidden (B, L, C)
        
        hidden = layernorm_layer(hidden)  # hidden (B, L, C)
        hidden = dropout(hidden)  # hidden (B, L, C)
        return hidden
```

å¤‡æ³¨ï¼šåœ¨ ğŸ¤— Transformers çš„å®ç°ä¸­ï¼Œå°†æ­¤å¤„çš„ `block.layernorm_layer`, `block.attention_layer`ã€`block.dropout` çš„è®¡ç®—é€»è¾‘åŒ…è£…åœ¨äº†ä¸€èµ·ï¼Œç§°ä¸º `T5LayerSelfAttention`ã€‚è€Œæ­¤å¤„çš„ `block.ff_layer` ä¸º `T5LayerFF`ã€‚

#### LayerNorm Layer (Encoder)

```python
class LayerNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        # T5ç”¨çš„æ˜¯ç®€åŒ–ç‰ˆçš„layernormå¯¹æœ€åä¸€ç»´l2å½’ä¸€åŒ–åå†æ¯ä¸€ç»´ä¹˜ä¸Šä¸€ä¸ªæƒé‡, ä¸å¸¦åç½®é¡¹
        # hidden_states: (B, L, C)
        # return: (B, L, C)
        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states
```

#### Self-Attention Layer (Encoder)

**relative positional embedding**

æ€»å…±çš„ postional embedding æ•°ç›®ä¸º (num_bucket, n_head), T5 çš„ postional embedding çš„ index çš„å–å€¼èŒƒå›´ä¸º [0, num_bucket)

åŒå‘ mask çš„æƒ…å†µä¸‹, $n=num\_bucket, m=max\_distance$

$$
\begin{equation*}
index(i, j) = \frac{n}{2} * \mathbb{1}[i-j<0] + \left\{
\begin{aligned}
    &abs(i - j), &abs(i - j) < \frac{n}{4} \\
&\min(\frac{n}{2}-1, \frac{n}{4}\times(1+\frac{log(4\times abs(i - j)/n)}{log(4\times m/n)})), &abs(i - j) \ge \frac{n}{4}

\end{aligned}
\right.
\end{equation*}
$$

```python
def relative_position_bidirectional(i, j, num_buckets=32, max_distance=128):
    position = i - j
    abs_position = abs(position)
    num_buckets = num_buckets // 2
    max_exact = num_buckets // 2
    offset = num_buckets if position < 0 else 0
    if abs_position < max_exact:
        return abs_position + offset
    else:
        ratio = math.log(abs_position/ max_exact) / math.log(max_distance / max_exact)
        return min(int(max_exact*(1+ratio)), num_buckets - 1) + offset
```

casual mask çš„æƒ…å†µä¸‹,

$$
\begin{equation*}
index(i, j) = 
\left\{
\begin{aligned}

&0, &i \ge j \\
&abs(i - j), &i < j\ and\ abs(i - j) < \frac{n}{2} \\
&\min(n-1, \frac{n}{2}\times(1+\frac{log(2\times abs(i - j)/n)}{log(2\times m/n)})), &i < j\ and\ abs(i - j) \ge \frac{n}{2}

\end{aligned}
\right.
\end{equation*}
$$

```python
def relative_position_onedirectional(i, j, num_buckets=32, max_distance=128):
    position = i - j
    if position <= 0:
        return 0
    elif position < (num_buckets // 2):
        return position
    else:
        ratio = math.log(2 * position / num_buckets) / math.log(2 * max_distance / num_buckets)
        return min(int(num_buckets // 2 * (1 + ratio)), num_buckets - 1)
```

åœ¨ T5 æ¨¡å‹çš„å®éªŒè®¾ç½®ä¸­:

```python
num_bucket, max_distance = 32, 128
```

åœ¨ encoder ä¸ decoder çš„ç¬¬ä¸€å±‚åŠ ä¸Šäº† positional bias:

```python
bias = nn.Embedding(num_buckect, n_heads)
positional_idx = ...  # å³ä¸Šé¢çš„å…¬å¼, (L, L)
scores = q @ k.T  # (B, L, L, n_heads)
positional_bias = bias(positional_idx)  # (L, L, n_heads)
scores += positional_bias
# weights = softmax(scores)
```

**self-attention**

```python
class EncoderSelfAttention(torch.nn.Module):
    def __init__(self, d_model=768, d_qkv=64, n_heads=12,
        relative_attention_num_buckets=32, has_relative_bias=False, dropout_rate=0.1):
        """
        relative_attention_num_buckets: è§åé¢å…³äºpositional biasçš„è¯´æ˜
        has_relative_bias: ç¬¬1ä¸ªEncoderBlockå–å€¼ä¸ºTrue, å…¶ä½™å‡ä¸ºFalse
        """
        super().__init__()
        self.inner_dim = d_qkv * n_heads
        self.q, self.k, self.v = [nn.Linear(d_model, self.inner_dim) for i in range(3)]
        self.o = nn.Linear(self.inner_dim, d_model)
        self.dropout_rate = dropout_rate
        if has_relative_bias:
            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)

    def compute_bias(self, q_len=512, k_len=512):
        # q_lenå’Œk_lenéƒ½æ˜¯encoderè¾“å…¥çš„åºåˆ—é•¿åº¦
        # åœ¨decoderçš„self-attentionçš„è®­ç»ƒé˜¶æ®µ, q_lenå’Œk_lenéƒ½æ˜¯decoderçš„è¾“å…¥é•¿åº¦
        
        # positions: (q_len, k_len) long tensor
        # æ¯ä¸ªå…ƒç´ çš„å–å€¼èŒƒå›´éƒ½æ˜¯[0, self.relative_attention_num_buckets=32)
        positions = get_relative_idx(q_len, k_len)
        
        bias = self.relative_attention_bias(positions).unsqueeze(0)  # (1, q_len, k_len, n_heads)
        bias = bias.transpose(0, 3, 1, 2)
        # bias: (1, n_heads, q_len, k_len), å…¶ä¸­ç¬¬0ç»´åœ¨è®¡ç®—ä¸­è¢«å¹¿æ’­, å³(B, n_heads, q_len, k_len)
        return bias
    
    def forward(self, hidden, attention_mask, bias=None):
        """
        Args:
            hidden: (B, L, d_model)
            attention_mask: (B, L) LongTensor, æœ‰tokençš„åœ°æ–¹ä¸º1, padå¤„ä¸º0
            bias: ç¬¬1å±‚è¾“å…¥ä¸ºNone, åç»­å±‚å°†ç¬¬ä¸€å±‚è¾“å‡ºçš„biasä½œä¸ºè¾“å…¥
        """
        # q, k, v: (B, L, self.inner_dim)
        q, k, v = self.q(hidden), self.k(hidden), self.v(hidden)
        q = q.reshape(B, L, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, L=q_len, d_qkv)
        k = k.reshape(B, L, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, L=k_len, d_qkv)
        v = v.reshape(B, L, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, L=k_len, d_qkv)
        
        scores = torch.matmul(q, k.transpose(2, 3))  # (B, n_head, L, L)
        if bias is None:
            bias = self.compute_bias(L, L)  # (1, n_head, L, L)
            extended_mask = torch.where(attention_mask[:, None, None, :]==1, 0, -inf)  # (B, 1, 1, L)
            bias = bias + extended_mask  # (B, n_head, L, L)
        scores += bias
        attn_weights = nn.functional.softmax(scores, dim=-1)
        attn_weights = nn.functional.dropout(attn_weights, self.dropout_rate)
        hidden = torch.matmul(atten_weights, v)  # (B, n_heads, L, d_qkv)
        hidden = hidden.transpose(1, 2).view(B, L, self.inner_dim)  # (B, L, inner_dim)
        hidden = self.o(hidden)  # (B, L, d_model)
        return hidden, bias
```


#### Feed-Forward

è§ä¸‹å›¾ï¼Œå«ä¹‰è‡ªæ˜

![](../assets/figures/t5/feed-forward.png)



### decoder

é¦–å…ˆç»™å‡ºæ€»ä½“çš„ç»“æ„å›¾

![](../assets/figures/t5/T5_decoder_training.png)

#### Self-Attention Layer (Decoder)

ä¸ `Self-Attention Layer(Encoder)` çš„è®¡ç®—è¿‡ç¨‹ä¸€è‡´, ä½†æœ‰å¦‚ä¸‹ä¸¤ä¸ªåŒºåˆ«ï¼š

- positional bias ä½¿ç”¨å•å‘çš„æ–¹å¼è¿›è¡Œè·å–

- `mask` æœ‰äº›å˜åŒ–:

  ```python
  bias = self.compute_bias(L, L)  # (1, n_head, L=trg_len, L=trg_len)
  mask = torch.triu(torch.ones((B, 1, L, L)))  # (B, 1, L, L), ä¸‹ä¸‰è§’å«å¯¹è§’çº¿ä¸º1, å…¶ä½™å‡ä¸º0
  extended_mask = torch.where(mask==1, 0, -inf)  # ä¸‹ä¸‰è§’å«å¯¹è§’çº¿ä¸º0, å…¶ä½™å‡ä¸º-inf
  bias = bias + extended_mask  # (B, n_head, L, L)
  ```

#### Cross-Attention Layer (Decoder)

```python
class DecoderCrossAttention(torch.nn.Module):
    def __init__(self, d_model=768, d_qkv=64, n_heads=12, dropout_rate=0.1):
        # æ²¡æœ‰postion biasçš„è®¡ç®—
        super().__init__()
        self.inner_dim = d_qkv * n_heads
        self.q, self.k, self.v = [nn.Linear(d_model, self.inner_dim) for i in range(3)]
        self.o = nn.Linear(self.inner_dim, d_model)
        self.dropout_rate = dropout_rate
    
    def forward(self, decoder_hidden, encoder_hidden, encoder_attention_mask):
        """
        Args:
            decoder_hidden: (B, trg_len, d_model)
            encoder_hidden: (B, src_len, d_model)
            encoder_attention_mask: (B, L) LongTensor, è¾“å…¥åºåˆ—æœ‰tokençš„åœ°æ–¹ä¸º1, padå¤„ä¸º0
        """
        q, k, v = self.q(decoder_hidden), self.k(encoder_hidden), self.v(encoder_hidden)
        q = q.reshape(B, trg_len, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, q_len=trg_len, d_qkv)
        k = k.reshape(B, src_len, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, k_len=src_len, d_qkv)
        v = v.reshape(B, src_len, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, k_len=src_len, d_qkv)
        
        scores = torch.matmul(q, k.transpose(2, 3))  # (B, n_heads, trg_len, src_len)
        
        bias = torch.zeros(B, n_heads, trg_len, src_len)  # (1, n_heads, trg_len, src_len)
        extended_mask = torch.where(attention_mask[:, None, None, :]==1, 0, -inf)  # (B, 1, 1, src_len)
        bias = bias + extended_mask  # (B, n_heads, trg_len, src_len)
        scores += bias

        attn_weights = nn.functional.softmax(scores, dim=-1)
        attn_weights = nn.functional.dropout(attn_weights, self.dropout_rate)
        hidden = torch.matmul(atten_weights, v)  # (B, n_heads, trg_len, d_qkv) = (B, n_heads, trg_len, src_len) * (B, n_heads, src_len, d_qkv)
        hidden = hidden.transpose(1, 2).view(B, trg_len, self.inner_dim)  # (B, trg_len, inner_dim)
        hidden = self.o(hidden)  # (B, trg_len, d_model)
        return hidden, bias
```

## æºç è§£æï¼šğŸ¤— Transformers ä¸­ T5 è®­ç»ƒè¿‡ç¨‹çš„å‰å‘è®¡ç®—æµç¨‹

å¦‚æœå¯¹T5çš„è®¡ç®—é€»è¾‘åŸºæœ¬ç†Ÿæ‚‰çš„è¯ï¼Œè¿™é‡Œç»™å‡º ğŸ¤— Transformers ä¸­çš„æ¨¡å‹å±‚æ¬¡ï¼Œå¯ä»¥å¸®åŠ©å¿«é€Ÿç†è§£æºç çš„å®ç°é€»è¾‘ï¼š

```yaml
# æ³¨æ„: T5Attention è¿™ä¸ªç±»åŒæ—¶å®ç°äº†ä¸‰ç±»æ³¨æ„åŠ›æœºåˆ¶
T5ForConditionalGeneration:
  - nn.Embedding  # A
  - encoder: T5Stack
    - nn.Embedding  # ä¸ A æ˜¯åŒä¸€ä¸ª
    - T5Block 
      - T5LayerSelfAttention
        - T5LayerNorm
        - T5Attention  # å…¨è‡ªæ³¨æ„åŠ›, ä½äºç¬¬ä¸€ä¸ªT5Blockä¸­æ­¤æ¨¡å—å«æœ‰ä¸€ä¸ªnn.Embeddingç”¨äºå­¦ä¹ relative postional bias, å¯å­¦ä¹ å‚æ•°å½¢çŠ¶:(num_bucket=32, num_heads=64)
        - nn.Dropout
      - T5LayerFF
    - T5Block
    ...
    - T5Block
    - T5LayerNorm
    - nn.Dropout
  - decoder: T5Stack
    - nn.Embedding  # ä¸ A æ˜¯åŒä¸€ä¸ª
    - T5Block
      - T5LayerSelfAttention
        - T5LayerNorm
        - T5Attention  # å› æœè‡ªæ³¨æ„åŠ›, ä½äºç¬¬ä¸€ä¸ªT5Blockä¸­æ­¤æ¨¡å—å«æœ‰ä¸€ä¸ªnn.Embeddingç”¨äºå­¦ä¹ relative postional bias, å¯å­¦ä¹ å‚æ•°å½¢çŠ¶:(num_bucket=32, num_heads=64)
        - nn.Dropout
      - T5LayerCrossAttention
        - T5LayerNorm
        - T5Attention  # ä¸encoderçš„è¾“å‡ºåšæ³¨æ„åŠ›, æ²¡æœ‰relative postional bias
        - nn.Dropout
      - T5LayerFF
    - T5Block
    ...
    - T5Block
    - T5LayerNorm
    - nn.Dropout
  - nn.Linear  # T5ä¸­çš„è®¾è®¡é‡Œä¸ A å…±äº«å‚æ•°
```

å¤‡æ³¨ï¼šåœ¨ ğŸ¤— Transformers çš„æºç å®ç°é‡Œ `T5Attention` æ¯”è¾ƒå¤æ‚ï¼Œå®ƒéœ€è¦æ‰¿æ‹…å‡ é¡¹ä¸åŒçš„å·¥ä½œï¼š

- è®­ç»ƒé˜¶æ®µï¼š
  - åœ¨ encoder ä¸­æ‰§è¡Œå…¨è‡ªæ³¨æ„åŠ›æœºåˆ¶
  - åœ¨ decoder ä¸­çš„ `T5LayerSelfAttention` ä¸­æ‰§è¡Œå› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆè®­ç»ƒæ—¶å› ä¸ºå¯ä»¥å¹¶è¡Œè®¡ç®—æ•´ä¸ªdecoderåºåˆ—çš„å„ä¸ªéšå±‚å‘é‡ï¼Œä¸éœ€è¦è€ƒè™‘decoderå‰åºtokençš„keyå’Œvalueçš„ç¼“å­˜ï¼‰
  - åœ¨ decoder ä¸­çš„ `T5LayerCrossAttention` ä¸­æ‰§è¡Œå¯¹encoderè¾“å‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆè®­ç»ƒæ—¶å› ä¸ºå¯ä»¥å¹¶è¡Œè®¡ç®—æ•´ä¸ªdecoderåºåˆ—çš„å„ä¸ªéšå±‚å‘é‡ï¼Œä¸éœ€è¦è€ƒè™‘encoderæœ€åä¸€å±‚çš„keyå’Œvalueçš„ç¼“å­˜ï¼‰
- æ¨ç†é˜¶æ®µï¼š
  - åœ¨ encoder ä¸­æ‰§è¡Œå…¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
  - åœ¨ decoder ä¸­çš„ `T5LayerSelfAttention` ä¸­æ‰§è¡Œå› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆæ¨ç†æ—¶æ˜¯ä¸²è¡Œè§£ç ï¼Œå› æ­¤éœ€è¦ç¼“å­˜decoderçš„ä¹‹å‰æ‰€æœ‰tokençš„keyå’Œvalueçš„ç¼“å­˜ï¼Œè®¡ç®—å½“å‰tokençš„éšå±‚å‘é‡æ—¶ä¹ŸæŠŠå½“å‰tokençš„keyå’Œvalueä¹Ÿç¼“å­˜ä¸‹æ¥ä¾›åç»­è®¡ç®—ï¼‰
  -  åœ¨ decoder ä¸­çš„ `T5LayerCrossAttention` ä¸­æ‰§è¡Œå¯¹encoderè¾“å‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆæ¨ç†æ—¶æ˜¯ä¸²è¡Œè§£ç ï¼Œå› æ­¤è§£ç ç¬¬ä¸€ä¸ªå­—ç¬¦æ—¶ä¼šç¼“å­˜æ¯ä¸€å±‚é’ˆå¯¹encoderè¾“å‡ºå‘é‡çš„keyå’Œvalueï¼Œè§£ç åç»­å­—ç¬¦æ—¶ç›´æ¥ä½¿ç”¨è¿™äº›keyå’Œvalueç¼“å­˜è¿›è¡Œè®¡ç®—ï¼‰

ä¸‹é¢å°†ä¸å†æŒ‰ç…§ ğŸ¤— Transformers çš„æºç è¿›è¡Œæ¢³ç†ï¼Œè€Œæ˜¯ç›´æ¥æ‰‹å†™å¤§éƒ¨åˆ†å±‚çš„å®ç°æ¥è®²è§£ï¼Œæ‰‹å†™å®ç°ä¸ ğŸ¤— Transformers å®ç°çš„å¯¹åº”ä¹Ÿåœ¨å„å°èŠ‚ç»™å‡ºã€‚æ›´ä¸ºå®Œæ•´çš„å¯¹åº”å…³ç³»å¯ä»¥å‚è€ƒï¼š[../assets/code/t5](../assets/code/t5)



## åŸç†/æºç è§£æï¼šğŸ¤— Transformers ä¸­çš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥

æœ¬èŠ‚ä»‹ç» ğŸ¤— Transformers é‡Œå„ç§ç”Ÿæˆæ–¹å¼çš„è¯¦ç»†ç®—æ³•

å…³äºæ–‡æœ¬ç”Ÿæˆï¼ŒğŸ¤— Transformers å®˜æ–¹æœ‰å¦‚ä¸‹å‡ ç¯‡åšå®¢å€¼å¾—é˜…è¯»ï¼š

- åŸºç¡€ç¯‡-å„ç±»ç”Ÿæˆç­–ç•¥ï¼š[how-to-generate](https://huggingface.co/blog/how-to-generate)
- 4.24.0ç‰ˆæœ¬ï¼ˆå‘å¸ƒæ—¶é—´2022/11/01ï¼‰å¼•å…¥contrastive searchï¼š[contrastive-search](https://huggingface.co/blog/introducing-csearch)

ä½¿ç”¨ ğŸ¤— Transformers ç”Ÿæˆæ–‡æœ¬ï¼Œç”¨æ³•å¦‚ä¸‹:

```python
from transformers import T5Tokenizer, T5ForConditionalGeneration
pretrained_name_or_path = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(pretrained_name_or_path)
model = T5ForConditionalGeneration.from_pretrained(pretrained_name_or_path)
inputs = tokenizer(["I'm a student, ", "Deep learning"])
generated_ids = model.generate(
  input_ids=inputs["input_ids"],
  attention_mask=inputs["attention_mask"], 
  max_length=32, 
  num_beams=5,
  repetition_penalty=2.5, 
  length_penalty=1.0, 
  early_stopping=True
  )
```

è¿™é‡Œçš„ `generate` å‡½æ•°æ˜¯å®ç°åœ¨ `T5ForConditionalGeneration` çš„åŸºç±» `PreTrainedModel` ä¸­çš„ï¼Œè€Œ `generate` æ–¹æ³•ä¼šæ ¹æ®ä¼ å…¥çš„å‚æ•°(ä¾‹å¦‚è¿™ä¸ªä¾‹å­é‡Œçš„ `max_length`, `num_beams`, `repetition_penalty`, `length_penalty`, `early_stopping`)é€‰æ‹©æ›´ä¸ºå…·ä½“çš„ç”Ÿæˆæ–¹å¼è¿›è¡Œç”Ÿæˆ, è¿™äº›ç”Ÿæˆæ–¹å¼ä¹Ÿéƒ½æ˜¯åœ¨åŸºç±» `PreTrainedModel` ä¸­è¿›è¡Œå®ç°çš„, ç”±æ­¤å¯ä»¥çœ‹å‡ºåœ¨ç”Ÿæˆç­–ç•¥ä¸Šå…¶å®å¯¹äºä¸åŒçš„æ¨¡å‹æ˜¯ç»Ÿä¸€çš„ã€‚è¿™äº›æ›´ä¸ºå…·ä½“çš„ç”Ÿæˆæ–¹å¼åˆ—ä¸¾å¦‚ä¸‹ï¼Œåé¢å†åŠ ä»¥è¯¦ç»†ä»‹ç»ï¼š

- `greedy_search`ï¼šè´ªå¿ƒç­–ç•¥
- `beam_search`ï¼šbeam search
- `sample`ï¼šå¯¹æ¯æ¬¡å¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒä¸Šè¿›è¡Œé‡‡æ ·
- `beam_sample`ï¼šå¯¹æ¯æ¬¡å¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒä¸Šè¿›è¡Œé‡‡æ ·åå†è¿›è¡Œ beam search
- `group_beam_search`ï¼šå¯¹ beam_size è¿›è¡Œåˆ†ç»„, æ¯ç»„åˆ†åˆ«ä½¿ç”¨ beam search, ä»¥æå‡ beam search ç»“æœçš„å¤šæ ·æ€§
- `constrained_beam_search`ï¼šå¸¦çº¦æŸæ¡ä»¶çš„ beam search, ä¾‹å¦‚ç”Ÿæˆç»“æœé‡Œå¿…é¡»è¦åŒ…å«ç‰¹å®šçš„è¯
- `contrastive_search`ï¼šè¿™ç¯‡[è®ºæ–‡](https://arxiv.org/abs/2210.14140)é‡Œæå‡ºçš„ä¸€ç§ç”Ÿæˆæ–¹å¼ï¼Œæ”¹å–„äº† greedy search/beam search ç”Ÿæˆç»“æœç»å¸¸å‡ºç°é‡å¤å¥å­ï¼ˆæ–‡çŒ®ä¸­æˆä¸º model degenerationï¼‰çš„æƒ…å†µ, ä¹Ÿæ”¹å–„äº† sample/beam sample æ–¹æ³•å®¹æ˜“å‡ºç°è¯­ä¹‰å‰åä¸ä¸€è‡´çš„ç°è±¡ã€‚

é˜…è¯» [how-to-generate](https://huggingface.co/blog/how-to-generate) åšå®¢çš„è¯»è€…ï¼Œå¯èƒ½ä¼šç–‘æƒ‘äº top k sampling ä¸ top p sampling ï¼ˆtop pï¼‰å®ç°åœ¨å“ªé‡Œ, è¿™é‡Œç»™å‡ºç®€è¦çš„è§£é‡Šï¼šè¿™ä¸¤ä¸ªè¢«æŠ½è±¡ä¸ºå¯¹æŠ½æ ·è¿‡ç¨‹æ¦‚ç‡åˆ†å¸ƒçš„è°ƒæ•´ï¼Œå› æ­¤æ’åœ¨æ¦‚ç‡åˆ†å¸ƒä¸é‡‡æ ·ä¹‹é—´ã€‚å¦å¤–ï¼Œç›®å‰ä¸ºæ­¢æ¯”è¾ƒå…¬è®¤çš„ç›¸å¯¹ä¼˜ç§€ç”Ÿæˆæ–¹å¼ä¸ºå¸¦æœ‰ top p/top k çš„ beam sample æ–¹å¼ï¼Œå¹¶ä¸”éœ€è¦é€‚å½“è°ƒèŠ‚ `length_penalty` å‚æ•°ç”¨ä»¥ä¿ƒè¿›/æŠ‘åˆ¶ç”Ÿæˆçš„åºåˆ—é•¿åº¦ï¼Œè°ƒèŠ‚ `repetition_penalty` ç­‰ç›¸å…³å‚æ•°æ§åˆ¶é‡å¤è¯ç»„å‡ºç°çš„æ¬¡æ•°ã€‚æ€»çš„æ¥è¯´ï¼Œè°ƒæ•´è¿™äº›å‚æ•°å¯¹ç”Ÿæˆè´¨é‡è¿˜æ˜¯æœ‰ä¸€äº›å½±å“çš„ï¼Œå› æ­¤åœ¨ decoder çš„è§£ç ç­–ç•¥ä¸Šä¹Ÿä¸€ç›´æœ‰è®ºæ–‡è¿›è¡Œç ”ç©¶ï¼ˆä¾‹å¦‚ï¼šcontrastive search æ˜¯ 2022 å¹´çš„å·¥ä½œï¼‰ï¼Œä¸åŒçš„å¤§æ¨¡å‹åœ¨å¼€å‘è¿‡ç¨‹ä¸­ä¹Ÿå¯èƒ½ä¼šæ¢ç´¢å‡ºæ–°çš„ç”Ÿæˆç­–ç•¥ã€‚

æœ¬èŠ‚åç»­å†…å®¹ç»„ç»‡å¦‚ä¸‹ï¼š

- å…³äº generate æ–¹æ³•çš„ç®€è¦ä»‹ç»ï¼Œæ¶‰åŠåˆ° `PreTrainedModel` çš„ä¸€äº›ç»§æ‰¿å…³ç³»
- `GenerationConfig` çš„ç®€ä»‹ï¼šåªç®€å•ä»‹ç»ä¸€éƒ¨åˆ†å‚æ•°ï¼Œå…¶ä½™çš„å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚é‡ç‚¹ä»‹ç»è·Ÿç”Ÿæˆç­–ç•¥å¼ºç›¸å…³çš„å‚æ•°
- æœ€ç®€ç‰ˆæœ¬çš„ `greedy search` ä»‹ç»
- `LogitsProcessor`ã€`LogitsWarpper` ç®€ä»‹
- `beam_search`
- `sample`/`beam_sample`
- `group_beam_search`
- `constrained_beam_search`: è¿™ä¸ªæ–¹æ³•è‡ªæˆä½“ç³»ä¸”å®ç°ä¸Šæœ‰äº›å¤æ‚, éœ€è¦é¢å¤–ä»‹ç»ç›¸å…³çš„å®ç°ç»†èŠ‚
- `contrastive_search`


### `PreTrainedModel`, `GenerationMixin`, `generate`

æœ¬èŠ‚ä¸»è¦æŒ‰ç…§ ğŸ¤— Transformers çš„æºç è¿›è¡Œä»‹ç»ï¼ŒæŒ‰ç…§ ğŸ¤— Transformers ä¸­çš„å®ç°ï¼Œ`T5ForConditionalGeneration` çš„ç»§æ‰¿å…³ç³»å¦‚ä¸‹ï¼š

```python
class T5ForConditionalGeneration(T5PreTrainedModel):
  # ç”¨äº load æƒé‡æ—¶å¯ä»¥å¿½ç•¥
  _keys_to_ignore_on_load_missing = [
        r"encoder.embed_tokens.weight",
        r"decoder.embed_tokens.weight",
        r"lm_head.weight",
  ]
  _keys_to_ignore_on_load_unexpected = [
      r"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight",
  ]
  # ...

class T5PreTrainedModel(PreTrainedModel):
  config_class = T5Config
  base_model_prefix = "transformer"  # åœ¨ from_pretrained å‡½æ•°ä¸­ load æƒé‡æ—¶æœ‰ç”¨
  def _init_weights(self, module):
    ...
  # å…¶ä»–ä¸€äº›æ–¹æ³•å’Œå±æ€§ä»ç•¥

class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):
  pass
```

è€Œä¸ç”Ÿæˆç›¸å…³çš„ä»£ç ä¸»è¦å®ç°åœ¨ `GenerationMixin` ä¸­ï¼Œä¸€èˆ¬è€Œè¨€ï¼Œé€šè¿‡è¿™ä¸ªç±»çš„ `generate` æ–¹æ³•è¿›è¡Œä½¿ç”¨ï¼Œæ ¹æ®ä¸åŒçš„å‚æ•°è®¾å®šï¼Œä¼šå®é™…ä¸Šé€šè¿‡è°ƒç”¨ä»¥ä¸‹ 7 ç§æ–¹æ³•æ¥å®Œæˆå®é™…çš„ç”Ÿæˆï¼š

`generate` æ–¹æ³•å¦‚ä¸‹:

```python
def generate(
  self,
  inputs: Optional[torch.Tensor] = None,
  generation_config: Optional[GenerationConfig] = None,
  logits_processor: Optional[LogitsProcessorList] = None,
  stopping_criteria: Optional[StoppingCriteriaList] = None,
  prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
  synced_gpus: Optional[bool] = False,
  **kwargs):
  # åªæ‘˜å½•æ ¸å¿ƒéƒ¨åˆ†
  if generation_config is None:
    generation_config = self.generation_config

  generation_config = copy.deepcopy(generation_config)
  # æ ¹æ®kwargsæ›´æ–°
  model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs
  self._validate_model_kwargs(model_kwargs.copy())

  logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
  stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
  inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(
    inputs, generation_config.bos_token_id, model_kwargs
  )
  # å†æ ¹æ®generation_configå¢åŠ ä¸€éƒ¨åˆ†logits_processor
  logits_processor = self._get_logits_processor(generation_config, input_ids_seq_length, encoder_input_ids,
    prefix_allowed_tokens_fn, logits_processor)
  # å†æ ¹æ®generation_configå¢åŠ ä¸€éƒ¨åˆ†stopping_criteria
  stopping_criteria = self._get_stopping_criteria(generation_config, stopping_criteria)
  # æ ¹æ®ä¸åŒçš„generation_configè®¾ç½®, åˆ†åˆ«è°ƒç”¨ä¸Šè¿°7ç§æ–¹æ³•
  ...
```

å¤‡æ³¨ï¼šæ­¤å¤„çš„ `generation_config` å˜é‡çš„ç±»å‹ä¸º `GenerationConfig`, è€Œ `self.generation_config` æ˜¯ `PreTrainedModel` å®ä¾‹åŒ–æ—¶å¾—åˆ°çš„ã€‚

ã€æ­¤å¤„éœ€å¢åŠ ä¸€ä¸ªéšè—æŒ‰é’®ã€‘
```python
class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):
  def __init__(self, config, *inputs, **kwargs):
    # *inputs, **kwargs åœ¨æ­¤å¤„æœªè¢«ä½¿ç”¨åˆ°
    super().__init__()  # nn.Moduleçš„__init__å‡½æ•°, å…¶ä½™ç»§æ‰¿ç±»å‡ä¸ºMixin, æ²¡æœ‰__init__å‡½æ•°
    self.config = config
    self.name_or_path = config.name_or_path
    self.warnings_issued = {}
    self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None
  @classmethod
  def from_pretrained(self, pretrained_model_name_or_path, *model_args, **kwargs):
    # åªæ‘˜å½•é‡è¦çš„éƒ¨åˆ†
    config = kwargs.pop("config", None)
    if not isinstance(config, PretrainedConfig):
      config_path = config if config is not None else pretrained_model_name_or_path
      config, model_kwargs = cls.config_class.from_pretrained(..., **kwargs)
    else:
      model_kwargs = kwargs
    model = cls(config, *model_args, **model_kwargs)  # è°ƒç”¨ __init__
    state_dict = load_state_dict(resolved_archive_file)
    model, ... = cls._load_pretrained_model(model, state_dict, ...)  # loadæƒé‡è‡³æ¨¡å‹
    model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºevalæ¨¡å¼
    if model.can_generate():  # å¦‚æœpretrained_model_name_or_pathç›®å½•ä¸‹åŒ…å«generation_config.jsonæ–‡ä»¶, åˆ™æŒ‰è¿™ä¸ªæ–‡ä»¶é‡æ–°åˆå§‹åŒ–model.generation_config
      model.generation_config = GenerationConfig.from_pretrained(pretrained_model_name_or_path, ..., **kwargs)
  def save_pretrained(self, save_directory, ...):
    # åªæ‘˜å½•é‡è¦çš„éƒ¨åˆ†
    os.makedirs(save_directory, exist_ok=True)
    model_to_save = unwrap_model(self)
    if is_main_process:
      model_to_save.config.save_pretrained(save_directory)
      if self.can_generate():
          model_to_save.generation_config.save_pretrained(save_directory)
    if state_dict is None:
      state_dict = model_to_save.state_dict()
    # æŸäº›å‚æ•°åœ¨ä¿å­˜æ—¶å¯ä»¥å¿½ç•¥
    if self._keys_to_ignore_on_save is not None:
        for ignore_key in self._keys_to_ignore_on_save:
            if ignore_key in state_dict.keys():
                del state_dict[ignore_key]
    # åœ¨æ­£å¸¸æƒ…å†µä¸‹(æ¨¡å‹å‚æ•°ä¸å¤š), shardæ˜¯ä¸€ä¸ªåªæœ‰ä¸€ä¸ªé”®å€¼å¯¹çš„å­—å…¸, keyä¸º"pytorch_model.bin", valueå³ä¸ºstate_dict
    weights_name = SAFE_WEIGHTS_NAME if safe_serialization else WEIGHTS_NAME
    shards, index = shard_checkpoint(state_dict, max_shard_size=max_shard_size, weights_name=weights_name)
    for shard_file, shard in shards.items():
        if safe_serialization:
            safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={"format": "pt"})
        else:
            save_function(shard, os.path.join(save_directory, shard_file))
    # ä¿å­˜index
    ...
```


### `GenerationConfig` çš„ä¸»è¦å‚æ•°ã€TODOã€‘


### greedy_search

å›¾è§£å¦‚ä¸‹

![](../assets/figures/t5/greedy_search.png)


### `LogitsProcessor`ã€`LogitsWarpper`ã€`StoppingCriteria`ã€TODOã€‘

### beam_search

- å½“ beam_size ä¸º 1 æ—¶é€€åŒ–ä¸º greedy_search

```
logits_processor: å®é™…ä¸Šæ˜¯å¯¹å½“å‰é¢„æµ‹çš„log-softmaxåˆ†æ•°è¿›è¡Œåå¤„ç†(ä¾‹å¦‚é‡å¤å‡ºç°çš„å­—åšäº›scoreä¸Šçš„æƒ©ç½š)
stopping_criteria: åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸ, è¿”å›Trueè¡¨ç¤ºåº”è¯¥ç»“æŸ(æœ€å…¸å‹çš„æ˜¯beamè¾¾åˆ°æœ€å¤§é•¿åº¦)

input_ids: å½¢çŠ¶ä¸º(batch_size*beam_size, 1)  # å…¨éƒ¨ä¸ºdecoder_satrt_token
beam_scores: å½¢çŠ¶ä¸º(batch_size, beam_size)  # å…¶ä¸­æ¯ä¸€è¡Œçš„ç¬¬ä¸€ä¸ªå…ƒç´ ä¸º0, å…¶ä½™å…ƒç´ ä¸º-inf
beam_scores.view(-1, 1)
beam_hypotheses: batch_sizeä¸ªå€™é€‰æ± 
is_done: åˆå§‹åŒ–ä¸ºbatch_sizeä¸ªFalse

while True:

  æˆªå–æœ€åä¸€ä¸ªinput_idså¾—åˆ°input_tensor(batch_size*beam_size, 1)
  é€šè¿‡å‰å‘è®¡ç®—å¾—åˆ°logits(batch_size*beam_size, vocab_size)å
  è¿›è¡Œlog_softmaxä¹‹åå¾—åˆ°next_token_scores
  next_token_scores_processed = logits_processor(input_ids, next_token_scores)  # (batch_size*beam_size, vocab_size)
  next_token_scores = beam_scores + next_token_scores_processed  # (batch_size*beam_size, vocab_size)

  next_token_scores.view(batch_size, beam_size*vocab_size)
  # å¯¹äºbatchä¸­çš„æ¯ä¸€ä¸ª, éƒ½ç•™ä¸‹2*beam_sizeä¸ªå¯é€‰é¡¹ï¼ˆæ³¨æ„æ ¹æ®æ­¤å¤„çš„è§„åˆ™è¿™äº›å¯é€‰é¡¹é‡Œè‡³å¤šæœ‰beam_sizeä¸ªeosï¼‰
  next_token_scores, next_tokens = next_token_scores.topk(2*beam_size)

  # è¿™ä¸ªè¿‡ç¨‹çš„é€»è¾‘å¦‚ä¸‹:
  å¯¹äºæ¯ä¸ªæ ·æœ¬
    å¦‚æœis_doneå–å€¼ä¸ºTrue, åˆ™ä¸ºæ¯ä¸ªbeamå¡«å……pad_token, continue
    ä»next_token_scoresæœ€å¤§çš„å¼€å§‹
      å¦‚æœå¯¹åº”çš„é¢„æµ‹tokenä¸ºeosï¼š
        å¦‚æœæ­¤scoreæ˜¯å‰beam_sizeå¤§çš„score, åˆ™å°†å…¶åŠ å…¥è¯¥æ ·æœ¬å¯¹åº”çš„å€™é€‰é›†beam_hypotheses
          åŠ å…¥è§„åˆ™å¦‚ä¸‹ï¼š
            å¦‚æœå€™é€‰æ± å½“å‰ä¸è¶³beam_sizeä¸ªæ ·æœ¬, åˆ™ç›´æ¥å°†å…¶åŠ å…¥, å¹¶è®¡ç®—scoreè®¡ç®—é•¿åº¦æƒ©ç½šåï¼Œæ›´æ–°æ± å­ä¸­çš„æœ€å·®åˆ†æ•°
            å¦‚æœå½“å‰å€™é€‰æ± å·²æœ‰beam_sizeä¸ªæ ·æœ¬, åˆ™å¯¹æ­¤scoreè®¡ç®—é•¿åº¦æƒ©ç½šåä¸æ± å­é‡Œçš„scoreè¿›è¡Œæ¯”è¾ƒï¼Œå¦‚æœä¼˜äºæœ€å·®åˆ†æ•°ï¼Œåˆ™åŠ å…¥å¹¶å‰”é™¤æ± å­é‡Œæœ€å·®çš„é‚£ä¸ªåºåˆ—, ä¹‹åæ›´æ–°æ± å­çš„æœ€å·®åˆ†æ•°
            å¤‡æ³¨: æ± å­ä¸­çš„scoreå‡ä¸ºé•¿åº¦æƒ©ç½šåçš„score
        å¦‚æœæ­¤scoreä¸æ˜¯å‰beam_sizeå¤§çš„score, åˆ™ç›´æ¥å¿½ç•¥è¿™ä¸ªæ ·æœ¬
      å¦‚æœå¯¹åº”çš„é¢„æµ‹tokenä¸æ˜¯eosï¼š
        å°†å…¶åŠ å…¥åˆ°beam_scores, beam_next_tokensä¸­ç›´åˆ°è¾¾åˆ°beam_sizeä¸ª
    åˆ¤æ–­beam_hypothesesæ˜¯å¦å®Œæˆï¼Œç”±æ­¤æ›´æ–°is_done
      åˆ¤æ–­è§„åˆ™å¦‚ä¸‹ï¼š
        (1) å¦‚æœbeam_hypothesesçš„æ¨¡å¼ä¸ºearly_stop, é‚£ä¹ˆåªè¦æ± å­é‡Œæœ‰num_beamä¸ªæ ·æœ¬, å°±è®¤ä¸ºæœç´¢ç»“æŸ
        (2) éearly_stopæ¨¡å¼, åˆ™æ ¹æ®beam_scoresä¸­çš„æœ€å¤§è€…æ˜¯å¦åœ¨è®¡ç®—é•¿åº¦æƒ©ç½šåæ¯”å€™é€‰æ± ä¸­çš„æœ€å·®åˆ†æ•°å¤§, å¦‚æœæ›´å¤§åˆ™ç»§ç»­æœç´¢(is_done=False), å¦‚æœæ›´å°åˆ™è®¤ä¸ºæœç´¢ç»“æŸ(is_done=True)

  beam_scores, beam_next_tokens = beam_scorer.process(input_ids, next_token_scores, next_tokens) # é€»è¾‘è§å‰é¢ä¸€å¤§æ®µçš„è¯´æ˜
  input_ids = cat(input_ids, beam_next_tokens)

  å¦‚æœis_doneå‡ä¸ºTrueæˆ–è€…stopping_criteria(input_ids, scores)ä¸ºTrue, åˆ™è·³å‡ºwhile True

æœ€ååšæ”¶å°¾:
  å¯¹äºè¿˜æ²¡ç»“æŸçš„beam, å°è¯•æ·»åŠ è‡³beam_hypothesesä¸­
  å°†è¾“å‡ºåºåˆ—ä½¿ç”¨pad_tokenè¡¥é½
```

### group_beam_search

group_beam_searchä¸beam_searchçš„åŒºåˆ«åœ¨äº, å°†å½“å‰çš„beamåˆ†ä¸ºè‹¥å¹²ç»„, æ¯ç»„group_sizeä¸ªåºåˆ—, æ¯æ¬¡å¯¹è¿™ä¸ªåºåˆ—åšbeam_search, å¹¶ç•™ä¸‹group_sizeä¸ªåºåˆ—, è¿™æ ·æ€»å…±ä»ç•™æœ‰beam_sizeä¸ªåºåˆ—
- å½“ group_size ä¸ beam_size ç›¸ç­‰æ—¶, é€€åŒ–ä¸ºbeam_search

### beam_sample/sample

beam_sampleä¸beam_searchçš„åŒºåˆ«åœ¨äºå°†è¿™å‡ è¡Œ

```
next_token_scores_processed = logits_processor(input_ids, next_token_scores)  # (batch_size*beam_size, vocab_size)
next_token_scores = beam_scores + next_token_scores_processed  # (batch_size*beam_size, vocab_size)
next_token_scores.view(batch_size, beam_size*vocab_size)
next_token_scores, next_tokens = next_token_scores.topk(2*beam_size)
```

æ›¿æ¢ä¸º

```
logit_warpper: é€šå¸¸è¿›è¡Œtop-k/top-pä¿®æ”¹åˆ†æ•°/ä¸ä¿®æ”¹åˆ†æ•°, å½±å“åç»­çš„æŠ½æ ·ç»“æœ

next_token_scores_processed = logits_processor(input_ids, next_token_scores)  # (batch_size*beam_size, vocab_size)
next_token_scores = beam_scores + next_token_scores_processed  # (batch_size*beam_size, vocab_size)
next_token_scores.view(batch_size, beam_size*vocab_size)

next_token_scores = logits_warper(input_ids, next_token_scores)
probs = nn.functional.softmax(next_token_scores, dim=-1)
next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)
next_token_scoresæ ¹æ®next_tokensçš„é€‰æ‹©å¾—åˆ°
```

### constrained_beam_searchã€TODOã€‘

```
<<<ç¬¬ä¸€é¡¹æ”¹åŠ¨>>>
å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œè¿›è¡Œæ­£å¸¸çš„beam_sizeå¾—åˆ°ï¼š

beam_scores: beam_sizeä¸ªåˆ†æ•°
input_ids: (beam_size, cur_len)
next_tokens: (beam_size)

å¯¹æ¯ä¸ªinput_id, æ ¹æ®çº¦æŸè®¡ç®—ä¸‹ä¸€æ­¥å¯èƒ½çš„tokenï¼Œå‡è®¾å¢åŠ äº† H ä¸ªæ–°çš„å¯é€‰é¡¹ï¼Œå¾—åˆ°
full_hypo: (beam_size+H, cur_len+1) æ‰€æœ‰çš„å‡è®¾
full_score: (beam_size+H,) æ‰€æœ‰å‡è®¾çš„åˆ†æ•°
bank: (beam_size+H,) æ•´æ•°å€¼, å¯¹beam_size+Hä¸­æ¯ä¸ªå‡è®¾è®¡ç®—ä¸€ä¸ªæ»¡è¶³çº¦æŸçš„åˆ†æ•°: æ‰€æœ‰çº¦æŸæ¡ä»¶çš„æœ€å¤§é•¿åº¦*å·²å®Œæˆçš„çº¦æŸ+è¿›è¡Œä¸­çš„çº¦æŸå·²å®Œæˆçš„é•¿åº¦

zipped = bank * 100 + full_score  # 100æ˜¯hfä¸­å†™æ­»çš„è¶…å‚æ•°

æŒ‰ç…§zippedè¿›è¡Œä»å¤§åˆ°å°æ’åºå¾—åˆ°indice, å¹¶æŒ‰ç…§æ­¤é¡ºåºé‡æ’bankï¼Œå¾—åˆ°sorted_bankï¼Œä¾‹å¦‚ï¼š

indice=[5, 4, 1, 2, 0, 3]  # å³zippedä¸­ä¸‹æ ‡ä¸º5çš„å…ƒç´ æœ€å¤§ï¼Œç›¸åº”çš„bankå€¼ä¸º
sorted_bank=[2, 2, 1, 0, 2, 3]
ç”±sort_bankè®¡ç®—dup_num = [0, 1, 0, 0, 0, 0]ï¼Œdup_numä¸ºé‡å¤å‰ä¸€ä¸ªbankå€¼å¾—æ¬¡æ•°

å‡è®¾beam_sizeä¸º3, åˆ™æœ€ç»ˆå¾—indiceä¸º[5, 1, 2]  # æŒ‰dup_numä»å°åˆ°å¤§ç¨³å®šæ’åº, å› æ­¤ä¼šè·³è¿‡indice[1]
<<<ç¬¬äºŒé¡¹æ”¹åŠ¨>>>
æ·»åŠ è‡³hypæ—¶éœ€è¦æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶ï¼Œæ»¡è¶³åˆ™æ·»åŠ ï¼Œä¸æ»¡è¶³åˆ™ä¸æ·»åŠ 
```


constrained_beam_searchç›¸å…³


```python
class Constraint(ABC):
    def __init__(self):
        self.test()  # æµ‹è¯•å­ç±»çš„å®šä¹‰æ˜¯å¦åˆæ³•

    def test(self):
        counter = 0
        completed = False
        while not completed:
            if counter == 1:  # å¦‚æœéœ€è¦è‡³å°‘2æ­¥æ‰èƒ½å®Œæˆ,åˆ™å¯ä»¥æµ‹è¯•resetæ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
                # self.resetå‡½æ•°è¯­ä¹‰(æ”¹å˜çŠ¶æ€): é‡æ–°åˆå§‹åŒ–å†…éƒ¨çš„çŠ¶æ€
                self.reset()
            # self.advanceå‡½æ•°è¯­ä¹‰(ä¸æ”¹å˜çŠ¶æ€): ç»™å‡ºä¸€ä¸ªèƒ½æ»¡è¶³ä¸‹ä¸€æ­¥çº¦æŸæ¡ä»¶çš„token_id(å¦‚æœæœ‰å¤šä¸ªtoken_idèƒ½æ»¡è¶³æ—¶åˆ™è¿”å›token_idåˆ—è¡¨)
            advance = self.advance()  # å¾—åˆ°ä¸€ä¸ªèƒ½å®Œæˆä¸‹ä¸€ä¸ªæ­¥éª¤çš„token_id
            # self.does_advanceè¯­ä¹‰(ä¸æ”¹å˜çŠ¶æ€): åˆ¤æ–­è¾“å…¥çš„å€¼æ˜¯å¦æ»¡è¶³ä¸‹ä¸€æ­¥çº¦æŸçš„æ¡ä»¶
            if not self.does_advance(advance):  # éªŒè¯ä¸€å®šèƒ½èµ°åˆ°ä¸‹ä¸€æ­¥
                raise Exception(
                    "Custom Constraint is not defined correctly. self.does_advance(self.advance()) must be true."
                )
            # self.updateè¯­ä¹‰: steppedè¡¨ç¤ºå½“å‰è¾“å…¥æ˜¯å¦èƒ½æ»¡è¶³ä¸‹ä¸€æ­¥çš„çº¦æŸæ¡ä»¶, completedä¸ºæ˜¯å¦å®Œå…¨ç»“æŸ, resetè¡¨ç¤ºæ˜¯å¦éœ€è¦é‡ç½®
            # å¹¶ä¸”æ ¹æ®å„ç§æƒ…å†µæ”¹å˜çŠ¶æ€, ä¾‹å¦‚ä¿®æ”¹å†…éƒ¨çš„çŠ¶æ€æˆ–è€…è°ƒç”¨self.reset
            stepped, completed, reset = self.update(advance)
            counter += 1

            if counter > 10000:
                raise Exception("update() does not fulfill the constraint.")
        # self.remainingè¯­ä¹‰: å‰©ä½™æ­¥æ•°
        if self.remaining() != 0:  # åœ¨completedçš„æ—¶å€™, å‰©ä½™æ­¥æ•°åº”è¯¥ä¸º0
            raise Exception("Custom Constraint is not defined correctly.")
    
    # å¤åˆ¶(å¤åˆ¶å½“å‰çŠ¶æ€/ä¸å¤åˆ¶å½“å‰çŠ¶æ€)
    @abstractmethod
    def copy(self, stateful=False):
        pass

```

ğŸ¤— Transformers 4.26.1 ç‰ˆæœ¬ä¸­ç›®å‰åªå®ç°äº†ä¸¤ç±» `Constraint`

- `PhrasalConstraint`: ç”Ÿæˆçš„ç»“æœé‡Œå¿…é¡»å‡ºç°æŒ‡å®šçš„ token_id åºåˆ—
  ```python
  constraint = PhrasalConstraint([1, 2, 5])  # é™åˆ¶è¾“å‡ºåºåˆ—å¿…é¡»åŒ…å«è¿ç»­çš„tokenåºåˆ—[1, 2, 5]
  ```
- `DisjunctiveConstraint`: ç”Ÿæˆçš„ç»“æœé‡Œå¿…é¡»å‡ºç°æŒ‡å®šçš„ token_id åºåˆ—(æ»¡è¶³å…¶ä¸­ä¹‹ä¸€å³å¯)
  ```python
  constraint = PhrasalConstraint([[1, 2, 5], [3, 4]])  # é™åˆ¶è¾“å‡ºåºåˆ—å¿…é¡»åŒ…å«è¿ç»­çš„tokenåºåˆ—[1, 2, 5]æˆ–[3, 4]
  ```
- `ConstraintListState`
  ```python
  constraints = [PhrasalConstraint([1, 2, 5]), PhrasalConstraint([1, 3])]
  ConstraintListState(constraints)  # é™åˆ¶è¾“å‡ºåºåˆ—å¿…é¡»æ»¡è¶³å¤šä¸ªé™åˆ¶æ¡ä»¶
  ```


`DisjunctiveConstraint` çš„å®ç°ä¾èµ–äºä¸€ä¸ªè¾…åŠ©ç±», æœ¬è´¨ä¸Šæ˜¯å°†ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨è½¬æ¢ä¸ºäº†ä¸€ä¸ªå­—å…¸(å‰ç¼€æ ‘)

```python
class DisjunctiveTrie:
    def __init__(self, nested_token_ids: List[List[int]], no_subsets=True):
        ...

t = DisjunctiveTrie([[1, 2], [1, 3, 4], [1, 4, 5]])
t.trie
# {
#   1: {
#     2: {},
#     3: {4: {}},
#     4: {5: {}}
#   }
# }
```


```python
class ConstraintListState:
    def __init__(self, constraints: List[Constraint]):
        self.constraints = constraints

        # max # of steps required to fulfill a given constraint
        self.max_seqlen = max([c.seqlen for c in constraints])
        self.n_constraints = len(constraints)
        self.completed = False

        self.init_state()

    def init_state(self):
        # complete_constraints + inprogress_constraint + pending_constraints = å…¨éƒ¨çš„constraint
        self.complete_constraints = []
        self.inprogress_constraint = None  # åªèƒ½ä¸ºNoneæˆ–è€…å…¶ä¸­ä¸€ä¸ªconstraint
        self.pending_constraints = [constraint.copy(stateful=False) for constraint in self.constraints]
    def advance(self) -> List[int]:
        ...
    def reset(self, token_ids: Optional[List[int]]):
        # åœ¨ä¼ å…¥token_idså‚æ•°æ—¶, ä¼šè°ƒç”¨self.addå‡½æ•°
        ...
    def add(self, token_id: int):
        # å¦‚æœinprogress_constraintä¸ºç©º, åˆ™åœ¨pending_constraintsæ‰¾æœ‰æ²¡æœ‰èƒ½updateçš„çº¦æŸ, å¦‚æœæœ‰, å°±å°†å®ƒupdateå¹¶ä½œä¸ºinprogress_constraint
        # å¦‚æœinprogress_constraintä¸ä¸ºç©º, åˆ™update inprogress_constraintç›´è‡³è¿™ä¸ªçº¦æŸè¾¾åˆ°å®ŒæˆçŠ¶æ€
        return complete: bool, stepped: bool
```

### contrastive_searchã€TODO: å¾…è¡¥å……ã€‘

Huggingface å®˜æ–¹åšå®¢ï¼ˆ2022/10 å¼•å…¥ï¼‰ï¼šhttps://huggingface.co/blog/introducing-csearch


ç®—æ³•ä¼ªä»£ç å¦‚ä¸‹ï¼š
```
é¦–å…ˆåˆå§‹åŒ–decoderçš„input_ids: (0,)*B
å°†å…¶è¾“å…¥è‡³decoderè®¡ç®—å‡º next_logits: (B, vocab_size), cur_hidden: (B, 1, d_model)
L = 1
while True:
  # input_ids: å·²ç¡®å®šåºåˆ—, next_logits: å·²ç¡®å®šåºåˆ—çš„logits, cur_hidden: å·²ç¡®å®šåºåˆ—çš„decoderçš„è¾“å‡º
  å¯¹next_logitsè¿›è¡Œåå¤„ç†
  å–å‡ºnext_logitsä¸­å‰top_kä¸ªå¯èƒ½çš„token: (B, top_k), æ‹¼æ¥input_idsåºåˆ— possible_input_ids: (B*top_k, L)
  å°†possibleåºåˆ—ä½œä¸ºdecoderçš„è¾“å…¥, å¾—åˆ° hidden: (B*top_k, d_model), new_next_logits: (B*top_k, vocab_size)
  æ ¹æ®contrastive searchçš„è¯„åˆ†è§„åˆ™æ ¹æ® next_logits, cur_hidden, hidden æŒ‘é€‰å‡ºæ¯ä¸ªæ ·æœ¬æœ€ä¼˜çš„ä¸‹ä¸€ä¸ªåºåˆ— input_ids: (B, L+1)
  æ ¹æ®input_idsæŒ‘é€‰å¾—åˆ°cur_hidden: (B, L+1, d_model), æŒ‘é€‰å¾—åˆ° new_next_logits: (B, vocab_size)
  next_logits = new_next_logits
  L += 1
  åˆ¤æ–­æ˜¯å¦è¾¾åˆ°æœ€å¤§é•¿åº¦æˆ–å…¶ä»–ä¸­æ­¢æ¡ä»¶æ˜¯å¦æˆç«‹ï¼šbreak
è¿”å›input_ids
```

## ä¸»è¦å‚è€ƒèµ„æ–™ã€TODOã€‘

- Huggingface å®˜æ–¹è¯¾ç¨‹: [course](https://huggingface.co/learn/nlp-course)

- Transformer æ¨¡å‹ç»“æ„
  - Huggingface å®˜æ–¹åšå®¢ï¼Œä»‹ç» encoder-decoder æ¶æ„ï¼š[encoder-decoder](https://huggingface.co/blog/encoder-decoder)
  - å›¾è§£ Transformer çš„ä¸€ç¯‡åšå®¢(å¼ºåŠ›æ¨è)ï¼š[illustrated-transformer](http://jalammar.github.io/illustrated-transformer/)ï¼Œ[ä¸­æ–‡ç¿»è¯‘ç‰ˆ](https://blog.csdn.net/longxinchen_ml/article/details/86533005)
  - ä»£ç å®ç° Transformer çš„ä¸€ç¯‡åšå®¢(å“ˆä½›å‡ºå“ï¼Œå¼ºåŠ›æ¨è): [åŸç‰ˆ](https://nlp.seas.harvard.edu/2018/04/03/attention.html)ï¼Œ[æ›´æ–°ç‰ˆ](http://nlp.seas.harvard.edu/annotated-transformer/)

- tokenizer
  - ğŸ¤— Transformers å®˜æ–¹æ–‡æ¡£å¯¹å„ä¸ªæ¨¡å‹æ‰€ä½¿ç”¨çš„ tokenizer çš„æ¦‚è¿°ï¼š[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/tokenizer_summary)
  - ğŸ¤— nlp course: [chapter 6](https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt)
  - BPE
    - BPE ç®—æ³•è¯¦è§£åšå®¢ï¼š[åšå®¢](https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0)
  - Unigram/SentencePiece:
    - åŸå§‹è®ºæ–‡1: [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959)
    - åŸå§‹è®ºæ–‡2: [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/abs/1808.06226)
    - åšå®¢: [sentencepiece tokenizer demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)
  - æ‚é¡¹
    - æœºå™¨ç¿»è¯‘é‡Œä¸èƒ½ç”¨ word å½“ä½œè¯è¡¨çš„åŸå› åŠè§£å†³æ–¹æ³•ç®€ä»‹ï¼š[open vocabulary problem in neural machine translation(NMT)](https://homepages.inf.ed.ac.uk/rsennric/mt18/7_4up.pdf)