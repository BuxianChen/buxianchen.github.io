---
layout: post
title: "(WIP) T5 详解"
date: 2023-03-10 14:31:04 +0800
labels: [paper]
---

## 动机、参考资料、涉及内容

动机

- 熟悉 huggingface transformers 的相关 API 与源码
- 深入理解 T5 的训练与推理步骤，包括每一步的计算过程
- 适当补充相关知识

参考资料

- transformers 4.26.1 源代码
- transformers 官方文档
- T5原始论文
  - 论文地址：https://arxiv.org/pdf/1910.10683.pdf
  - 标题：Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
  - 机构：Google

注意事项

主要从两个视角来写这篇博客：

- 原理视角：主要是论文里描述为主，但缺点是某些地方可能会有一定的模糊
- 实现视角：以 huggingface transformers 的实际实现为准

## Overview

T5 模型尝试将所有的 NLP 任务做了一个统一处理，即：将所有的 NLP 任务都转化为 Text-to-Text 任务。如原论文下图所示：
![](../assets/figures/t5/text-to-text.png)

绿色的框是一个翻译任务（英文翻译为德文），按照以往标准的翻译模型的做法，模型的输入为：`That is good.`，期望模型的输出为：`Das ist gut.`，而 T5 的做法是将输入转化为：`translate English to German: That is good.`，期望的输出依然维持原样。也就是将 NLP 任务的描述也加在了模型输入里。原文中附录 D 中给出了更多的例子。

在模型结构上，T5 模型采用了 Encoder-Decoder 的架构，从大体上说，对于训练过程，伪代码如下：

```python
x, y = "translate English to German: That is good.", "Das ist gut."
x = tokenizer(x)  # [0, 23, 45, 89, 1230, 4, 9], 其中0代表<BOS>, 在实现中<PAD>也是0
y = tokenizer(y)  # [0, 44, 156, 4, 1], 其中1代表<EOS>
x_embedding = encoder.embed_layer(x)  # 将token转换为embedding, x_embedding的形状为(7, 768)
encoded_x = encoder.other_layer(x_embedding)  # 经过encoder后encoded_x的形状为(7, 768)

input_y = y[:-1]  # [0,  44,  156, 4]
# 将token转化为emdedding, input_y_emdedding的形状为(4, 768)
input_y_emdedding = decoder.embed_layer(input_y)  # 在T5的设计中，encoder.embed_layer与decoder.embed_layer共享参数
target_y = y[1:]  # [44, 156, 4,   1]

# decoder_output的形状为(4, 768)
decoder_output = decoder.other_layer(encoded_x, input_y_emdedding)

# logits 的形状为(4, vocab_size=32128)
logits = linear_layer(decoder_output)  # 在T5的设计中，decoder.embed_layer与linear_layer共享参数

# 接下来使用 softmax 与普通的交叉熵计算损失
loss = loss_fn(logits, target_y)
```

## Tokenizer(待续)

从一个疑惑引入：[issue](https://github.com/huggingface/transformers/issues/5087)

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer
pretrained_name_or_path = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(pretrained_name_or_path)

print(tokenizer.special_tokens_map_extended)
# {"eos_token": "</s>", "unk_token": "<unk>", "pad_token": "<pad>", "additional_special_tokens": ['<extra_id_0>', ..., '<extra_id_99>']}
print(tokenizer.eos_token, tokenizer.unk_token, tokenizer.pad_token)
# eos_token: 1, unk_token: 2, pad_token: 0

text = "abc __"
tokens = tokenizer.tokenize(text)  # ["__ab", "c", "__", "_", "_"]
ids = tokenizer.convert_tokens_to_ids(tokens)  # [703, 75, 3, 834, 834]

ids = tokenizer.encode(text)  # [703, 75, 3, 834, 834, 1]
```

由此可见，在 `T5Tokenizer` 的实现里，没有 `bos_token` 这个属性，并且每个 word 起始的 subword 会加上 `__` 的前缀。注意词表中既有以 `__` 开头的 token，例如 `__ab`，而 `__` 本身也在词表中。这种处理方式是因为 `T5Tokenizer` 使用了 Sentence Piece Tokenizer。

## 原理解析：训练过程的前向计算流程

### Encoder

首先给出总体的结构图

![](../assets/figures/t5/T5_encoder.png)

T5 模型的 Encoder 部分由若干个 Block 构成，每个 Block 都具有相同的结构：一个 Self-Attention Layer 和一个 Feed-Forward Layer。这里也首先给出伪代码：

```python
class Encoder:
    def forward(self, x_token, x_attention_mask):
        # x_token: (B, L=512), long
        # x_attention: (B, L), 0/1 mask
        x_embedding = embedding_layer(x_token)
        hidden = dropout(x_embedding)  # (B, L, C=768)
        
        positional_bias = None
        for block in blocks:
            hidden_1 = block.layernorm_layer(hidden)  # LayerNorm层, hidden_1: (B, L, C)
            # Self-Attention层, attention_hidden: (B, L, C), postional_bias: (1, n_heads, L, L)
            # postional_bias在第一层被产生, 后面每一层都使用它(共享参数)
            attention_hidden, positional_bias = block.attention_layer(hidden_1, x_attention_mask, positional_bias)
            hidden = block.dropout(attention_hidden) + hidden  # 残差连接: hidden: (B, L, C)
            
            hidden = block.ff_layer(hidden)  # Feed-Forward层: hidden (B, L, C)
        
        hidden = layernorm_layer(hidden)  # hidden (B, L, C)
        hidden = dropout(hidden)  # hidden (B, L, C)
        return hidden
```

备注：在 huggingface transformers 的实现中，将此处的 `block.layernorm_layer`, `block.attention_layer`、`block.dropout` 的计算逻辑包装在了一起，称为 `T5LayerSelfAttention`。而此处的 `block.ff_layer` 为 `T5LayerFF`。

#### LayerNorm Layer (Encoder)

```python
class LayerNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        # T5用的是简化版的layernorm对最后一维l2归一化后再每一维乘上一个权重, 不带偏置项
        # hidden_states: (B, L, C)
        # return: (B, L, C)
        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states
```

#### Self-Attention Layer (Encoder)

**relative positional embedding**

总共的 postional embedding 数目为 (num_bucket, n_head), T5 的 postional embedding 的 index 的取值范围为 [0, num_bucket)

双向 mask 的情况下, $n=num\_bucket, m=max\_distance$

$$
\begin{equation*}
index(i, j) = \frac{n}{2} * \mathbb{1}[i-j<0] + \left\{
\begin{aligned}
    &abs(i - j), &abs(i - j) < \frac{n}{4} \\
&\min(\frac{n}{2}-1, \frac{n}{4}\times(1+\frac{log(4\times abs(i - j)/n)}{log(4\times m/n)})), &abs(i - j) \ge \frac{n}{4}

\end{aligned}
\right.
\end{equation*}
$$

```python
def relative_position_bidirectional(i, j, num_buckets=32, max_distance=128):
    position = i - j
    abs_position = abs(position)
    num_buckets = num_buckets // 2
    max_exact = num_buckets // 2
    offset = num_buckets if position < 0 else 0
    if abs_position < max_exact:
        return abs_position + offset
    else:
        ratio = math.log(abs_position/ max_exact) / math.log(max_distance / max_exact)
        return min(int(max_exact*(1+ratio)), num_buckets - 1) + offset
```

casual mask 的情况下,

$$
\begin{equation*}
index(i, j) = 
\left\{
\begin{aligned}

&0, &i \ge j \\
&abs(i - j), &i < j\ and\ abs(i - j) < \frac{n}{2} \\
&\min(n-1, \frac{n}{2}\times(1+\frac{log(2\times abs(i - j)/n)}{log(2\times m/n)})), &i < j\ and\ abs(i - j) \ge \frac{n}{2}

\end{aligned}
\right.
\end{equation*}
$$

```python
def relative_position_onedirectional(i, j, num_buckets=32, max_distance=128):
    position = i - j
    if position <= 0:
        return 0
    elif position < (num_buckets // 2):
        return position
    else:
        ratio = math.log(2 * position / num_buckets) / math.log(2 * max_distance / num_buckets)
        return min(int(num_buckets // 2 * (1 + ratio)), num_buckets - 1)
```

在 T5 模型的实验设置中:

```python
num_bucket, max_distance = 32, 128
```

在 encoder 与 decoder 的第一层加上了 positional bias:

```python
bias = nn.Embedding(num_buckect, n_heads)
positional_idx = ...  # 即上面的公式, (L, L)
scores = q @ k.T  # (B, L, L, n_heads)
positional_bias = bias(positional_idx)  # (L, L, n_heads)
scores += positional_bias
# weights = softmax(scores)
```

**self-attention**

```python
class EncoderSelfAttention(torch.nn.Module):
    def __init__(self, d_model=768, d_qkv=64, n_heads=12,
        relative_attention_num_buckets=32, has_relative_bias=False, dropout_rate=0.1):
        """
        relative_attention_num_buckets: 见后面关于positional bias的说明
        has_relative_bias: 第1个EncoderBlock取值为True, 其余均为False
        """
        super().__init__()
        self.inner_dim = d_qkv * n_heads
        self.q, self.k, self.v = [nn.Linear(d_model, self.inner_dim) for i in range(3)]
        self.o = nn.Linear(self.inner_dim, d_model)
        self.dropout_rate = dropout_rate
        if has_relative_bias:
            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)

    def compute_bias(self, q_len=512, k_len=512):
        # q_len和k_len都是encoder输入的序列长度
        # 在decoder的self-attention的训练阶段, q_len和k_len都是decoder的输入长度
        
        # positions: (q_len, k_len) long tensor
        # 每个元素的取值范围都是[0, self.relative_attention_num_buckets=32)
        positions = get_relative_idx(q_len, k_len)
        
        bias = self.relative_attention_bias(positions).unsqueeze(0)  # (1, q_len, k_len, n_heads)
        bias = bias.transpose(0, 3, 1, 2)
        # bias: (1, n_heads, q_len, k_len), 其中第0维在计算中被广播, 即(B, n_heads, q_len, k_len)
        return bias
    
    def forward(self, hidden, attention_mask, bias=None):
        """
        Args:
            hidden: (B, L, d_model)
            attention_mask: (B, L) LongTensor, 有token的地方为1, pad处为0
            bias: 第1层输入为None, 后续层将第一层输出的bias作为输入
        """
        # q, k, v: (B, L, self.inner_dim)
        q, k, v = self.q(hidden), self.k(hidden), self.v(hidden)
        q = q.reshape(B, L, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, L=q_len, d_qkv)
        k = k.reshape(B, L, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, L=k_len, d_qkv)
        v = v.reshape(B, L, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, L=k_len, d_qkv)
        
        scores = torch.matmul(q, k.transpose(2, 3))  # (B, n_head, L, L)
        if bias is None:
            bias = self.compute_bias(L, L)  # (1, n_head, L, L)
            extended_mask = torch.where(attention_mask[:, None, None, :]==1, 0, -inf)  # (B, 1, 1, L)
            bias = bias + extended_mask  # (B, n_head, L, L)
        scores += bias
        attn_weights = nn.functional.softmax(scores, dim=-1)
        attn_weights = nn.functional.dropout(attn_weights, self.dropout_rate)
        hidden = torch.matmul(atten_weights, v)  # (B, n_heads, L, d_qkv)
        hidden = hidden.transpose(1, 2).view(B, L, self.inner_dim)  # (B, L, inner_dim)
        hidden = self.o(hidden)  # (B, L, d_model)
        return hidden, bias
```


#### Feed-Forward

见下图，含义自明

![](../assets/figures/t5/feed-forward.png)



### decoder

首先给出总体的结构图

![](../assets/figures/t5/T5_decoder_training.png)

#### Self-Attention Layer (Decoder)

与 `Self-Attention Layer(Encoder)` 的计算过程一致, 但有如下两个区别：

- positional bias 使用单向的方式进行获取

- `mask` 有些变化:

  ```python
  bias = self.compute_bias(L, L)  # (1, n_head, L=trg_len, L=trg_len)
  mask = torch.triu(torch.ones((B, 1, L, L)))  # (B, 1, L, L), 下三角含对角线为1, 其余均为0
  extended_mask = torch.where(mask==1, 0, -inf)  # 下三角含对角线为0, 其余均为-inf
  bias = bias + extended_mask  # (B, n_head, L, L)
  ```

#### Cross-Attention Layer (Decoder)

```python
class DecoderCrossAttention(torch.nn.Module):
    def __init__(self, d_model=768, d_qkv=64, n_heads=12, dropout_rate=0.1):
        # 没有postion bias的计算
        super().__init__()
        self.inner_dim = d_qkv * n_heads
        self.q, self.k, self.v = [nn.Linear(d_model, self.inner_dim) for i in range(3)]
        self.o = nn.Linear(self.inner_dim, d_model)
        self.dropout_rate = dropout_rate
    
    def forward(self, decoder_hidden, encoder_hidden, encoder_attention_mask):
        """
        Args:
            decoder_hidden: (B, trg_len, d_model)
            encoder_hidden: (B, src_len, d_model)
            encoder_attention_mask: (B, L) LongTensor, 输入序列有token的地方为1, pad处为0
        """
        q, k, v = self.q(decoder_hidden), self.k(encoder_hidden), self.v(encoder_hidden)
        q = q.reshape(B, trg_len, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, q_len=trg_len, d_qkv)
        k = k.reshape(B, src_len, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, k_len=src_len, d_qkv)
        v = v.reshape(B, src_len, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, k_len=src_len, d_qkv)
        
        scores = torch.matmul(q, k.transpose(2, 3))  # (B, n_heads, trg_len, src_len)
        
        bias = torch.zeros(B, n_heads, trg_len, src_len)  # (1, n_heads, trg_len, src_len)
        extended_mask = torch.where(attention_mask[:, None, None, :]==1, 0, -inf)  # (B, 1, 1, src_len)
        bias = bias + extended_mask  # (B, n_heads, trg_len, src_len)
        scores += bias

        attn_weights = nn.functional.softmax(scores, dim=-1)
        attn_weights = nn.functional.dropout(attn_weights, self.dropout_rate)
        hidden = torch.matmul(atten_weights, v)  # (B, n_heads, trg_len, d_qkv) = (B, n_heads, trg_len, src_len) * (B, n_heads, src_len, d_qkv)
        hidden = hidden.transpose(1, 2).view(B, trg_len, self.inner_dim)  # (B, trg_len, inner_dim)
        hidden = self.o(hidden)  # (B, trg_len, d_model)
        return hidden, bias
```

## 源码解析：训练过程的前向计算流程

如果对T5的计算逻辑基本熟悉的话，这里给出 huggingface transformers 中的模型层次，可以帮助快速理解源码的实现逻辑：

```yaml
# 注意: T5Attention 这个类同时实现了三类注意力机制
T5ForConditionalGeneration:
  - nn.Embedding  # A
  - encoder: T5Stack
    - nn.Embedding  # 与 A 是同一个
    - T5Block 
      - T5LayerSelfAttention
        - T5LayerNorm
        - T5Attention  # 全自注意力, 位于第一个T5Block中此模块含有一个nn.Embedding用于学习relative postional bias, 可学习参数形状:(num_bucket=32, num_heads=64)
        - nn.Dropout
      - T5LayerFF
    - T5Block
    ...
    - T5Block
    - T5LayerNorm
    - nn.Dropout
  - decoder: T5Stack
    - nn.Embedding  # 与 A 是同一个
    - T5Block
      - T5LayerSelfAttention
        - T5LayerNorm
        - T5Attention  # 因果自注意力, 位于第一个T5Block中此模块含有一个nn.Embedding用于学习relative postional bias, 可学习参数形状:(num_bucket=32, num_heads=64)
        - nn.Dropout
      - T5LayerCrossAttention
        - T5LayerNorm
        - T5Attention  # 与encoder的输出做注意力, 没有relative postional bias
        - nn.Dropout
      - T5LayerFF
    - T5Block
    ...
    - T5Block
    - T5LayerNorm
    - nn.Dropout
  - nn.Linear  # T5中的设计里与 A 共享参数
```

备注：在 huggingface transformers 的源码实现里 `T5Attention` 比较复杂，它需要承担几项不同的工作：

- 训练阶段：
  - 在 encoder 中执行全自注意力机制
  - 在 decoder 中的 `T5LayerSelfAttention` 中执行因果自注意力机制（训练时因为可以并行计算整个decoder序列的各个隐层向量，不需要考虑decoder前序token的key和value的缓存）
  - 在 decoder 中的 `T5LayerCrossAttention` 中执行对encoder输出的注意力机制（训练时因为可以并行计算整个decoder序列的各个隐层向量，不需要考虑encoder最后一层的key和value的缓存）
- 推理阶段：
  - 在 encoder 中执行全自注意力机制，与训练时完全一致
  - 在 decoder 中的 `T5LayerSelfAttention` 中执行因果自注意力机制（推理时是串行解码，因此需要缓存decoder的之前所有token的key和value的缓存，计算当前token的隐层向量时也把当前token的key和value也缓存下来供后续计算）
  -  在 decoder 中的 `T5LayerCrossAttention` 中执行对encoder输出的注意力机制（推理时是串行解码，因此解码第一个字符时会缓存每一层针对encoder输出向量的key和value，解码后续字符时直接使用这些key和value缓存进行计算）

下面将不再按照 huggingface transformers 的源码进行梳理，而是直接手写大部分层的实现来讲解，手写实现与 huggingface 实现的对应也在各小节给出。更为完整的对应关系可以参考：[../assets/code/t5](../assets/code/t5)



## 原理/源码解析：文本生成策略

本节介绍 huggingface 里各种生成方式的详细算法

关于文本生成，huggingface 官方有如下几篇博客值得阅读：

- 基础篇-各类生成策略：[how-to-generate](https://huggingface.co/blog/how-to-generate)
- 4.24.0版本（发布时间2022/11/01）引入contrastive search：[contrastive-search](https://huggingface.co/blog/introducing-csearch)

使用 huggingface 生成文本，用法如下:

```python
from transformers import T5Tokenizer, T5ForConditionalGeneration
pretrained_name_or_path = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(pretrained_name_or_path)
model = T5ForConditionalGeneration.from_pretrained(pretrained_name_or_path)
inputs = tokenizer(["I'm a student, ", "Deep learning"])
generated_ids = model.generate(
  input_ids=inputs["input_ids"],
  attention_mask=inputs["attention_mask"], 
  max_length=32, 
  num_beams=5,
  repetition_penalty=2.5, 
  length_penalty=1.0, 
  early_stopping=True
  )
```

这里的 `generate` 函数是实现在 `T5ForConditionalGeneration` 的基类 `PreTrainedModel` 中的，而 `generate` 方法会根据传入的参数(例如这个例子里的 `max_length`, `num_beams`, `repetition_penalty`, `length_penalty`, `early_stopping`)选择更为具体的生成方式进行生成, 这些生成方式也都是在基类 `PreTrainedModel` 中进行实现的, 由此可以看出在生成策略上其实对于不同的模型是统一的。这些更为具体的生成方式列举如下，后面再加以详细介绍：

- `greedy_search`：贪心策略
- `beam_search`：beam search
- `sample`：对每次得到的概率分布上进行采样
- `beam_sample`：对每次得到的概率分布上进行采样后再进行 beam search
- `group_beam_search`：对 beam_size 进行分组, 每组分别使用 beam search, 以提升 beam search 结果的多样性
- `constrained_beam_search`：带约束条件的 beam search, 例如生成结果里必须要包含特定的词
- `contrastive_search`：这篇[论文](https://arxiv.org/abs/2210.14140)里提出的一种生成方式，改善了 greedy search/beam search 生成结果经常出现重复句子（文献中成为 model degeneration）的情况, 也改善了 sample/beam sample 方法容易出现语义前后不一致的现象。

阅读 [how-to-generate](https://huggingface.co/blog/how-to-generate) 博客的读者，可能会疑惑于 top k sampling 与 top p sampling （top p）实现在哪里, 这里给出简要的解释：这两个被抽象为对抽样过程概率分布的调整，因此插在概率分布与采样之间。另外，目前为止比较公认的相对优秀生成方式为带有 top p/top k 的 beam sample 方式，并且需要适当调节 `length_penalty` 参数用以促进/抑制生成的序列长度，调节 `repetition_penalty` 等相关参数控制重复词组出现的次数。总的来说，调整这些参数对生成质量还是有一些影响的，因此在 decoder 的解码策略上也一直有论文进行研究（例如：contrastive search 是 2022 年的工作），不同的大模型在开发过程中也可能会探索出新的生成策略。

本节后续内容组织如下：

- 关于 generate 方法的简要介绍，涉及到 `PreTrainedModel` 的一些继承关系
- `GenerationConfig` 的简介：只简单介绍一部分参数，其余的参考官方文档。重点介绍跟生成策略强相关的参数
- 最简版本的 `greedy search` 介绍
- `LogitsProcessor`、`LogitsWarpper` 简介
- `beam_search`
- `sample`/`beam_sample`
- `group_beam_search`
- `constrained_beam_search`: 这个方法自成体系且实现上有些复杂, 需要额外介绍相关的实现细节
- `contrastive_search`


### `PreTrainedModel`, `GenerationMixin`, `generate`

本节主要按照 Huggingface Transformers 的源码进行介绍，按照 huggingface 中的实现，`T5ForConditionalGeneration` 的继承关系如下：

```python
class T5ForConditionalGeneration(T5PreTrainedModel):
  # 用于 load 权重时可以忽略
  _keys_to_ignore_on_load_missing = [
        r"encoder.embed_tokens.weight",
        r"decoder.embed_tokens.weight",
        r"lm_head.weight",
  ]
  _keys_to_ignore_on_load_unexpected = [
      r"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight",
  ]
  # ...

class T5PreTrainedModel(PreTrainedModel):
  config_class = T5Config
  base_model_prefix = "transformer"  # 在 from_pretrained 函数中 load 权重时有用
  def _init_weights(self, module):
    ...
  # 其他一些方法和属性从略

class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):
  pass
```

而与生成相关的代码主要实现在 `GenerationMixin` 中，一般而言，通过这个类的 `generate` 方法进行使用，根据不同的参数设定，会实际上通过调用以下 7 种方法来完成实际的生成：

`generate` 方法如下:

```python
def generate(
  self,
  inputs: Optional[torch.Tensor] = None,
  generation_config: Optional[GenerationConfig] = None,
  logits_processor: Optional[LogitsProcessorList] = None,
  stopping_criteria: Optional[StoppingCriteriaList] = None,
  prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
  synced_gpus: Optional[bool] = False,
  **kwargs):
  # 只摘录核心部分
  if generation_config is None:
    generation_config = self.generation_config

  generation_config = copy.deepcopy(generation_config)
  # 根据kwargs更新
  model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs
  self._validate_model_kwargs(model_kwargs.copy())

  logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
  stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
  inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(
    inputs, generation_config.bos_token_id, model_kwargs
  )
  # 再根据generation_config增加一部分logits_processor
  logits_processor = self._get_logits_processor(generation_config, input_ids_seq_length, encoder_input_ids,
    prefix_allowed_tokens_fn, logits_processor)
  # 再根据generation_config增加一部分stopping_criteria
  stopping_criteria = self._get_stopping_criteria(generation_config, stopping_criteria)
  # 根据不同的generation_config设置, 分别调用上述7种方法
  ...
```

备注：此处的 `generation_config` 变量的类型为 `GenerationConfig`, 而 `self.generation_config` 是 `PreTrainedModel` 实例化时得到的。

【此处需增加一个隐藏按钮】
```python
class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):
  def __init__(self, config, *inputs, **kwargs):
    # *inputs, **kwargs 在此处未被使用到
    super().__init__()  # nn.Module的__init__函数, 其余继承类均为Mixin, 没有__init__函数
    self.config = config
    self.name_or_path = config.name_or_path
    self.warnings_issued = {}
    self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None
  @classmethod
  def from_pretrained(self, pretrained_model_name_or_path, *model_args, **kwargs):
    # 只摘录重要的部分
    config = kwargs.pop("config", None)
    if not isinstance(config, PretrainedConfig):
      config_path = config if config is not None else pretrained_model_name_or_path
      config, model_kwargs = cls.config_class.from_pretrained(..., **kwargs)
    else:
      model_kwargs = kwargs
    model = cls(config, *model_args, **model_kwargs)  # 调用 __init__
    state_dict = load_state_dict(resolved_archive_file)
    model, ... = cls._load_pretrained_model(model, state_dict, ...)  # load权重至模型
    model.eval()  # 将模型设置为eval模式
    if model.can_generate():  # 如果pretrained_model_name_or_path目录下包含generation_config.json文件, 则按这个文件重新初始化model.generation_config
      model.generation_config = GenerationConfig.from_pretrained(pretrained_model_name_or_path, ..., **kwargs)
  def save_pretrained(self, save_directory, ...):
    # 只摘录重要的部分
    os.makedirs(save_directory, exist_ok=True)
    model_to_save = unwrap_model(self)
    if is_main_process:
      model_to_save.config.save_pretrained(save_directory)
      if self.can_generate():
          model_to_save.generation_config.save_pretrained(save_directory)
    if state_dict is None:
      state_dict = model_to_save.state_dict()
    # 某些参数在保存时可以忽略
    if self._keys_to_ignore_on_save is not None:
        for ignore_key in self._keys_to_ignore_on_save:
            if ignore_key in state_dict.keys():
                del state_dict[ignore_key]
    # 在正常情况下(模型参数不多), shard是一个只有一个键值对的字典, key为"pytorch_model.bin", value即为state_dict
    weights_name = SAFE_WEIGHTS_NAME if safe_serialization else WEIGHTS_NAME
    shards, index = shard_checkpoint(state_dict, max_shard_size=max_shard_size, weights_name=weights_name)
    for shard_file, shard in shards.items():
        if safe_serialization:
            safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={"format": "pt"})
        else:
            save_function(shard, os.path.join(save_directory, shard_file))
    # 保存index
    ...
```


### `GenerationConfig` 的主要参数


### greedy_search

图解如下

![](../assets/figures/t5/greedy_search.png)


### `LogitsProcessor`、`LogitsWarpper`、`StoppingCriteria`

### beam_search

- 当 beam_size 为 1 时退化为 greedy_search

```
logits_processor: 实际上是对当前预测的log-softmax分数进行后处理(例如重复出现的字做些score上的惩罚)
stopping_criteria: 判断是否应该结束, 返回True表示应该结束(最典型的是beam达到最大长度)

input_ids: 形状为(batch_size*beam_size, 1)  # 全部为decoder_satrt_token
beam_scores: 形状为(batch_size, beam_size)  # 其中每一行的第一个元素为0, 其余元素为-inf
beam_scores.view(-1, 1)
beam_hypotheses: batch_size个候选池
is_done: 初始化为batch_size个False

while True:

  截取最后一个input_ids得到input_tensor(batch_size*beam_size, 1)
  通过前向计算得到logits(batch_size*beam_size, vocab_size)后
  进行log_softmax之后得到next_token_scores
  next_token_scores_processed = logits_processor(input_ids, next_token_scores)  # (batch_size*beam_size, vocab_size)
  next_token_scores = beam_scores + next_token_scores_processed  # (batch_size*beam_size, vocab_size)

  next_token_scores.view(batch_size, beam_size*vocab_size)
  # 对于batch中的每一个, 都留下2*beam_size个可选项（注意根据此处的规则这些可选项里至多有beam_size个eos）
  next_token_scores, next_tokens = next_token_scores.topk(2*beam_size)

  # 这个过程的逻辑如下:
  对于每个样本
    如果is_done取值为True, 则为每个beam填充pad_token, continue
    从next_token_scores最大的开始
      如果对应的预测token为eos：
        如果此score是前beam_size大的score, 则将其加入该样本对应的候选集beam_hypotheses
          加入规则如下：
            如果候选池当前不足beam_size个样本, 则直接将其加入, 并计算score计算长度惩罚后，更新池子中的最差分数
            如果当前候选池已有beam_size个样本, 则对此score计算长度惩罚后与池子里的score进行比较，如果优于最差分数，则加入并剔除池子里最差的那个序列, 之后更新池子的最差分数
            备注: 池子中的score均为长度惩罚后的score
        如果此score不是前beam_size大的score, 则直接忽略这个样本
      如果对应的预测token不是eos：
        将其加入到beam_scores, beam_next_tokens中直到达到beam_size个
    判断beam_hypotheses是否完成，由此更新is_done
      判断规则如下：
        (1) 如果beam_hypotheses的模式为early_stop, 那么只要池子里有num_beam个样本, 就认为搜索结束
        (2) 非early_stop模式, 则根据beam_scores中的最大者是否在计算长度惩罚后比候选池中的最差分数大, 如果更大则继续搜索(is_done=False), 如果更小则认为搜索结束(is_done=True)

  beam_scores, beam_next_tokens = beam_scorer.process(input_ids, next_token_scores, next_tokens) # 逻辑见前面一大段的说明
  input_ids = cat(input_ids, beam_next_tokens)

  如果is_done均为True或者stopping_criteria(input_ids, scores)为True, 则跳出while True

最后做收尾:
  对于还没结束的beam, 尝试添加至beam_hypotheses中
  将输出序列使用pad_token补齐
```

### group_beam_search

group_beam_search与beam_search的区别在于, 将当前的beam分为若干组, 每组group_size个序列, 每次对这个序列做beam_search, 并留下group_size个序列, 这样总共仍留有beam_size个序列
- 当 group_size 与 beam_size 相等时, 退化为beam_search

### beam_sample/sample

beam_sample与beam_search的区别在于将这几行

```
next_token_scores_processed = logits_processor(input_ids, next_token_scores)  # (batch_size*beam_size, vocab_size)
next_token_scores = beam_scores + next_token_scores_processed  # (batch_size*beam_size, vocab_size)
next_token_scores.view(batch_size, beam_size*vocab_size)
next_token_scores, next_tokens = next_token_scores.topk(2*beam_size)
```

替换为

```
logit_warpper: 通常进行top-k/top-p修改分数/不修改分数, 影响后续的抽样结果

next_token_scores_processed = logits_processor(input_ids, next_token_scores)  # (batch_size*beam_size, vocab_size)
next_token_scores = beam_scores + next_token_scores_processed  # (batch_size*beam_size, vocab_size)
next_token_scores.view(batch_size, beam_size*vocab_size)

next_token_scores = logits_warper(input_ids, next_token_scores)
probs = nn.functional.softmax(next_token_scores, dim=-1)
next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)
next_token_scores根据next_tokens的选择得到
```

### constrained_beam_search

```
<<<第一项改动>>>
对于每个样本，进行正常的beam_size得到：

beam_scores: beam_size个分数
input_ids: (beam_size, cur_len)
next_tokens: (beam_size)

对每个input_id, 根据约束计算下一步可能的token，假设增加了 H 个新的可选项，得到
full_hypo: (beam_size+H, cur_len+1) 所有的假设
full_score: (beam_size+H,) 所有假设的分数
bank: (beam_size+H,) 整数值, 对beam_size+H中每个假设计算一个满足约束的分数: 所有约束条件的最大长度*已完成的约束+进行中的约束已完成的长度

zipped = bank * 100 + full_score  # 100是hf中写死的超参数

按照zipped进行从大到小排序得到indice, 并按照此顺序重排bank，得到sorted_bank，例如：

indice=[5, 4, 1, 2, 0, 3]  # 即zipped中下标为5的元素最大，相应的bank值为
sorted_bank=[2, 2, 1, 0, 2, 3]
由sort_bank计算dup_num = [0, 1, 0, 0, 0, 0]，dup_num为重复前一个bank值得次数

假设beam_size为3, 则最终得indice为[5, 1, 2]  # 按dup_num从小到大稳定排序, 因此会跳过indice[1]
<<<第二项改动>>>
添加至hyp时需要检查是否满足条件，满足则添加，不满足则不添加
```


constrained_beam_search相关


```python
class Constraint(ABC):
    def __init__(self):
        self.test()  # 测试子类的定义是否合法

    def test(self):
        counter = 0
        completed = False
        while not completed:
            if counter == 1:  # 如果需要至少2步才能完成,则可以测试reset是否能正常工作
                # self.reset函数语义(改变状态): 重新初始化内部的状态
                self.reset()
            # self.advance函数语义(不改变状态): 给出一个能满足下一步约束条件的token_id(如果有多个token_id能满足时则返回token_id列表)
            advance = self.advance()  # 得到一个能完成下一个步骤的token_id
            # self.does_advance语义(不改变状态): 判断输入的值是否满足下一步约束的条件
            if not self.does_advance(advance):  # 验证一定能走到下一步
                raise Exception(
                    "Custom Constraint is not defined correctly. self.does_advance(self.advance()) must be true."
                )
            # self.update语义: stepped表示当前输入是否能满足下一步的约束条件, completed为是否完全结束, reset表示是否需要重置
            # 并且根据各种情况改变状态, 例如修改内部的状态或者调用self.reset
            stepped, completed, reset = self.update(advance)
            counter += 1

            if counter > 10000:
                raise Exception("update() does not fulfill the constraint.")
        # self.remaining语义: 剩余步数
        if self.remaining() != 0:  # 在completed的时候, 剩余步数应该为0
            raise Exception("Custom Constraint is not defined correctly.")
    
    # 复制(复制当前状态/不复制当前状态)
    @abstractmethod
    def copy(self, stateful=False):
        pass

```

huggingface transformers 4.26.1 版本中目前只实现了两类 `Constraint`

- `PhrasalConstraint`: 生成的结果里必须出现指定的 token_id 序列
  ```python
  constraint = PhrasalConstraint([1, 2, 5])  # 限制输出序列必须包含连续的token序列[1, 2, 5]
  ```
- `DisjunctiveConstraint`: 生成的结果里必须出现指定的 token_id 序列(满足其中之一即可)
  ```python
  constraint = PhrasalConstraint([[1, 2, 5], [3, 4]])  # 限制输出序列必须包含连续的token序列[1, 2, 5]或[3, 4]
  ```
- `ConstraintListState`
  ```python
  constraints = [PhrasalConstraint([1, 2, 5]), PhrasalConstraint([1, 3])]
  ConstraintListState(constraints)  # 限制输出序列必须满足多个限制条件
  ```


`DisjunctiveConstraint` 的实现依赖于一个辅助类, 本质上是将一个列表的列表转换为了一个字典(前缀树)

```python
class DisjunctiveTrie:
    def __init__(self, nested_token_ids: List[List[int]], no_subsets=True):
        ...

t = DisjunctiveTrie([[1, 2], [1, 3, 4], [1, 4, 5]])
t.trie
# {
#   1: {
#     2: {},
#     3: {4: {}},
#     4: {5: {}}
#   }
# }
```


```python
class ConstraintListState:
    def __init__(self, constraints: List[Constraint]):
        self.constraints = constraints

        # max # of steps required to fulfill a given constraint
        self.max_seqlen = max([c.seqlen for c in constraints])
        self.n_constraints = len(constraints)
        self.completed = False

        self.init_state()

    def init_state(self):
        # complete_constraints + inprogress_constraint + pending_constraints = 全部的constraint
        self.complete_constraints = []
        self.inprogress_constraint = None  # 只能为None或者其中一个constraint
        self.pending_constraints = [constraint.copy(stateful=False) for constraint in self.constraints]
    def advance(self) -> List[int]:
        ...
    def reset(self, token_ids: Optional[List[int]]):
        # 在传入token_ids参数时, 会调用self.add函数
        ...
    def add(self, token_id: int):
        # 如果inprogress_constraint为空, 则在pending_constraints找有没有能update的约束, 如果有, 就将它update并作为inprogress_constraint
        # 如果inprogress_constraint不为空, 则update inprogress_constraint直至这个约束达到完成状态
        return complete: bool, stepped: bool
```

### contrastive_search【待补充】

Huggingface 官方博客（2022/10 引入）：https://huggingface.co/blog/introducing-csearch



## 主要参考资料

- Huggingface 官方博客，介绍 encoder-decoder 架构：[encoder-decoder](https://huggingface.co/blog/encoder-decoder)
- 图解 Transformer 的一篇博客(强力推荐)：[illustrated-transformer](http://jalammar.github.io/illustrated-transformer/)，[中文翻译版](https://blog.csdn.net/longxinchen_ml/article/details/86533005)
- 代码实现 Transformer 的一篇博客(哈佛出品，强力推荐): [原版](https://nlp.seas.harvard.edu/2018/04/03/attention.html)，[更新版](http://nlp.seas.harvard.edu/annotated-transformer/)