---
layout: post
title: "(WIP) T5 详解"
date: 2023-03-10 14:31:04 +0800
labels: [paper]
---

## 动机、参考资料、涉及内容

动机

- 熟悉 huggingface transformers 的相关 API 与源码
- 深入理解 T5 的训练与推理步骤，包括每一步的计算过程
- 适当补充相关知识

参考资料

- transformers 4.26.1 源代码
- transformers 官方文档
- T5原始论文
  - 论文地址：https://arxiv.org/pdf/1910.10683.pdf
  - 标题：Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
  - 机构：Google

## Overview

T5 模型尝试将所有的 NLP 任务做了一个统一处理，即：将所有的 NLP 任务都转化为 Text-to-Text 任务。如原论文下图所示：
![](../assets/figures/t5/text-to-text.png)

绿色的框是一个翻译任务（英文翻译为德文），按照以往标准的翻译模型的做法，模型的输入为：`That is good.`，期望模型的输出为：`Das ist gut.`，而 T5 的做法是将输入转化为：`translate English to German: That is good.`，期望的输出依然维持原样。也就是将 NLP 任务的描述也加在了模型输入里。原文中附录 D 中给出了更多的例子。

在模型结构上，T5 模型采用了 Encoder-Decoder 的架构，从大体上说，对于训练过程，伪代码如下：

```python
x, y = "translate English to German: That is good.", "Das ist gut."
x = tokenizer(x)  # [0, 23, 45, 89, 1230, 4, 9], 其中0代表<BOS>, 在实现中<PAD>也是0
y = tokenizer(y)  # [0, 44, 156, 4, 1], 其中1代表<EOS>
x_embedding = encoder.embed_layer(x)  # 将token转换为embedding, x_embedding的形状为(7, 768)
encoded_x = encoder.other_layer(x_embedding)  # 经过encoder后encoded_x的形状为(7, 768)

input_y = y[:-1]  # [0,  44,  156, 4]
# 将token转化为emdedding, input_y_emdedding的形状为(4, 768)
input_y_emdedding = decoder.embed_layer(input_y)  # 在T5的设计中，encoder.embed_layer与decoder.embed_layer共享参数
target_y = y[1:]  # [44, 156, 4,   1]

# decoder_output的形状为(4, 768)
decoder_output = decoder.other_layer(encoded_x, input_y_emdedding)

# logits 的形状为(4, vocab_size=32128)
logits = linear_layer(decoder_output)  # 在T5的设计中，decoder.embed_layer与linear_layer共享参数

# 接下来使用 softmax 与普通的交叉熵计算损失
loss = loss_fn(logits, target_y)
```

## Tokenizer(待续)

## 原理解析：训练过程的前向计算流程

### Encoder

T5 模型的 Encoder 部分由若干个 Block 构成，每个 Block 都具有相同的结构：一个 Self-Attention Layer 和一个 Feed-Forward Layer。这里也首先给出伪代码：

```python
class Encoder:
    def forward(self, x_token, x_attention_mask):
        # x_token: (B, L=512), long
        # x_attention: (B, L), 0/1 mask
        x_embedding = embedding_layer(x_token)
        hidden = dropout(x_embedding)  # (B, L, C=768)
        
        positional_bias = None
        for block in blocks:
            hidden_1 = block.layernorm_layer(hidden)  # LayerNorm层, hidden_1: (B, L, C)
            # Self-Attention层, attention_hidden: (B, L, C), postional_bias: (1, n_heads, L, L)
            # postional_bias在第一层被产生, 后面每一层都使用它(共享参数)
            attention_hidden, positional_bias = block.attention_layer(hidden_1, x_attention_mask, positional_bias)
            hidden = block.dropout(attention_hidden) + hidden  # 残差连接: hidden: (B, L, C)
            
            hidden = block.ff_layer(hidden)  # Feed-Forward层: hidden (B, L, C)
        
        hidden = layernorm_layer(hidden)  # hidden (B, L, C)
        hidden = dropout(hidden)  # hidden (B, L, C)
        return hidden
```

备注：在 huggingface transformers 的实现中，将此处的 `block.layernorm_layer`, `block.attention_layer`、`block.dropout` 的计算逻辑包装在了一起，称为 `T5LayerSelfAttention`。而此处的 `block.ff_layer` 为 `T5LayerFF`。

#### LayerNorm Layer (Encoder)

```python
class LayerNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        # T5用的是简化版的layernorm对最后一维l2归一化后再每一维乘上一个权重, 不带偏置项
        # hidden_states: (B, L, C)
        # return: (B, L, C)
        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states
```

#### Self-Attention Layer (Encoder)

**relative positional embedding**

总共的 postional embedding 数目为 (num_bucket, n_head), T5 的 postional embedding 的 index 的取值范围为 [0, num_bucket)

双向 mask 的情况下, $n=num\_bucket, m=max\_distance$

$$
\begin{equation*}
index(i, j) = \frac{n}{2} * \mathbb{1}[i-j<0] + \left\{
\begin{aligned}
    &abs(i - j), &abs(i - j) < \frac{n}{4} \\
&\min(\frac{n}{2}-1, \frac{n}{4}\times(1+\frac{log(4\times abs(i - j)/n)}{log(4\times m/n)})), &abs(i - j) \ge \frac{n}{4}

\end{aligned}
\right.
\end{equation*}
$$

```python
def relative_position_bidirectional(i, j, num_buckets=32, max_distance=128):
    position = i - j
    abs_position = abs(position)
    num_buckets = num_buckets // 2
    max_exact = num_buckets // 2
    offset = num_buckets if position < 0 else 0
    if abs_position < max_exact:
        return abs_position + offset
    else:
        ratio = math.log(abs_position/ max_exact) / math.log(max_distance / max_exact)
        return min(int(max_exact*(1+ratio)), num_buckets - 1) + offset
```

casual mask 的情况下,

$$
\begin{equation*}
index(i, j) = 
\left\{
\begin{aligned}

&0, &i \ge j \\
&abs(i - j), &i < j\ and\ abs(i - j) < \frac{n}{2} \\
&\min(n-1, \frac{n}{2}\times(1+\frac{log(2\times abs(i - j)/n)}{log(2\times m/n)})), &i < j\ and\ abs(i - j) \ge \frac{n}{2}

\end{aligned}
\right.
\end{equation*}
$$

```python
def relative_position_onedirectional(i, j, num_buckets=32, max_distance=128):
    position = i - j
    if position <= 0:
        return 0
    elif position < (num_buckets // 2):
        return position
    else:
        ratio = math.log(2 * position / num_buckets) / math.log(2 * max_distance / num_buckets)
        return min(int(num_buckets // 2 * (1 + ratio)), num_buckets - 1)
```

在 T5 模型的实验设置中:

```python
num_bucket, max_distance = 32, 128
```

在 encoder 与 decoder 的第一层加上了 positional bias:

```python
bias = nn.Embedding(num_buckect, n_heads)
positional_idx = ...  # 即上面的公式, (L, L)
scores = q @ k.T  # (B, L, L, n_heads)
positional_bias = bias(positional_idx)  # (L, L, n_heads)
scores += positional_bias
# weights = softmax(scores)
```

**self-attention**

```python
class EncoderSelfAttention(torch.nn.Module):
    def __init__(self, d_model=768, d_qkv=64, n_heads=12,
        relative_attention_num_buckets=32, has_relative_bias=False, dropout_rate=0.1):
        """
        relative_attention_num_buckets: 见后面关于positional bias的说明
        has_relative_bias: 第1个EncoderBlock取值为True, 其余均为False
        """
        super().__init__()
        self.inner_dim = d_qkv * n_heads
        self.q, self.k, self.v = [nn.Linear(d_model, self.inner_dim) for i in range(3)]
        self.o = nn.Linear(self.inner_dim, d_model)
        self.dropout_rate = dropout_rate
        if has_relative_bias:
            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)

    def compute_bias(self, q_len=512, k_len=512):
        # q_len和k_len都是encoder输入的序列长度
        # 在decoder的self-attention的训练阶段, q_len和k_len都是decoder的输入长度
        
        # positions: (q_len, k_len) long tensor
        # 每个元素的取值范围都是[0, self.relative_attention_num_buckets=32)
        positions = get_relative_idx(q_len, k_len)
        
        bias = self.relative_attention_bias(positions).unsqueeze(0)  # (1, q_len, k_len, n_heads)
        bias = bias.transpose(0, 3, 1, 2)
        # bias: (1, n_heads, q_len, k_len), 其中第0维在计算中被广播, 即(B, n_heads, q_len, k_len)
        return bias
    
    def forward(self, hidden, attention_mask, bias=None):
        """
        Args:
            hidden: (B, L, d_model)
            attention_mask: (B, L) LongTensor, 有token的地方为1, pad处为0
            bias: 第1层输入为None, 后续层将第一层输出的bias作为输入
        """
        # q, k, v: (B, L, self.inner_dim)
        q, k, v = self.q(hidden), self.k(hidden), self.v(hidden)
        q = q.reshape(B, L, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, L=q_len, d_qkv)
        k = k.reshape(B, L, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, L=k_len, d_qkv)
        v = v.reshape(B, L, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, L=k_len, d_qkv)
        
        scores = torch.matmul(q, k.transpose(2, 3))  # (B, n_head, L, L)
        if bias is None:
            bias = self.compute_bias(L, L)  # (1, n_head, L, L)
            extended_mask = torch.where(attention_mask[:, None, None, :]==1, 0, -inf)  # (B, 1, 1, L)
            bias = bias + extended_mask  # (B, n_head, L, L)
        scores += bias
        attn_weights = nn.functional.softmax(scores, dim=-1)
        attn_weights = nn.functional.dropout(attn_weights, self.dropout_rate)
        hidden = torch.matmul(atten_weights, v)  # (B, n_heads, L, d_qkv)
        hidden = hidden.transpose(1, 2).view(B, L, self.inner_dim)  # (B, L, inner_dim)
        hidden = self.o(hidden)  # (B, L, d_model)
        return hidden, bias
```


#### Feed-Forward

见下图，含义自明

![](../assets/figures/t5/feed-forward.png)



### decoder

#### Self-Attention Layer (Decoder)

与 `Self-Attention Layer(Encoder)` 的计算过程一致, 但有如下两个区别：

- positional bias 使用单向的方式进行获取

- `mask` 有些变化:

  ```python
  bias = self.compute_bias(L, L)  # (1, n_head, L=trg_len, L=trg_len)
  mask = torch.triu(torch.ones((B, 1, L, L)))  # (B, 1, L, L), 下三角含对角线为1, 其余均为0
  extended_mask = torch.where(mask==1, 0, -inf)  # 下三角含对角线为0, 其余均为-inf
  bias = bias + extended_mask  # (B, n_head, L, L)
  ```

#### Cross-Attention Layer (Decoder)

```python
class DecoderCrossAttention(torch.nn.Module):
    def __init__(self, d_model=768, d_qkv=64, n_heads=12, dropout_rate=0.1):
        # 没有postion bias的计算
        super().__init__()
        self.inner_dim = d_qkv * n_heads
        self.q, self.k, self.v = [nn.Linear(d_model, self.inner_dim) for i in range(3)]
        self.o = nn.Linear(self.inner_dim, d_model)
        self.dropout_rate = dropout_rate
    
    def forward(self, decoder_hidden, encoder_hidden, encoder_attention_mask):
        """
        Args:
            decoder_hidden: (B, trg_len, d_model)
            encoder_hidden: (B, src_len, d_model)
            encoder_attention_mask: (B, L) LongTensor, 输入序列有token的地方为1, pad处为0
        """
        q, k, v = self.q(decoder_hidden), self.k(encoder_hidden), self.v(encoder_hidden)
        q = q.reshape(B, trg_len, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, q_len=trg_len, d_qkv)
        k = k.reshape(B, src_len, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, k_len=src_len, d_qkv)
        v = v.reshape(B, src_len, n_heads, d_qkv).transpose(1, 2)  # (B, n_heads, k_len=src_len, d_qkv)
        
        scores = torch.matmul(q, k.transpose(2, 3))  # (B, n_heads, trg_len, src_len)
        
        bias = torch.zeros(B, n_heads, trg_len, src_len)  # (1, n_heads, trg_len, src_len)
        extended_mask = torch.where(attention_mask[:, None, None, :]==1, 0, -inf)  # (B, 1, 1, src_len)
        bias = bias + extended_mask  # (B, n_heads, trg_len, src_len)
        scores += bias

        attn_weights = nn.functional.softmax(scores, dim=-1)
        attn_weights = nn.functional.dropout(attn_weights, self.dropout_rate)
        hidden = torch.matmul(atten_weights, v)  # (B, n_heads, trg_len, d_qkv) = (B, n_heads, trg_len, src_len) * (B, n_heads, src_len, d_qkv)
        hidden = hidden.transpose(1, 2).view(B, trg_len, self.inner_dim)  # (B, trg_len, inner_dim)
        hidden = self.o(hidden)  # (B, trg_len, d_model)
        return hidden, bias
```

## 源码解析：训练过程的前向计算流程

如果对T5的计算逻辑基本熟悉的话，这里给出 huggingface transformers 中的模型层次，可以帮助快速理解源码的实现逻辑：

```yaml
# 注意: T5Attention 这个类同时实现了三类注意力机制
T5ForConditionalGeneration:
  - nn.Embedding  # A
  - encoder: T5Stack
    - nn.Embedding  # 与 A 是同一个
    - T5Block 
      - T5LayerSelfAttention
        - T5LayerNorm
        - T5Attention  # 全自注意力, 位于第一个T5Block中此模块含有一个nn.Embedding用于学习relative postional bias, 可学习参数形状:(num_bucket=32, num_heads=64)
        - nn.Dropout
      - T5LayerFF
    - T5Block
    ...
    - T5Block
    - T5LayerNorm
    - nn.Dropout
  - decoder: T5Stack
    - nn.Embedding  # 与 A 是同一个
    - T5Block
      - T5LayerSelfAttention
        - T5LayerNorm
        - T5Attention  # 因果自注意力, 位于第一个T5Block中此模块含有一个nn.Embedding用于学习relative postional bias, 可学习参数形状:(num_bucket=32, num_heads=64)
        - nn.Dropout
      - T5LayerCrossAttention
        - T5LayerNorm
        - T5Attention  # 与encoder的输出做注意力, 没有relative postional bias
        - nn.Dropout
      - T5LayerFF
    - T5Block
    ...
    - T5Block
    - T5LayerNorm
    - nn.Dropout
  - nn.Linear  # T5中的设计里与 A 共享参数
```

备注：在 huggingface transformers 的源码实现里 `T5Attention` 比较复杂，它需要承担几项不同的工作：

- 训练阶段：
  - 在 encoder 中执行全自注意力机制
  - 在 decoder 中的 `T5LayerSelfAttention` 中执行因果自注意力机制（训练时因为可以并行计算整个decoder序列的各个隐层向量，不需要考虑decoder前序token的key和value的缓存）
  - 在 decoder 中的 `T5LayerCrossAttention` 中执行对encoder输出的注意力机制（训练时因为可以并行计算整个decoder序列的各个隐层向量，不需要考虑encoder最后一层的key和value的缓存）
- 推理阶段：
  - 在 encoder 中执行全自注意力机制，与训练时完全一致
  - 在 decoder 中的 `T5LayerSelfAttention` 中执行因果自注意力机制（推理时是串行解码，因此需要缓存decoder的之前所有token的key和value的缓存，计算当前token的隐层向量时也把当前token的key和value也缓存下来供后续计算）
  -  在 decoder 中的 `T5LayerCrossAttention` 中执行对encoder输出的注意力机制（推理时是串行解码，因此解码第一个字符时会缓存每一层针对encoder输出向量的key和value，解码后续字符时直接使用这些key和value缓存进行计算）

下面将不再按照 huggingface transformers 的源码进行梳理，而是直接手写大部分层的实现来讲解，手写实现与 huggingface 实现的对应也在各小节给出。更为完整的对应关系可以参考：[../assets/code/t5](../assets/code/t5)



## 原理解析：文本生成策略

本节总结了 huggingface 的源码阅读后各种生成方式的详细算法

### beam_search/greedy_search

- 当 beam_size 为 1 时退化为 greedy_search

```
logits_processor: 实际上是对当前预测的log-softmax分数进行后处理(例如重复出现的字做些score上的惩罚)
stopping_criteria: 判断是否应该结束, 返回True表示应该结束(最典型的是beam达到最大长度)

input_ids: 形状为(batch_size*beam_size, 1)  # 全部为decoder_satrt_token
beam_scores: 形状为(batch_size, beam_size)  # 其中每一行的第一个元素为0, 其余元素为-inf
beam_scores.view(-1, 1)
beam_hypotheses: batch_size个候选池
is_done: 初始化为batch_size个False

while True:

  截取最后一个input_ids得到input_tensor(batch_size*beam_size, 1)
  通过前向计算得到logits(batch_size*beam_size, vocab_size)后
  进行log_softmax之后得到next_token_scores
  next_token_scores_processed = logits_processor(input_ids, next_token_scores)  # (batch_size*beam_size, vocab_size)
  next_token_scores = beam_scores + next_token_scores_processed  # (batch_size*beam_size, vocab_size)

  next_token_scores.view(batch_size, beam_size*vocab_size)
  # 对于batch中的每一个, 都留下2*beam_size个可选项（注意根据此处的规则这些可选项里至多有beam_size个eos）
  next_token_scores, next_tokens = next_token_scores.topk(2*beam_size)

  # 这个过程的逻辑如下:
  对于每个样本
    如果is_done取值为True, 则为每个beam填充pad_token, continue
    从next_token_scores最大的开始
      如果对应的预测token为eos：
        如果此score是前beam_size大的score, 则将其加入该样本对应的候选集beam_hypotheses
          加入规则如下：
            如果候选池当前不足beam_size个样本, 则直接将其加入, 并计算score计算长度惩罚后，更新池子中的最差分数
            如果当前候选池已有beam_size个样本, 则对此score计算长度惩罚后与池子里的score进行比较，如果优于最差分数，则加入并剔除池子里最差的那个序列, 之后更新池子的最差分数
            备注: 池子中的score均为长度惩罚后的score
        如果此score不是前beam_size大的score, 则直接忽略这个样本
      如果对应的预测token不是eos：
        将其加入到beam_scores, beam_next_tokens中直到达到beam_size个
    判断beam_hypotheses是否完成，由此更新is_done
      判断规则如下：
        (1) 如果beam_hypotheses的模式为early_stop, 那么只要池子里有num_beam个样本, 就认为搜索结束
        (2) 非early_stop模式, 则根据beam_scores中的最大者是否在计算长度惩罚后比候选池中的最差分数大, 如果更大则继续搜索(is_done=False), 如果更小则认为搜索结束(is_done=True)

  beam_scores, beam_next_tokens = beam_scorer.process(input_ids, next_token_scores, next_tokens) # 逻辑见前面一大段的说明
  input_ids = cat(input_ids, beam_next_tokens)

  如果is_done均为True或者stopping_criteria(input_ids, scores)为True, 则跳出while True

最后做收尾:
  对于还没结束的beam, 尝试添加至beam_hypotheses中
  将输出序列使用pad_token补齐
```

### group_beam_search

group_beam_search与beam_search的区别在于, 将当前的beam分为若干组, 每组group_size个序列, 每次对这个序列做beam_search, 并留下group_size个序列, 这样总共仍留有beam_size个序列
- 当 group_size 与 beam_size 相等时, 退化为beam_search

### beam_sample/sample

beam_sample与beam_search的区别在于将这几行

```
next_token_scores_processed = logits_processor(input_ids, next_token_scores)  # (batch_size*beam_size, vocab_size)
next_token_scores = beam_scores + next_token_scores_processed  # (batch_size*beam_size, vocab_size)
next_token_scores.view(batch_size, beam_size*vocab_size)
next_token_scores, next_tokens = next_token_scores.topk(2*beam_size)
```

替换为

```
logit_warpper: 通常进行top-k/top-p修改分数/不修改分数, 影响后续的抽样结果

next_token_scores_processed = logits_processor(input_ids, next_token_scores)  # (batch_size*beam_size, vocab_size)
next_token_scores = beam_scores + next_token_scores_processed  # (batch_size*beam_size, vocab_size)
next_token_scores.view(batch_size, beam_size*vocab_size)

next_token_scores = logits_warper(input_ids, next_token_scores)
probs = nn.functional.softmax(next_token_scores, dim=-1)
next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)
next_token_scores根据next_tokens的选择得到
```

### constrained_beam_search【待补充】


### contrastive_search【待补充】

## 源码解析：文本生成策略

本节主要按照 Huggingface Transformers 的源码进行介绍，按照 huggingface 中的实现，`T5ForConditionalGeneration` 的继承关系如下：

```python
class T5ForConditionalGeneration(T5PreTrainedModel):
  # 用于 load 权重时可以忽略
  _keys_to_ignore_on_load_missing = [
        r"encoder.embed_tokens.weight",
        r"decoder.embed_tokens.weight",
        r"lm_head.weight",
  ]
  _keys_to_ignore_on_load_unexpected = [
      r"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight",
  ]
  # ...

class T5PreTrainedModel(PreTrainedModel):
  config_class = T5Config
  base_model_prefix = "transformer"  # 在 from_pretrained 函数中 load 权重时有用
  def _init_weights(self, module):
    ...
  # 其他一些方法和属性从略

class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):
  pass
```

而与生成相关的代码主要实现在 `GenerationMixin` 中，一般而言，通过这个类的 `generate` 方法进行使用，根据不同的参数设定，会实际上通过调用以下 7 种方法来完成实际的生成：

- `greedy_search`
- `beam_search`
- `sample`
- `beam_sample`
- `group_beam_search`
- `constrained_beam_search`
- `contrastive_search`

`generate` 方法如下:

```python
def generate(
  self,
  inputs: Optional[torch.Tensor] = None,
  generation_config: Optional[GenerationConfig] = None,
  logits_processor: Optional[LogitsProcessorList] = None,
  stopping_criteria: Optional[StoppingCriteriaList] = None,
  prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
  synced_gpus: Optional[bool] = False,
  **kwargs):
  # 只摘录核心部分
  if generation_config is None:
    generation_config = self.generation_config

  generation_config = copy.deepcopy(generation_config)
  # 根据kwargs更新
  model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs
  self._validate_model_kwargs(model_kwargs.copy())

  logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
  stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
  inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(
    inputs, generation_config.bos_token_id, model_kwargs
  )
  # 再根据generation_config增加一部分logits_processor
  logits_processor = self._get_logits_processor(generation_config, input_ids_seq_length, encoder_input_ids,
    prefix_allowed_tokens_fn, logits_processor)
  # 再根据generation_config增加一部分stopping_criteria
  stopping_criteria = self._get_stopping_criteria(generation_config, stopping_criteria)
  # 根据不同的generation_config设置, 分别调用上述7种方法
  ...
```

备注：此处的 `generation_config` 变量的类型为 `GenerationConfig`, 而 `self.generation_config` 是 `PreTrainedModel` 实例化时得到的。

【此处需增加一个隐藏按钮】
```python
class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):
  def __init__(self, config, *inputs, **kwargs):
    # *inputs, **kwargs 在此处未被使用到
    super().__init__()  # nn.Module的__init__函数, 其余继承类均为Mixin, 没有__init__函数
    self.config = config
    self.name_or_path = config.name_or_path
    self.warnings_issued = {}
    self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None
  @classmethod
  def from_pretrained(self, pretrained_model_name_or_path, *model_args, **kwargs):
    # 只摘录重要的部分
    config = kwargs.pop("config", None)
    if not isinstance(config, PretrainedConfig):
      config_path = config if config is not None else pretrained_model_name_or_path
      config, model_kwargs = cls.config_class.from_pretrained(..., **kwargs)
    else:
      model_kwargs = kwargs
    model = cls(config, *model_args, **model_kwargs)  # 调用 __init__
    state_dict = load_state_dict(resolved_archive_file)
    model, ... = cls._load_pretrained_model(model, state_dict, ...)  # load权重至模型
    model.eval()  # 将模型设置为eval模式
    if model.can_generate():  # 如果pretrained_model_name_or_path目录下包含generation_config.json文件, 则按这个文件重新初始化model.generation_config
      model.generation_config = GenerationConfig.from_pretrained(pretrained_model_name_or_path, ..., **kwargs)
  def save_pretrained(self, save_directory, ...):
    # 只摘录重要的部分
    os.makedirs(save_directory, exist_ok=True)
    model_to_save = unwrap_model(self)
    if is_main_process:
      model_to_save.config.save_pretrained(save_directory)
      if self.can_generate():
          model_to_save.generation_config.save_pretrained(save_directory)
    if state_dict is None:
      state_dict = model_to_save.state_dict()
    # 某些参数在保存时可以忽略
    if self._keys_to_ignore_on_save is not None:
        for ignore_key in self._keys_to_ignore_on_save:
            if ignore_key in state_dict.keys():
                del state_dict[ignore_key]
    # 在正常情况下(模型参数不多), shard是一个只有一个键值对的字典, key为"pytorch_model.bin", value即为state_dict
    weights_name = SAFE_WEIGHTS_NAME if safe_serialization else WEIGHTS_NAME
    shards, index = shard_checkpoint(state_dict, max_shard_size=max_shard_size, weights_name=weights_name)
    for shard_file, shard in shards.items():
        if safe_serialization:
            safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={"format": "pt"})
        else:
            save_function(shard, os.path.join(save_directory, shard_file))
    # 保存index
    ...
```