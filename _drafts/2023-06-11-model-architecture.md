---
layout: post
title: "(P1) NLP Model Architecture Examples"
date: 2023-06-11 17:20:04 +0800
labels: [transformers]
---

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- æ¢³ç†ç›®å‰è¾ƒä¸ºç»å…¸çš„ NLP æ¨¡å‹çš„ç»“æ„ (åœ¨åŸå§‹ Transformers ç»“æ„ä¸Šçš„å¾®å°æ”¹åŠ¨çš„éƒ¨åˆ†), ä¹Ÿä¸ºäº†æ–¹ä¾¿æ¨¡å‹ç»“æ„/è§„æ¨¡å¯¹æ¨ç†æ€§èƒ½çš„å½±å“

å‚è€ƒèµ„æ–™

- ğŸ¤— Transformer çš„å®ç°

æ¶‰åŠå†…å®¹

- æ¨¡å‹ç»“æ„
- å…·ä½“çš„è¶…å‚æ•°, ä¾‹å¦‚å±‚æ•°, ç‰¹å¾å‘é‡ç»´æ•°ç­‰
- å„ä¸ªæ¨¡å‹é—´çš„â€œæ¸Šæºâ€æ¢³ç†

ä¸æ¶‰åŠå†…å®¹

- è®­ç»ƒæ•°æ®é‡, tokenizer, è®­ç»ƒæ–¹æ³• (è¿™ä¸€éƒ¨åˆ†åœ¨å¦ä¸€ç¯‡åšå®¢ llm-survey ä¸­åšä»‹ç»), å› æ­¤å¯ä»¥è®¤ä¸ºæœ¬åšå®¢åªæ˜¯ä»‹ç»éšæœºåˆå§‹åŒ–çš„æ¨¡å‹é•¿ä»€ä¹ˆæ ·.

## æ¸Šæº

## Blocks

### RoPE ä½ç½®ç¼–ç 

Rotary Position Embedding ä½ç½®ç¼–ç è¢«å¹¿æ³›ç”¨äºåç»­çš„æ¨¡å‹ç»“æ„é‡Œ:

$$
\text{position\_enc} = \begin{bmatrix}
0\cdot10000^{-\frac{0}{d} }&0\cdot10000^{-\frac{2}{d} }&\cdots&0\cdot10000^{-\frac{d-2}{d} } \\
1\cdot10000^{-\frac{0}{d} }&1\cdot10000^{-\frac{2}{d} }&\cdots&1\cdot10000^{-\frac{d-2}{d} } \\
\vdots & \vdots & \ddots & \vdots\\
(L-1)\cdot10000^{-\frac{0}{d} }&(L-1)\cdot10000^{-\frac{2}{d} }&\cdots&(L-1)\cdot10000^{-\frac{d-2}{d} } \\
\end{bmatrix}
$$

- å…¬å¼å‚è€ƒ [ä½œè€…åŸæ–‡å…¬å¼](https://spaces.ac.cn/archives/8265)
- å®ç°å‚è€ƒ: [huggingface](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roformer/modeling_roformer.py)

```python
# ä½¿ç”¨é€»è¾‘: æ¯ä¸€æ¬¡ attention ä¹‹å‰éƒ½å…ˆå¯¹ query, key, value åšæ—‹è½¬å¤„ç† (value æœªå¿…éœ€è¦)

# ============ æ—‹è½¬ ===============
# query, key, value: (B, num_head, L, head_dim)
position_enc = torch.tensor([[i * 10000 ** (-2*j/head_dim) for j in range(head_dim//2)] for i in range(L)])  # (L, head_dim // 2)
# sin: (L, head_dim // 2), cos: (L, head_dim // 2)
sin, cos = torch.sin(position_enc), torch.cos(position_enc)

# sin [Î¸0,Î¸1,Î¸2......Î¸d/2-1] -> sin_pos [Î¸0,Î¸0,Î¸1,Î¸1,Î¸2,Î¸2......Î¸d/2-1,Î¸d/2-1]
sin_pos = torch.stack([sin, sin], dim=-1).reshape_as(sinusoidal_pos)
# cos [Î¸0,Î¸1,Î¸2......Î¸d/2-1] -> cos_pos [Î¸0,Î¸0,Î¸1,Î¸1,Î¸2,Î¸2......Î¸d/2-1,Î¸d/2-1]
cos_pos = torch.stack([cos, cos], dim=-1).reshape_as(sinusoidal_pos)
# rotate_half_query [-q1,q0,-q3,q2......,-qd-1,qd-2]
rotate_half_query = torch.stack([-query[..., 1::2], query[..., ::2]], dim=-1).reshape_as(query)
query = query * cos_pos + rotate_half_query * sin_pos
# apply to key, value

# ========= æ™®é€šçš„ dot-product attention ==========
score = torch.matmul(query_layer, key_layer.transpose(-1, -2))
# ...
```

### RMSNorm

ä»¥ä¸‹æ¨¡å‹å‡é‡‡ç”¨äº† RMSNorm:

- T5
- Llama

LayerNorm: å¯¹äºä¸€ä¸ªè¾“å…¥å½¢çŠ¶ä¸º `(B, L, C)` çš„è¾“å…¥, å¯¹æœ€åä¸€ç»´åšå‡å‡å€¼é™¤æ–¹å·®çš„æ“ä½œ, æ•°å­¦è¡¨è¿°å¦‚ä¸‹: $\mathbf{x}$ ä¸ºä¸€ä¸ª $n$ ç»´å‘é‡, LayerNorm çš„æ“ä½œå¦‚ä¸‹:

$$
\begin{align*}
\bar{x}&=\text{Mean}(\mathbf{x}) \\
\text{Var}(\mathbf{x}) &= \frac{\sum_{i=1}^{n}{(x_i - \bar{x})^2} }{n} \\
\hat{x}_i &= \gamma * \frac{x_i-\text{Mean}(x)}{\sqrt{\text{Var}(\mathbf{x})+\epsilon} } + \beta
\end{align*}
$$

RMSNorm çš„æ•°å­¦è¡¨ç¤ºå¦‚ä¸‹:

$$
\begin{align*}
\text{RMS}(\mathbf{x}) &= \frac{\sum_{i=1}^{n}{x_i^2} }{n} \\
\hat{x}_i &= \gamma * \frac{x_i}{\sqrt{\text{RMS}(\mathbf{x}) + \epsilon} }
\end{align*}
$$

### Multi-Query Attention

ä»¥ä¸‹æ¨¡å‹é‡‡ç”¨äº† Multi-Query Attention

- chatglm2

```python
# x: (B, L, D), D = head_dim * num_heads
# qkv_layer: Linear(in_features=D, out_features=D+2*head_dim)
q, k, v = qkv_layer(x).split([D, head_dim, head_dim], dim=-1)

q = q.reshape(B, L, num_head, head_dim).transpose(1, 2)  # (B, num_head, L, head_dim)
k = k.reshape(B, L, 1, head_dim).transpose(1, 2)         # (B, 1, L, head_dim)
v = v.reshape(B, L, 1, head_dim).transpose(1, 2)         # (B, 1, L, head_dim)

scores = q @ k.transpose(-1, -2)                         # scores: (B, num_head, L, L)
weight = torch.nn.softmax(scores, dim=-1)
y = weight @ v  # (B, num_head, L, L) x (B, 1, L, head_dim) = (B, num_head, L, head_dim)
y = y.transpose(1, 2).reshape(B, L, D)
```

ä¸»è¦çš„åŠ é€Ÿåœ¨äºä¸¤ç‚¹(ä¸ªäººè§‚ç‚¹, è¿˜éœ€æ‰¾åŸè®ºæ–‡æ ¸å¯¹):

- `qkv_layer` æ¶‰åŠçš„çŸ©é˜µè¿ç®—çš„è®¡ç®—é‡å‡å°‘äº†
- `scores = q @ k.transpose(-1, -2)` æ¶‰åŠçš„è¿ç®—é‡ä¸å˜, ä½†åœ¨æ¨ç†æ—¶, `q` å¯¹åº”çš„ `L=1`, è€Œ `k` å¯¹åº”çš„ `L` ä¼šæŒç»­å¢é•¿, å› æ­¤å†…å­˜å¸¦å®½å ç”¨ä¸º $O(B\times D+B\times L \times head\_dim)$, å…¶ä¸­ç¬¬ä¸€é¡¹æ˜¯ `q` çš„å…ƒç´ ä¸ªæ•°, ç¬¬äºŒé¡¹æ˜¯ `k` çš„å…ƒç´ ä¸ªæ•°, è€ŒåŸå§‹çš„ attention åœ¨æ­¤å¤„çš„å†…å­˜å¸¦å®½å ç”¨é‡ä¸º $O(B\times D+B\times L \times D)$
- `y = weight @ v` åŒç†, æ¶‰åŠçš„è¿ç®—é‡ä¸å˜, åœ¨æ¨ç†æ—¶, å†…å­˜å¸¦å®½å ç”¨ä¸º $O(B\times num\_head \times L+B\times num\_head \times L)$, è€ŒåŸå§‹çš„ attention åœ¨æ­¤å¤„çš„å†…å­˜å¸¦å®½å ç”¨é‡ä¸º $O(B\times num\_head \times L+B\times D \times L)$

ä»åŸå§‹è®ºæ–‡çš„å®éªŒç»“æœçœ‹, ç›¸æ¯”äºåŸå§‹çš„ attention æœºåˆ¶, ä½¿ç”¨ Multi-Query Attention å¯èƒ½ä¼šå¯¹æ¨ç†é€Ÿåº¦æå‡ 5-10 å€, ä½†è®­ç»ƒé€Ÿåº¦æå‡å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡.


## GPT-2

```yaml

GPT2Model
  - wte: nn.Embedding
  - wpe: nn.Embedding
  - drop: nn.Dropout
  - h: nn.ModuleList
    - block: GPT2Block
        - ln_1: nn.LayerNorm
        - attn: GPT2Attention
        - ln_2: nn.LayerNorm
        - mlp: nn.ModuleList([linear(C, 4C), gelu, linear(4C, C), dropout])
    - block: GPT2Block
    - ...
  - ln_f: nn.LayerNorm
```

```python
class GPT2Attention:
    def forward(self, hidden_states):
        # (B, L, C) -> (B, L, 3C) -> split
        q, k, v = self.linear(hidden_states).split(C, dim=2)
        # (B, L, C) -> (B, num_head, L, head_dim)
        q, k, v = transpose(q), transpose(k), transpose(v)

        # (B, num_head, L, L)
        attn_weights = torch.matmul(q, k.transpose(-1, -2)) / (head_dim ** 0.5)
        # mask: (B, 1, 1, L) = (1.0 - attention_mask) * torch.finfo(dtype).min
        attn_weights += mask
        attn_weights = dropout(softmax(attn_weights, dim=-1))
        attn_output = torch.matmul(attn_weights, v)  # (B, num_head, L, head_dim)
        
        hidden_states = transpose(attn_output)  # (B, L, C)
        hidden_states = dropout(self.out_linear(hidden_state))
        k, v = concat(past_k, k), concat(past_v, v)
        return hidden_states, (k, v), attn_weights

GPT2MLP: nn.ModuleList([nn.Linear(C, 4*C), gelu, nn.Linear(4*C, C), dropout])

class GPT2Block:
    def forward(self, hidden_states):
        # hidden_states: (B, L, C)
        residual = hidden_states
        hidden_states = self.ln_1(hidden_states)  # (B, L, C) -> (B, L, C)
        # (B, L, C), ((B, num_head, L, head_dim), (B, num_head, L, head_dim)), (B, num_head, L, L)
        (hidden_states, (key, value), attn_weights) = self.attn(hidden_states, ...)
        hidden_states = hidden_states + residual

        residual = hidden_states
        hidden_states = self.ln_2(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = hidden_states + residual
        return hidden_states, (key, value), attn_weights

class GPT2Model:
    def forward(self, input_ids, attention_mask):
        inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)  # position_ids: range(0, L)
        hidden_states = dropout(inputs_embeds + position_embeds)
        for block in self.h:
            hidden_states, (key, value), attn_weights = block(hidden_states)
        last_hidden_states = self.ln_f(hidden_states)  # layernorm
        return (
            last_hidden_states,
            [(key, value), (key, value), ...],
            all_hidden_states,  # list
            attentions  # list of attn_weights
        )
```

## GPT-J

## GPT-Neo/GPT-NeoX

## LLAMA

## Open-LLAMA

## RWKV

## GLM

- å½’ä¸€åŒ–: RMSNorm
- æ¿€æ´»å‡½æ•°:
- ä½ç½®ç¼–ç : RoPE
- æ³¨æ„åŠ›: é‡‡ç”¨ Multi-Query Attention, å®ç°ä¸Šè¿˜åˆ©ç”¨äº† flashattention-v1

## MOSS

åŸºæœ¬ä¸Šå®Œå…¨å°±æ˜¯ GPTJ çš„ç»“æ„?

- å½’ä¸€åŒ–: RMSNorm
- æ¿€æ´»å‡½æ•°:
- ä½ç½®ç¼–ç : RoPE
- æ³¨æ„åŠ›: æ™®é€šçš„è‡ªæ³¨æ„åŠ›

## internlm

- å½’ä¸€åŒ–: RMSNorm
- æ¿€æ´»å‡½æ•°:
- ä½ç½®ç¼–ç : RoPE(å¯èƒ½æœ‰ä¸€äº›ä¿®æ”¹?)
- æ³¨æ„åŠ›: æ™®é€šçš„è‡ªæ³¨æ„åŠ›