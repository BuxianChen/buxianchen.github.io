---
layout: post
title: "(WIP) LLMs Survey"
date: 2023-05-04 14:31:04 +0800
labels: [paper]
---

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- ç§¯ç´¯ä¸€äº›å…³äºå¤§æ¨¡å‹çš„åŸºæœ¬çŸ¥è¯†(å¸¸ç”¨æœ¯è¯­ã€æŒ‡æ ‡)ï¼Œä»¥åŠä¸€äº›å…·ä½“çš„æ•°å€¼

å‚è€ƒèµ„æ–™

- 2021.03ç»¼è¿°ï¼š[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08241)
- 2023.04ç»¼è¿°ï¼š[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)

æ¶‰åŠå†…å®¹

- å¤§æ¨¡å‹ï¼ˆä¾‹å­ï¼‰çš„è®­ç»ƒæ•°æ®é›†æ„æˆã€æ¨¡å‹å¤§è‡´æ¶æ„åŠç›®æ ‡å‡½æ•°
- ä¸€äº›å¯ä»¥å‚ç…§çš„æ•°å€¼ï¼Œä¾‹å¦‚æŸæ¨¡å‹å¯¹å¤šå¤§æ•°æ®é‡éœ€è¦ä½¿ç”¨å¤šå°‘æ˜¾å­˜è®­ç»ƒå¤šä¹…

ä¸æ¶‰åŠå†…å®¹

- å¤§æ¨¡å‹ç»“æ„çš„ä¸€äº›å…·ä½“ç»†èŠ‚ï¼ˆæš‚æ—¶æ‰“ç®—å¦èµ·ä¸€ç¯‡åšå®¢ç»“åˆ ğŸ¤— Transformers ä¸­çš„å…·ä½“å®ç°è¿›è¡Œä»‹ç»ï¼‰

## æœ¯è¯­

**petaflop/s-day(pfs-day)**

- K: 10^3 æˆ– 2^10
- M: 10^6 æˆ– 2^20
- G: 10^9 æˆ– 2^30
- T: 10^12 æˆ– 2^40
- P: 10^15 æˆ– 2^50

petaflop/s-day(pfs-day) æ˜¯è®¡ç®—é‡çš„å•ä½ã€‚1pfs-dayä¸ºï¼šå‡è®¾è®¡ç®—æœºæ¯ç§’è®¡ç®— 1 åƒä¸‡æ¬¡ï¼ˆ10^15æˆ–2^50ï¼‰ï¼Œè®¡ç®—æœºè®¡ç®— 1 å¤©ï¼ˆ86400ç§’çº¦10^5ï¼‰çš„æ€»è®¡ç®—é‡ï¼ˆçº¦ä¸º10^20)ã€‚

å‚è€ƒèµ„æ–™ï¼š[OpenAI blog](https://openai.com/research/ai-and-compute)

## ç¡¬ä»¶

[V100 è®¡ç®—èƒ½åŠ›](https://images.nvidia.cn/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf): 

## å¤§æ¨¡å‹çš„ Scaling Law

- [OpenAI paper(2020.1) Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)
- Chinchilla Scaling Law: [DeepMind paper(2022) Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)