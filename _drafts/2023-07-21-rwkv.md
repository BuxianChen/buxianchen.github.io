---
layout: post
title: "(WIP) RWKV 浅析"
date: 2023-07-21 10:31:04 +0800
labels: [paper]
---

## 动机、参考资料、涉及内容

## AFT

一般意义上的 attention 可以用这种方式进行定义: $V\in\mathbb{R}^{L\times d}$ 代表需要被注意的值, $\mathbf{y}\in\mathbb{R}^{d}$

$$
\mathbf{y}=\sum_{l=1}^{L}{a_l V_{l, :}}
$$

写成分量形式:

$$
y_i = \langle \mathbf{a}, V_{:, i}\rangle
$$


标准的 scale-dot self attention, 忽略多个头, 计算公式如下: $Q\in\mathbb{R}^{L\times d}, K\in\mathbb{R}^{L\times d}, V\in\mathbb{R}^{L\times d}$,

$$
Y = \text{softmax}(\frac{QK^T}{\sqrt{d} })V
$$

其中 $[\text{softmax}(A)]_{i, j}=\frac{\exp(A_{i, j})}{\sum_{j=1}^{L}\exp(A_{i, j})}$

AFT 的计算公式如下: $Q\in\mathbb{R}^{L\times d}, K\in\mathbb{R}^{L\times d}, V\in\mathbb{R}^{L\times d}, W\in\mathbb{R}^{L\times L}$

