---
layout: post
title: "(WIP) Memory efficient inference (ğŸ¤—'s dispatch and offload)"
date: 2023-06-01 17:20:04 +0800
labels: [transformers]
---

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- ğŸ¤— transformer/accelerate çš„ `init_empty_weight` å’Œ `dispatch_model` ç­‰ API

å‚è€ƒèµ„æ–™

- [https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling)
- [https://huggingface.co/blog/accelerate-large-models](https://huggingface.co/blog/accelerate-large-models)

æ¶‰åŠå†…å®¹

- ä½¿ç”¨æ–¹æ³•ã€æ ·ä¾‹ã€‘
- æ–‡æ¡£è¡¥å……ï¼šæŸäº›è¾…åŠ©å‡½æ•°å®˜æ–¹æ–‡æ¡£ç¼ºå¤±ï¼ŒæŸäº›å¯¹å¤–APIçš„å‚æ•°è¯´æ˜ä¸æ˜ç¡®
- æºç åˆ†æ

## æ ·ä¾‹ä»£ç 

```
# requirements.txt
torch >= 1.10.0
transformers==4.29.2
```

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausualLM, AutoConfig
from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch

pretrained_name_or_path = "./fnlp/moss-moon-003-sft"

# method 1: è°ƒç”¨ç›¸å…³çš„è¾…åŠ©å‡½æ•°(æœ‰åˆ©äºäº†è§£æ•´ä¸ªè¿‡ç¨‹)
config = AutoConfig.from_pretrained(path, trust_remote_code=True)
with init_empty_weights():
    model = AutoModelForCausalLM.from_config(
        config,
        # åœ¨æ‰§è¡Œ cls(...) ä¹‹å‰, ä¼šåˆ©ç”¨ torch.set_default_dtype(torch_dtype)å…¨å±€è®¾å®šé»˜è®¤æµ®ç‚¹æ•°ç±»å‹
        torch_dtype=torch.float16,
        trust_remote_code=True)
# !!!è¿™ä¸€æ­¥æ˜¯å¿…é¡»çš„!!!
model.tie_weights()

max_memory = {
    0: "10GB",
    1: "10GB",
    "cpu": "20GB",
}

device_map = infer_auto_device_map(
    model,
    max_memory=max_memory,
    dtype=None,  # ä½¿ç”¨modelä¸­çš„tensorç±»å‹ä¼°ç®—æ‰€éœ€çš„å†…å­˜/æ˜¾å­˜
    no_split_module_classes=model._no_split_modules,  # æŸäº›submoduleå¿…é¡»æ”¾åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Š
)

# å‰é¢çš„ä¸¤ä¸ªæ­¥éª¤æ˜¯ä¸ºäº†è·å–åˆé€‚çš„ device_map

# å†™æ³•1: å¾…ç¡®è®¤
# model = AutoModelForCausalLM.from_pretrained(
#     pretrained_name_or_path,
#     trust_remote_code=True,
#     # åœ¨æ‰§è¡Œ cls(...) ä¹‹å‰, ä¼šåˆ©ç”¨ torch.set_default_dtype(torch_dtype)å…¨å±€è®¾å®šé»˜è®¤æµ®ç‚¹æ•°ç±»å‹
#     torch_dtype=torch.float16,  
#     device_map=device_map,
#     offload_folder="offload",
#     offload_state_dict=True,  # ??
#     # low_cpu_memory_usage=True,  # æŸäº›æƒ…å†µä¸‹ä¼šæ ¹æ®å…¶ä»–å‚æ•°è‡ªåŠ¨è®¾å®šä¸ºTrue
#     # _fast_init=True,  # é»˜è®¤å€¼ä¸ºTrue
# )

# å†™æ³•2: å®˜ç½‘æ¨èçš„å†™æ³•(https://huggingface.co/docs/accelerate/usage_guides/big_modeling)
model = load_checkpoint_and_dispatch(
    model,
    pretrained_name_or_path,
    device_map=device_map
)


# method 2: (ç›´æ¥ä¸€æ­¥åˆ°ä½), ã€å¾…ç¡®è®¤ã€‘
model = AutoModelForCausalLM.from_pretrained(
    pretrained_name_or_path,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    # low_cpu_mem_usage=True,  # è®¾ç½®äº†device_mapålow_cpu_mem_usageä¼šè¢«é»˜è®¤è®¾ç½®ä¸ºTrue
    max_memory=max_memory,
    device_map="sequential",  # ä¼¼ä¹æ˜¯å¿…é¡»è®¾ç½®ä¸ºè¿™ä¸ª, æ‰ä¼šè®©max_memoryç”Ÿæ•ˆ
    offload_folder="offload",
    # offload_state_dict=True
)


tokenizer = AutoTokenizer.from_pretrained(
    pretrained_name_or_path,
    trust_remote_code=True
)

inputs = tokenizer(["Alice and Bob"], return_tensors="pt")
output = model.generate(
    inputs=inputs["input_ids"],
    temperature=0.7,
    top_k=0,  # 0 è¡¨ç¤ºä¸ä½¿ç”¨ top_k å¯¹ logits è¿›è¡Œåå¤„ç†
    top_p=0.8,
    repetition_penalty=1.02,
    max_length=50,
    do_sample=True,
    return_dict_in_generate=True,
    attention_mask=inputs["attention_mask"],
)

texts = tokenizer.batch_decode(output["sequences"])
```


å¤‡æ³¨ï¼šã€ä»¥ä¸‹APIé—´çš„å…³ç³»æœ‰äº›æ··ä¹±ï¼Œå¾…ç ”ç©¶ã€‘

- `PretrainedModel.from_pretrained(...)`
- `AutoModelForXXX.from_config(...)`
- `YYYModelForXXX._from_config(...)`
- `accelerate.init_empty_weight(...)`
- `accelerate.infer_auto_device_map(...)`
- `accelerate.get_balanced_memory(...)`
- `accelerate.dispatch_model(...)`
- `accelerate.load_checkpoint_and_dispatch(...)`

è¿™äº›å‚æ•°ä¹‹é—´æ€ä¹ˆé…åˆçš„:

- `torch_dtype`, `model.config.torch_dtype`
- `device_map`: None, auto, balanced, balanced_low_0, sequential, dict(...)
- `offload_state_dict`: 
- `low_cpu_mem_usage`:
- `_fast_init`:


## ä¸»è¦APIåŠåŠŸèƒ½

```python
import torch.nn as nn
import torch

class FeedForward(nn.Module):
    def __init__(self, num_layer, in_dim, hidden_dim):
        super().__init__()
        assert num_layer > 2
        self.num_layer = num_layer
        layers = nn.Sequential()
        layers.append(nn.Linear(in_dim, hidden_dim))
        for i in range(num_layer - 2):
            layers.append(nn.Linear(hidden_dim, hidden_dim))
        layers.append(nn.Linear(hidden_dim, in_dim))
        self.layers = layers
        self.activate = nn.ReLU()
    def forward(self, x):
        x = self.layers(x)
        x = self.activate(x)
        return x
    
class Head(nn.Module):
    def __init__(self, in_dim, num_class):
        super().__init__()
        self.out = nn.Linear(in_dim, num_class)
        self.softmax = nn.Softmax(dim=-1)
    def forward(self, x):
        x = self.out(x)
        probs = self.softmax(x)
        return probs

class ExampleModel(nn.Module):
    def __init__(self, vocab_size, in_dim, num_layer, num_class):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, in_dim)
        self.feed_forward = FeedForward(num_layer, in_dim, 4*in_dim)
        self.head = Head(in_dim, num_class)

    def forward(self, x):
        # x: (B, L), out: (B, L, num_class)
        out = self.embed(x)
        out = self.feed_forward(out)
        out = self.head(out)
        return out
```

### accelerate.utils.modeling.infer_auto_device_map

**signature**

```python
device_map: Dict[str, torch.dtype] = infer_auto_device_map(
    model: nn.Module,
    max_memory: Optional[Dict]=None,
    no_split_module_classes: List[str]=None,
    dtype: Optional[Union[torch.dtype, str]]=None,
    special_dtypes: Optional[Dict] = None,
    verbose: bool = False
)
```

**å…¥å‚**
- `model`: åªè¦æ±‚æ˜¯ `nn.Module` å³å¯, å› æ­¤è¿™ä¸ªå‡½æ•°**ä¸ä¼š**å°† `model._no_split_module` è‡ªåŠ¨çº³å…¥ `no_split_module_classes` ä¸­
- `max_memory`: è®¾å®šæ¯ä¸ªè®¾å¤‡æœ€å¤§å¯ä½¿ç”¨çš„CPUå†…å­˜/GPUæ˜¾å­˜
    - Dict:
        ```python
        max_memory={0: "10GB", 1: "20GB", "cpu": "200MB"}
        ```
    - None:
- `no_split_module_classes`: ä¸å¯åˆ†å‰²çš„å­æ¨¡å—å, ä¾‹å¦‚å‡è®¾å®šä¹‰åœ¨ `transformers.models.t5.modeling_t5.py` ä¸­çš„ `T5Block` è¢«è®¤ä¸ºæ˜¯ä¸å¯åˆ†å‰²åˆ°ä¸åŒè®¾å¤‡ä¸Šçš„åŸºæœ¬å•å…ƒ, åˆ™ä¼ å…¥:
    ```python
    no_split_module_classes=["T5Block"]
    ```
- `dtype`: 
    - None: è¡¨ç¤ºç”¨ model æœ¬èº«çš„æ¯ä¸ªå‚æ•°çš„ dtype è¿›è¡Œè®¡ç®—å†…å­˜å¤§å°
    - torch.float32: å¯¹äº model çš„æ¯ä¸ª tensor, tensor çš„ dtype ä¸ä¼ å…¥çš„ `dtype` çš„æœ€å°å€¼è®¡ç®—å†…å­˜å¤§å°
- `special_dtypes`:
    ```python
    # å­—å…¸çš„ key éœ€è¦åœ¨ model.state_dict().keys() ä¸­, å³å®ƒéœ€è¦ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„å¼ é‡
    special_dtypes = {"feed_forward.layers.0.weight": torch.float16}
    ```
**å‡ºå‚**

## è¾…åŠ©APIåŠåŠŸèƒ½


### accelerate.utils.modeling.compute_module_sizes

**signature**
```python
module_sizes: Dict[str, int] = compute_module_sizes(
    model: nn.Module,
    dtype: Optional[Union[torch.dtype, str]]=None,
    special_dtypes: Optional[Dict] = None
)
```

**å…¥å‚**
å‚æ•°ç±»å‹ä¸ `infer_auto_device_map` ç›¸åŒ, æ³¨æ„: **æ—¢ç»Ÿè®¡äº† Parameter ä¹Ÿç»Ÿè®¡äº† Buffer**

**å‡ºå‚**
- `module_sizes`: å„å±‚çº§çš„ Module/Parameter/Buffer çš„å†…å­˜å ç”¨å¤§å°, è§ç¤ºä¾‹

**ä¾‹å­**

```python
from accelerate.utils.modeling import compute_module_sizes
model = ExampleModel(100, 16, 4, 3)
compute_module_sizes(
    model.half(),
    dtype=torch.float32,
    special_dtypes={'feed_forward.layers.0.weight': torch.float32},
)

# è¾“å‡ºç»“æœå¦‚ä¸‹
{
    '': 26246,                              # Module
    'embed': 3200,                          # Module
    'embed.weight': 3200,                   # Parameter
    'feed_forward': 22944,                  # Module
    'feed_forward.layers': 22944,           # Module
    'feed_forward.layers.0': 4224,          # Module
    'feed_forward.layers.0.weight': 4096,   # Parameter: 4byte*16*64=4096byte
    'feed_forward.layers.0.bias': 128,      # Parameter: min(2, 4)=2byte*64=128byte
    'feed_forward.layers.1': 8320,          # Module
    'feed_forward.layers.1.weight': 8192,   # Parameter: min(2, 4)=2byte*64*64=8192byte
    'feed_forward.layers.1.bias': 128,      # Parameter
    'feed_forward.layers.2': 8320,          # Module
    'feed_forward.layers.2.weight': 8192,   # Parameter
    'feed_forward.layers.2.bias': 128,      # Parameter
    'feed_forward.layers.3': 2080,          # Module
    'feed_forward.layers.3.weight': 2048,   # Parameter
    'feed_forward.layers.3.bias': 32,       # Parameter
    'head': 102,                            # Module
    'head.out': 102,                        # Module
    'head.out.weight': 96,                  # Parameter
    'head.out.bias': 6                      # Parameter
}
```

### accelerate.utils.modeling.find_tied_parameters

é¦–å…ˆåŒºåˆ†å‡ ç±»â€œç»‘å®šâ€

- å˜é‡ç»‘å®š: `self.layer2=self.layer1`, è¿™ç§åšæ³•æ˜¯å¯¹åŒä¸€ä»½å†…å­˜ç»‘å®šäº†ä¸¤ä¸ªå®ä¾‹å˜é‡å
    - `state_dict=self.state_dict()`: {"layer1.weight": xx, "layer2.weight": xx}, ä¸¤ä¸ªkeyå¯¹åº”çš„valueæ˜¯ç›¸åŒçš„(å¯¹åº”åŒä¸€ä»½å†…å­˜), åœ¨ä½¿ç”¨ `torch.save(sate_dict)` æ—¶å†…å­˜ä»…å ç”¨ä¸€ä»½
    - `self.modules()`: è¿­ä»£å™¨ä¼šå»é‡, **åªè¿­ä»£å‡ºä¸€ä¸ª**
    - `layer1.weight.grad` ç­‰å…³äºæ¢¯åº¦çš„æ“ä½œä¼šåŒæ—¶ä½œç”¨äºä¸¤è€…, å› æ­¤å¯ä»¥ä¸€ç›´ä¿æŒä¸€è‡´
    - `layer1.weight is layer2.weight`: True
    - `find_tied_parameters`: **æ— æ³•æ£€æµ‹**å‡ºè¿™ç§æƒ…å½¢, è¿”å›å€¼ä¸ºç©ºåˆ—è¡¨
- tied Parameter (**ğŸ¤— Transformer ä¸­çš„ tie_weight æ˜¯æŒ‡è¿™ç§æƒ…å½¢**): `self.layer2.weight=self.layer1.weight`
    - `state_dict=self.state_dict()`: {"layer1.weight": xx, "layer2.weight": xx}, ä¸¤ä¸ªkeyå¯¹åº”çš„valueæ˜¯ç›¸åŒçš„(å¯¹åº”åŒä¸€ä»½å†…å­˜), åœ¨ä½¿ç”¨ `torch.save(sate_dict)` æ—¶å†…å­˜ä»…å ç”¨ä¸€ä»½
    - `self.modules()`: è¿­ä»£å™¨ä¼šå»é‡, ä½†layer1ä¸layer2æ˜¯ä¸åŒçš„, **è¿­ä»£å‡ºä¸¤ä¸ª**
    - `layer1.weight.grad` ç­‰å…³äºæ¢¯åº¦çš„æ“ä½œä¼šåŒæ—¶ä½œç”¨äºä¸¤è€…, å› æ­¤å¯ä»¥ä¸€ç›´ä¿æŒä¸€è‡´
    - `layer1.weight is layer2.weight`: True
    - `find_tied_parameters`: **å¯ä»¥æ£€æµ‹**å‡ºè¿™ç§æƒ…å½¢, è¿”å›å€¼ä¸º `[["layer1.weight", "layer2.weight"]]`
- copy Parameter data ã€è¿™ä¸¤ç§ä¼¼ä¹æ˜¯å®Œå…¨çš„è¡¨ç°,æœ‰äº›å¥‡æ€ªã€‘:
    - `layer2.weight.data=layer1.weight.data.clone()`
    - `layer2.weight.data=layer1.weight.data`

    ä¾‹å­ã€å¾…ç ”ç©¶æ¸…æ¥šã€‘
    ```python
    layer1 = nn.Linear(3, 3)
    layer2 = nn.Linear(3, 3)
    with torch.no_grad():
        layer2.weight.data = layer1.weight.data#.clone()
        layer2.bias.data = layer1.bias.data#.clone()
    import itertools
    opt=torch.optim.SGD(itertools.chain(layer1.parameters(), layer2.parameters()), lr=0.001)
    
    # clone: False, ä¸ä½¿ç”¨ cloneï¼šFalse
    layer2.weight.data is layer1.weight.data
    # å¤šæ¬¡è¿è¡Œ, è¿™ä¸¤ä¸ªå€¼çš„idä¸€ç›´åœ¨å˜, ä½†æ— è®ºä½¿ç”¨cloneè¿˜æ˜¯ä¸ä½¿ç”¨, ä¸¤è€…çš„å€¼ç›¸åŒ
    id(layer2.weight.data), id(layer1.weight.data)
    
    x = torch.rand(4, 3)
    z = layer2(layer1(x)).sum()
    y = torch.rand(())
    loss = z - y
    loss.backward()

    # clone: ä¸åŒ, ä¸ä½¿ç”¨ cloneï¼šç›¸åŒ
    layer1.weight.grad, layer2.weight.grad

    opt.step()

    # clone: ä¸åŒ, ä¸ä½¿ç”¨ clone: ç›¸åŒ
    layer1.weight, layer2.weight
    ```

**signature**
```python
tied_names: List[List[str]] = find_tied_parameters(model: nn.Module)
```

**å…¥å‚**

**å‡ºå‚**

- `tied_names`:
    ```python
    tied_names=[
        ["layer1.weight", "layer2.weight", "layer3.weight"],  # è¿™ä¸‰ä¸ªweightè¢«tiedäº†
        ["layer1.bias", "layer2.bias", "layer3.bias"] # è¿™ä¸‰ä¸ªbiasè¢«tiedäº†
    ]
    ```

**å†…éƒ¨é€»è¾‘**

æœ¬è´¨ä¸Š, æ˜¯ä½¿ç”¨ `nn.Module.named_parameters()`, ç„¶åä¸¤ä¸¤ä½¿ç”¨ `parameter1 is parameter 2` åˆ¤æ–­æ˜¯å¦ä¸º tied_weight. å› æ­¤å®Œå…¨ç›¸åŒçš„ submodule ç”±äº `nn.Module.named_parameters()` ä¼šè¢«é¢„å…ˆæ’é™¤æ‰, å› æ­¤ä¸ä¼šå‡ºç°åœ¨ç»“æœé‡Œã€‚

**ä¾‹å­**

```python
import torch.nn as nn
import torch
from accelerate.utils.modeling import find_tied_parameters
class ModelA(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(3, 3)
        self.layer2 = self.layer1
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        return out
    
class ModelB(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(3, 3)
        self.layer2 = nn.Linear(3, 3)
        self.layer2.weight = self.layer1.weight
        self.layer2.bias = self.layer1.bias
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        return out

model_a = ModelA()
model_b = ModelB()
# è¿™é‡Œè°ƒç”¨çš„æ˜¯ nn.Module çš„æ–¹æ³•, å®ƒä¼šå»é™¤é‡å¤çš„module, å› æ­¤åªä¼šæ‰“å°å‡º ModelA å’Œ ModelA.layer1
for module in model_a.modules():
    print(module)
find_tied_parameters(model_a)  # è¿”å›ä¸ºç©ºåˆ—è¡¨!!!

# è¿™é‡Œè°ƒç”¨çš„æ˜¯ nn.Module çš„æ–¹æ³•, å› ä¸ºlayer1å’Œlayer2æ˜¯åˆ†åˆ«å®šä¹‰çš„, åªæ˜¯å‚æ•°è¢«ç»‘å®š
for module in model_b.modules():
    print(module)
find_tied_parameters(model_a)  # è¿”å›ä¸º[['layer1.weight', 'layer2.weight'], ['layer1.bias', 'layer2.bias']]
```


### get_max_layer_size


**å†…éƒ¨é€»è¾‘**

é€’å½’éå†æ•´ä¸ªæ¨¡å‹, ç›´è‡³æœ€å°ä¸å¯åˆ†å‰²çš„ Module æˆ–è€…å•ç‹¬çš„ Parameter/Buffer, è®¡ç®—è¿™äº›ä¸å¯åˆ†å‰²çš„éƒ¨åˆ†æ‰€éœ€è¦çš„å†…å­˜çš„æœ€å¤§å€¼, åˆ¤æ–­ä¸å¯åˆ†å‰²çš„ Module çš„ä¾æ®ä¸º:

```python
module.__class__.__name__ in no_split_module_classes
# å‡è®¾ module æ˜¯ä¸€ä¸ª torch.nn.modules.linear.Linear å±‚
# module.__class__.__name__: "Linear"
# no_split_module_classes: ["Linear", "T5Block"]
```