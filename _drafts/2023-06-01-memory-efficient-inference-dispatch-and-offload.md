---
layout: post
title: "(WIP) Memory efficient inference (ğŸ¤—'s dispatch and offload)"
date: 2023-06-01 17:20:04 +0800
labels: [transformers]
---

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- ğŸ¤— transformer/accelerate çš„ `init_empty_weight` å’Œ `dispatch_model` ç­‰ API

å‚è€ƒèµ„æ–™

- [https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling)
- [https://huggingface.co/blog/accelerate-large-models](https://huggingface.co/blog/accelerate-large-models)

æ¶‰åŠå†…å®¹

- ä½¿ç”¨æ–¹æ³•ã€æ ·ä¾‹ã€‘
- æ–‡æ¡£è¡¥å……ï¼šæŸäº›è¾…åŠ©å‡½æ•°å®˜æ–¹æ–‡æ¡£ç¼ºå¤±ï¼ŒæŸäº›å¯¹å¤–APIçš„å‚æ•°è¯´æ˜ä¸æ˜ç¡®
- æºç åˆ†æ

## æ ·ä¾‹ä»£ç 

```
# requirements.txt
torch >= 1.10.0
transformers==4.29.2
accelerate==0.19.0
```

å†™æ³•ä¸€: **æ³¨æ„: æœ¬ç¯‡åšå®¢åç»­å‡ ä¹å…¨éƒ¨çš„å†…å®¹éƒ½æ˜¯ä¸ºäº†è§£é‡Šè¿™ä¸€è¡Œåˆå§‹åŒ–æ¨¡å‹çš„ä»£ç **

```python
pretrained_name_or_path = "./fnlp/moss-moon-003-sft"
model = AutoModelForCausalLM.from_pretrained(
    pretrained_name_or_path,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    # low_cpu_mem_usage=True,  # è®¾ç½®äº†device_mapålow_cpu_mem_usageä¼šè¢«é»˜è®¤è®¾ç½®ä¸ºTrue
    max_memory=max_memory,
    device_map="sequential",  # ä¼¼ä¹æ˜¯å¿…é¡»è®¾ç½®ä¸ºè¿™ä¸ª, æ‰ä¼šè®©max_memoryç”Ÿæ•ˆï¼ˆè§¦å‘infer_auto_device_mapçš„è°ƒç”¨ï¼‰
    offload_folder="offload",
    offload_state_dict=True  # cpuä¸Šçš„å‚æ•°æš‚æ—¶offload, å†é‡æ–°åŠ è½½å›cpu, ä»…ç”¨äºå‡å°‘åŠ è½½æ¨¡å‹æ—¶çš„å†…å­˜å ç”¨, ä½†åŠ è½½é€Ÿåº¦ä¼šå˜æ…¢ä¸€äº›
)
```

å†™æ³•äºŒ:


```python
import torch
from transformers import AutoTokenizer, AutoModelForCausualLM, AutoConfig
from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch

pretrained_name_or_path = "./fnlp/moss-moon-003-sft"
config = AutoConfig.from_pretrained(path, trust_remote_code=True)
with init_empty_weights():
    model = AutoModelForCausalLM.from_config(
        config,
        # åœ¨æ‰§è¡Œ cls(...) ä¹‹å‰, ä¼šåˆ©ç”¨ torch.set_default_dtype(torch_dtype)å…¨å±€è®¾å®šé»˜è®¤æµ®ç‚¹æ•°ç±»å‹
        torch_dtype=torch.float16,
        trust_remote_code=True)
# !!!è¿™ä¸€æ­¥æ˜¯å¿…é¡»çš„!!!
model.tie_weights()

max_memory = {
    0: "10GB",
    1: "10GB",
    "cpu": "20GB",
}

device_map = infer_auto_device_map(
    model,
    max_memory=max_memory,
    dtype=None,  # ä½¿ç”¨modelä¸­çš„tensorç±»å‹ä¼°ç®—æ‰€éœ€çš„å†…å­˜/æ˜¾å­˜
    no_split_module_classes=model._no_split_modules,  # æŸäº›submoduleå¿…é¡»æ”¾åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Š
)

# å‰é¢çš„ä¸¤ä¸ªæ­¥éª¤æ˜¯ä¸ºäº†è·å–åˆé€‚çš„ device_map
# https://huggingface.co/docs/accelerate/usage_guides/big_modeling
model = load_checkpoint_and_dispatch(
    model,
    pretrained_name_or_path,
    device_map=device_map
)
```

ç”¨æ³•:

```python
tokenizer = AutoTokenizer.from_pretrained(
    pretrained_name_or_path,
    trust_remote_code=True
)

inputs = tokenizer(["Alice and Bob"], return_tensors="pt")
output = model.generate(
    inputs=inputs["input_ids"],
    temperature=0.7,
    top_k=0,  # 0 è¡¨ç¤ºä¸ä½¿ç”¨ top_k å¯¹ logits è¿›è¡Œåå¤„ç†
    top_p=0.8,
    repetition_penalty=1.02,
    max_length=50,
    do_sample=True,
    return_dict_in_generate=True,
    attention_mask=inputs["attention_mask"],
)

texts = tokenizer.batch_decode(output["sequences"])
```


å¤‡æ³¨ï¼šã€ä»¥ä¸‹APIé—´çš„å…³ç³»æœ‰äº›æ··ä¹±ï¼Œå¾…ç ”ç©¶ã€‘

- `PretrainedModel.from_pretrained(...)`
- `AutoModelForXXX.from_config(...)`
- `YYYModelForXXX._from_config(...)`
- `accelerate.init_empty_weight(...)`: ä¸Šä¸‹æ–‡ç®¡ç†å™¨, åœ¨ä¸Šä¸‹æ–‡èŒƒå›´å†…, ä¿®æ”¹ `nn.Module` çš„ `register_parameter` ä¸ `register_buffer` å‡½æ•°, ä½¿å¾— `nn.Module` ä¸­çš„ tensor æ€»æ˜¯åœ¨ `torch.device("meta")` ä¸Š
- `accelerate.get_balanced_memory(...)`: åœ¨è®¾å®š `device_map` ä¸º `auto/balanced/balanced_low_0` æ—¶, å¾—åˆ° `max_memory` å­—å…¸
- `accelerate.infer_auto_device_map(...)`: æ ¹æ® `max_memory` å­—å…¸å¾—åˆ° `device_map` å­—å…¸, ç¡®å®šæ¯ä¸ª module æˆ– parameter/buffer çš„è®¾å¤‡
- `accelerate.dispatch_model(...)`: æ ¹æ® `device_map` å­—å…¸å¢åŠ  `forward` çš„ hook
- `accelerate.load_checkpoint_and_dispatch(...)`: å¹¶æœªåœ¨ ğŸ¤— transformer è¢«ä½¿ç”¨ (ä¸ from_pretrain æœ‰äº›é‡å¤ä»£ç , çŒœæƒ³åº”è¯¥æ˜¯ä»£ç è¿˜éœ€è¦é‡æ„å¥½)

è¿™äº›å‚æ•°ä¹‹é—´æ€ä¹ˆé…åˆçš„:

- `torch_dtype`, `model.config.torch_dtype`
- `device_map`: None, auto, balanced, balanced_low_0, sequential, dict(...)
- `offload_state_dict`: å¦‚æœä¸º False, é‚£ä¹ˆ device_map ä¸º cpu çš„ tensor ä¼šæ”¾åœ¨cpuä¸Š, å› æ­¤åœ¨åŠ è½½è¿‡ç¨‹ä¸­ï¼Œcpuçš„å†…å­˜è¦åŒ…å«ä¸€ä»½shardçš„state_dictä¸åº”è¯¥æ”¾åœ¨cpuä¸Šçš„modelé‡Œçš„tensorã€‚å¦‚æœè®¾ç½® `offload_state_dict` ä¸º Trueï¼Œé‚£ä¹ˆå¯¹äº device_map ä¸º cpu çš„ tensor ä¼šå…ˆä¸´æ—¶ offload åˆ° disk ä¸Šï¼Œè¿™æ ·å­ä¿è¯ cpu çš„å†…å­˜åªè¦èƒ½è£…ä¸‹æœ€å¤§çš„ shard å³å¯ï¼Œåœ¨æ‰€æœ‰ shard å¤„ç†å®Œæˆåï¼Œåœ¨å°† offload çš„ tensor å…¨éƒ¨ç§»å›åˆ° cpu ä¸Šï¼ˆoffloadç›®å½•ä¹Ÿå°†è¢«åˆ é™¤ï¼‰ã€‚ç”±æ­¤ cpu çš„å†…å­˜åªè¦è¶…è¿‡æœ€å¤§shardã€ä»¥åŠèƒ½è£…ä¸‹æ”¾åœ¨ cpu ä¸Šçš„ model çš„ tensor å³å¯ï¼Œè€Œä¸æ˜¯éœ€è¦èƒ½è£…ä¸‹è¿™ä¸¤è€…ä¹‹å’Œã€‚
- `low_cpu_mem_usage`:
- `_fast_init`:


## ä¸»è¦APIåŠåŠŸèƒ½

### `PretrainedModel.from_pretrained`

#### low_cpu_mem_usage å‚æ•°

æŒ‰ç…§ [ğŸ¤— blog](https://huggingface.co/blog/accelerate-large-models) çš„æè¿°, low_cpu_mem_usage ä¸º False ä»¥åŠ True åˆ†åˆ«å¯¹åº”å¦‚ä¸‹ä¸¤ç§æµç¨‹

æµç¨‹ä¸€ï¼š
> 1. Create the model
> 2. Load in memory its weights (in an object usually called state_dict)
> 3. Load those weights in the created model
> 4. Move the model on the device for inference

æµç¨‹äºŒï¼š
> 1. Create an empty (e.g. without weights) model
> 2. Decide where each layer is going to go (when multiple devices are available)
> 3. Load in memory parts of its weights
> 4. Load those weights in the empty model
> 5. Move the weights on the device for inference
> 6. Repeat from step 3 for the next weights until all the weights are loaded

ç»è¿‡å¯¹æºç çš„ä»”ç»†åˆ†æ, å°†ä¸Šè¿°è¿‡ç¨‹ç»†åŒ–å¦‚ä¸‹:

low_cpu_mem_usage çš„é»˜è®¤å€¼ä¸º False, ä½†å½“å…¶ä»–ä¸€äº›å‚æ•°è¢«è®¾å®šçš„æƒ…å†µä¸‹, low_cpu_mem_usage ä¼šè¢«è‡ªåŠ¨è®¾ç½®ä¸º True

- device_map ä¸ä¸º None æ—¶, low_cpu_mem_usage ä¼šè¢«è‡ªåŠ¨è®¾ç½®ä¸º True
- ...


**low_cpu_mem_usage=False**

å½“æ¨¡å‹çš„å‚æ•°æ–‡ä»¶åªæœ‰ä¸€ä¸ªæ—¶, ä¸æµç¨‹ä¸€çš„æè¿°ä¸€è‡´ï¼Œå› æ­¤åœ¨æµç¨‹ä¸€çš„ç¬¬ 2 æ­¥ç»“æŸæ—¶ï¼Œcpu ä¸­ä¼šæœ‰ç€ä¸¤ä»½æƒé‡

- åˆå§‹åŒ–ä¸€ä¸ªéšæœºå‚æ•°çš„æ¨¡å‹ï¼ˆåœ¨CPUä¸Šï¼‰
- load `pytorch_model.bin`
- å°†æƒé‡æ”¾å…¥æ¨¡å‹ä¸­ï¼ˆCPUä¸Šï¼‰

å½“æ¨¡å‹çš„å‚æ•°æ–‡ä»¶æœ‰å¤šä¸ªæ—¶ï¼ˆå³æ‰€è°“çš„ "shard checkpoint"ï¼‰

- åˆå§‹åŒ–ä¸€ä¸ªéšæœºå‚æ•°çš„æ¨¡å‹ï¼ˆåœ¨CPUä¸Šï¼‰
- é€ä¸ª `pytorch_model-0000x-of-0000y.bin` æ–‡ä»¶, å¹¶ç”¨äºåŠ è½½è‡³æ¨¡å‹ï¼ˆåœ¨CPUä¸Šï¼‰ä¸­, æ¯ä¸ªæ–‡ä»¶ load è¿›æ¨¡å‹å load çš„å‚æ•°å­—å…¸

å› æ­¤å®é™…ä¸Šåªéœ€è¦ â€œæ¨¡å‹æ–‡ä»¶å¤§å°+1ä¸ªshardå‚æ•°â€ çš„ CPU å†…å­˜ã€‚

**low_cpu_mem_usage=True**

å½“æ¨¡å‹çš„å‚æ•°æ–‡ä»¶åªæœ‰ä¸€ä¸ªæ—¶:

- åˆå§‹åŒ–ä¸€ä¸ªç©ºæ¨¡å‹(device="meta")
- å¦‚æœæ¨¡å‹çš„å‚æ•°æ–‡ä»¶åªæœ‰ä¸€ä¸ª, åˆ™é€šè¿‡ load `pytorch_model.bin` ï¼ˆload åœ¨ cpuä¸Šï¼‰æ¥è·å–ä¿å­˜çš„æ¨¡å‹å‚æ•°çš„ key åˆ—è¡¨, éšåå°† load çš„å‚æ•°å­—å…¸å…ˆé‡Šæ”¾
- æ ¹æ® device_map, max_memory, torch_dtype ç¡®å®šæ¯ä¸ªå‚æ•°æœ€ç»ˆéœ€è¦å­˜æ”¾çš„è®¾å¤‡
- é‡æ–° load `pytorch_model.bin`ï¼ˆload åœ¨ cpu ä¸Šï¼‰
- å°†æƒé‡æ”¾å…¥ç›¸åº”çš„è®¾å¤‡ä¸­ã€‚æ³¨æ„ï¼Œè¿™ä¸€æ­¥éª¤å¯èƒ½ä¼šæœ‰å¦‚ä¸‹å‡ ç§æƒ…å½¢å¯¼è‡´éœ€è¦æ›´å¤šçš„å†…å­˜èµ„æº
    - å¦‚æœæŸä¸ªæƒé‡éœ€è¦æ”¾ç½®åœ¨ GPU ä¸Šï¼Œé‚£ä¹ˆè¿™ä»½æƒé‡åœ¨ CPU å’Œ GPU ä¸Šéƒ½æœ‰ä¸€ä»½
    - å¦‚æœæŸä¸ªæƒé‡éœ€è¦çš„ dtype ä¸ load å‡ºæ¥çš„æƒé‡ä¸ç›¸åŒï¼Œé‚£ä¹ˆè¿™ä¸ªæƒé‡éœ€è¦å­˜å‚¨ä¸¤ä»½
    - å¦‚æœæŸä¸ªæƒé‡éœ€è¦çš„ dtype ä¸ load å‡ºæ¥çš„æƒé‡ç›¸åŒï¼Œé‚£ä¹ˆè¿™ä¸ªæƒé‡åªä¼šå ç”¨ä¸€ä»½ç©ºé—´
    
    æ³¨æ„åœ¨å°†æ¨¡å‹çš„æƒé‡åˆå§‹åŒ–å¥½ä¹‹å‰ï¼Œload çš„æƒé‡éƒ½ä¸ä¼šè¢«é‡Šæ”¾
- å°† load çš„æƒé‡ä¸€æ¬¡æ€§é‡Šæ”¾


å½“æ¨¡å‹çš„å‚æ•°æ–‡ä»¶æœ‰å¤šä¸ªæ—¶

- åˆå§‹åŒ–ä¸€ä¸ªç©ºæ¨¡å‹(device="meta")
- å¦‚æœæ¨¡å‹çš„å‚æ•°æ–‡ä»¶æœ‰å¤šä¸ª, åˆ™é€šè¿‡ load `pytorch_model.bin.index.json` æ–‡ä»¶è·å–ä¿å­˜çš„æ¨¡å‹å‚æ•°çš„ key åˆ—è¡¨
- æ ¹æ® device_map, max_memory, torch_dtype ç¡®å®šæ¯ä¸ªå‚æ•°æœ€ç»ˆéœ€è¦å­˜æ”¾çš„è®¾å¤‡
- æ¯æ¬¡ load ä¸€ä¸ª `pytorch_model-0000x-of-0000y.bin` æ–‡ä»¶, åˆå§‹åŒ–æ¨¡å‹çš„ä¸€éƒ¨åˆ†å‚æ•°, ç„¶åé‡Šæ”¾æ‰è¿™ä¸€ä¸ª `pytorch_model-0000x-of-0000y.bin` çš„å‚æ•°æ–‡ä»¶


ä»¥ä¸‹æ˜¯ç®€åŒ–åçš„ä¼ªä»£ç : `low_cpu_mem_usage=True`ï¼Œä¸”ä¸ºå•ä¸ªæƒé‡æ–‡ä»¶çš„æƒ…å½¢

```python
assert model.device == torch.device("meta")
state_dict = torch.load("pytorch_model.bin")
device_map = {
    "layer1.weight": "cuda:0",
    "layer1.bias": "cuda:0",
    "layer2.weight": "cpu",
    "layer2.bias": "cpu",
}
for name, param in state_dict.items():
    # submodule = model.layer1 æˆ– model.layer2
    submodule = get_submodule(model, name)
    tensor_name = name.split(".")[-1]
    with torch.no_grad():
        # æ³¨æ„å¦‚æœnew_valueçš„deviceä¸dtypeä¸paramç›¸åŒæ—¶, ä¸éœ€è¦é¢å¤–å ç”¨å†…å­˜ç©ºé—´
        new_value = torch.tensor(param.to(device=device_map[name], dtype=...))
        if is_buffer:
            submodule._buffers[tensor_name] = new_value
        else:
            submodule._parameters[tensor_name] = new_value
del state_dict
gc.collect()
```

#### _fast_init

è¿™ä¸ªå‚æ•°ä¼¼ä¹ä½œç”¨å¹¶ä¸ç®—å¤ªå¤§

#### device_map, max_memory, torch_dtype

è¿™ä¸¤ä¸ªå‚æ•°ä¸€èµ·å†³å®šæ¯ä¸ªå‚æ•°çš„ device
- `device_map: str/Dict`
- `max_memory: None/Dict`

é€»è¾‘å¦‚ä¸‹: 
- å¦‚æœ `device_map` æœ¬èº«æ˜¯ Dict æ—¶, åˆ™ç›´æ¥ç¡®å®šæ¯ä¸ªå‚æ•°çš„ device
- å¦‚æœ `device_map` å–å€¼ä¸ºå­—ç¬¦ä¸², è€Œ `max_memory=None`, é‚£ä¹ˆå°±ã€å¾…ç¡®è®¤ã€‘å‡å®šæ‰€æœ‰çš„ GPU ä¸ CPU çš„å†…å­˜éƒ½å¯ä»¥ä½¿ç”¨, ç”±æ­¤å¾—åˆ°å­—å…¸å½¢å¼çš„ `max_memory`, ç„¶åå†æ ¹æ® `device_map` çš„å…·ä½“å–å€¼æ¥ç¡®å®šæ¯ä¸ªå‚æ•°çš„ device:
    - `auto/balanced`
    - `balanced_low_0`
    - `sequential`
- å¦‚æœ `max_memory` ä¸º Dict, `device_map` å¿…é¡»å–å€¼ä¸º `"sequential"`

è¿™ä¸ªå‚æ•°ä»¥åŠconfigå†³å®šæ¯ä¸ªå‚æ•°dtype
- `torch_dtype: None/torch.dtype`



### accelerate.utils.modeling.infer_auto_device_map

**signature**

```python
device_map: Dict[str, torch.dtype] = infer_auto_device_map(
    model: nn.Module,
    max_memory: Optional[Dict]=None,
    no_split_module_classes: List[str]=None,
    dtype: Optional[Union[torch.dtype, str]]=None,
    special_dtypes: Optional[Dict] = None,
    verbose: bool = False
)
```

**å…¥å‚**
- `model`: åªè¦æ±‚æ˜¯ `nn.Module` å³å¯, å› æ­¤è¿™ä¸ªå‡½æ•°**ä¸ä¼š**å°† `model._no_split_module` è‡ªåŠ¨çº³å…¥ `no_split_module_classes` ä¸­
- `max_memory`: è®¾å®šæ¯ä¸ªè®¾å¤‡æœ€å¤§å¯ä½¿ç”¨çš„CPUå†…å­˜/GPUæ˜¾å­˜
    - Dict:
        ```python
        max_memory={0: "10GB", 1: "20GB", "cpu": "200MB"}
        ```
    - None:
- `no_split_module_classes`: ä¸å¯åˆ†å‰²çš„å­æ¨¡å—å, ä¾‹å¦‚å‡è®¾å®šä¹‰åœ¨ `transformers.models.t5.modeling_t5.py` ä¸­çš„ `T5Block` è¢«è®¤ä¸ºæ˜¯ä¸å¯åˆ†å‰²åˆ°ä¸åŒè®¾å¤‡ä¸Šçš„åŸºæœ¬å•å…ƒ, åˆ™ä¼ å…¥:
    ```python
    no_split_module_classes=["T5Block"]
    ```
    å…·ä½“é€»è¾‘å¯å‚è€ƒ `get_max_layer_size` ä¸­çš„è§£é‡Š
- `dtype`: 
    - None: è¡¨ç¤ºç”¨ model æœ¬èº«çš„æ¯ä¸ªå‚æ•°çš„ dtype è¿›è¡Œè®¡ç®—å†…å­˜å¤§å°
    - torch.float32: å¯¹äº model çš„æ¯ä¸ª tensor, tensor çš„ dtype ä¸ä¼ å…¥çš„ `dtype` çš„æœ€å°å€¼è®¡ç®—å†…å­˜å¤§å°
- `special_dtypes`:
    ```python
    # å­—å…¸çš„ key éœ€è¦åœ¨ model.state_dict().keys() ä¸­, å³å®ƒéœ€è¦ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„å¼ é‡
    special_dtypes = {"feed_forward.layers.0.weight": torch.float16}
    ```
**å‡ºå‚**

- `device_map` æ˜¯ä¸€ä¸ªå­—å…¸, key æ˜¯å­æ¨¡å—åæˆ–è€…æ˜¯æƒé‡å(å½“ key æ˜¯å­æ¨¡å—åæ—¶, è¡¨æ˜æ•´ä¸ªå­æ¨¡å—éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š), value æ˜¯è®¾å¤‡å(0/1/cpu/disk), ä¾‹å­:
    ```python
    device_map = {
        'embed': 0                              # Module, åŒ…å«æƒé‡ embed.weight
        'feed_forward.layers.0': 0,             # Module, åŒ…å«æƒé‡ feed_forward.layers.0.weight ä¸ feed_forward.layers.0.bias
        'feed_forward.layers.1.weight': 0,      # Parameter
        'feed_forward.layers.1.bias': "cpu",    # Parameter
        'feed_forward.layers.2': "cpu",         # Module
        'feed_forward.layers.3': "disk",        # Module
        'head': "disk"                          # Module, åŒ…å« head.out Module
    }
    ```

### accelerate.big_modeling.dispatch_model

ä¾èµ–çš„è¾…åŠ© API å¦‚ä¸‹:
- accelerate.utils.modeling.check_device_map

## è¾…åŠ©APIåŠåŠŸèƒ½


### accelerate.utils.modeling.compute_module_sizes

**signature**
```python
module_sizes: Dict[str, int] = compute_module_sizes(
    model: nn.Module,
    dtype: Optional[Union[torch.dtype, str]]=None,
    special_dtypes: Optional[Dict] = None
)
```

**å…¥å‚**
å‚æ•°ç±»å‹ä¸ `infer_auto_device_map` ç›¸åŒ, æ³¨æ„: **æ—¢ç»Ÿè®¡äº† Parameter ä¹Ÿç»Ÿè®¡äº† Buffer**

**å‡ºå‚**
- `module_sizes`: å„å±‚çº§çš„ Module/Parameter/Buffer çš„å†…å­˜å ç”¨å¤§å°, è§ç¤ºä¾‹

**ä¾‹å­**

```python
import torch.nn as nn
import torch

class FeedForward(nn.Module):
    def __init__(self, num_layer, in_dim, hidden_dim):
        super().__init__()
        assert num_layer > 2
        self.num_layer = num_layer
        layers = nn.Sequential()
        layers.append(nn.Linear(in_dim, hidden_dim))
        for i in range(num_layer - 2):
            layers.append(nn.Linear(hidden_dim, hidden_dim))
        layers.append(nn.Linear(hidden_dim, in_dim))
        self.layers = layers
        self.activate = nn.ReLU()
    def forward(self, x):
        x = self.layers(x)
        x = self.activate(x)
        return x
    
class Head(nn.Module):
    def __init__(self, in_dim, num_class):
        super().__init__()
        self.out = nn.Linear(in_dim, num_class)
        self.softmax = nn.Softmax(dim=-1)
    def forward(self, x):
        x = self.out(x)
        probs = self.softmax(x)
        return probs

class ExampleModel(nn.Module):
    def __init__(self, vocab_size, in_dim, num_layer, num_class):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, in_dim)
        self.feed_forward = FeedForward(num_layer, in_dim, 4*in_dim)
        self.head = Head(in_dim, num_class)

    def forward(self, x):
        # x: (B, L), out: (B, L, num_class)
        out = self.embed(x)
        out = self.feed_forward(out)
        out = self.head(out)
        return out
```

```python
from accelerate.utils.modeling import compute_module_sizes
model = ExampleModel(100, 16, 4, 3)
compute_module_sizes(
    model.half(),
    dtype=torch.float32,
    special_dtypes={'feed_forward.layers.0.weight': torch.float32},
)

# è¾“å‡ºç»“æœå¦‚ä¸‹
{
    '': 26246,                              # Module
    'embed': 3200,                          # Module
    'embed.weight': 3200,                   # Parameter
    'feed_forward': 22944,                  # Module
    'feed_forward.layers': 22944,           # Module
    'feed_forward.layers.0': 4224,          # Module
    'feed_forward.layers.0.weight': 4096,   # Parameter: 4byte*16*64=4096byte
    'feed_forward.layers.0.bias': 128,      # Parameter: min(2, 4)=2byte*64=128byte
    'feed_forward.layers.1': 8320,          # Module
    'feed_forward.layers.1.weight': 8192,   # Parameter: min(2, 4)=2byte*64*64=8192byte
    'feed_forward.layers.1.bias': 128,      # Parameter
    'feed_forward.layers.2': 8320,          # Module
    'feed_forward.layers.2.weight': 8192,   # Parameter
    'feed_forward.layers.2.bias': 128,      # Parameter
    'feed_forward.layers.3': 2080,          # Module
    'feed_forward.layers.3.weight': 2048,   # Parameter
    'feed_forward.layers.3.bias': 32,       # Parameter
    'head': 102,                            # Module
    'head.out': 102,                        # Module
    'head.out.weight': 96,                  # Parameter
    'head.out.bias': 6                      # Parameter
}
```

### accelerate.utils.modeling.find_tied_parameters

é¦–å…ˆåŒºåˆ†å‡ ç±»â€œç»‘å®šâ€

- å˜é‡ç»‘å®š: `self.layer2=self.layer1`, è¿™ç§åšæ³•æ˜¯å¯¹åŒä¸€ä»½å†…å­˜ç»‘å®šäº†ä¸¤ä¸ªå®ä¾‹å˜é‡å
    - `state_dict=self.state_dict()`: {"layer1.weight": xx, "layer2.weight": xx}, ä¸¤ä¸ªkeyå¯¹åº”çš„valueæ˜¯ç›¸åŒçš„(å¯¹åº”åŒä¸€ä»½å†…å­˜), åœ¨ä½¿ç”¨ `torch.save(sate_dict)` æ—¶å†…å­˜ä»…å ç”¨ä¸€ä»½
    - `self.modules()`: è¿­ä»£å™¨ä¼šå»é‡, **åªè¿­ä»£å‡ºä¸€ä¸ª**
    - `layer1.weight.grad` ç­‰å…³äºæ¢¯åº¦çš„æ“ä½œä¼šåŒæ—¶ä½œç”¨äºä¸¤è€…, å› æ­¤å¯ä»¥ä¸€ç›´ä¿æŒä¸€è‡´
    - `layer1.weight is layer2.weight`: True
    - `find_tied_parameters`: **æ— æ³•æ£€æµ‹**å‡ºè¿™ç§æƒ…å½¢, è¿”å›å€¼ä¸ºç©ºåˆ—è¡¨
- tied Parameter (**ğŸ¤— Transformer ä¸­çš„ tie_weight æ˜¯æŒ‡è¿™ç§æƒ…å½¢**): `self.layer2.weight=self.layer1.weight`
    - `state_dict=self.state_dict()`: {"layer1.weight": xx, "layer2.weight": xx}, ä¸¤ä¸ªkeyå¯¹åº”çš„valueæ˜¯ç›¸åŒçš„(å¯¹åº”åŒä¸€ä»½å†…å­˜), åœ¨ä½¿ç”¨ `torch.save(sate_dict)` æ—¶å†…å­˜ä»…å ç”¨ä¸€ä»½
    - `self.modules()`: è¿­ä»£å™¨ä¼šå»é‡, ä½†layer1ä¸layer2æ˜¯ä¸åŒçš„, **è¿­ä»£å‡ºä¸¤ä¸ª**
    - `layer1.weight.grad` ç­‰å…³äºæ¢¯åº¦çš„æ“ä½œä¼šåŒæ—¶ä½œç”¨äºä¸¤è€…, å› æ­¤å¯ä»¥ä¸€ç›´ä¿æŒä¸€è‡´
    - `layer1.weight is layer2.weight`: True
    - `find_tied_parameters`: **å¯ä»¥æ£€æµ‹**å‡ºè¿™ç§æƒ…å½¢, è¿”å›å€¼ä¸º `[["layer1.weight", "layer2.weight"]]`
- copy Parameter data ã€è¿™ä¸¤ç§ä¼¼ä¹æ˜¯å®Œå…¨çš„è¡¨ç°,æœ‰äº›å¥‡æ€ªã€‘:
    - `layer2.weight.data=layer1.weight.data.clone()`
    - `layer2.weight.data=layer1.weight.data`

    ä¾‹å­ã€å¾…ç ”ç©¶æ¸…æ¥šã€‘
    ```python
    layer1 = nn.Linear(3, 3)
    layer2 = nn.Linear(3, 3)
    with torch.no_grad():
        layer2.weight.data = layer1.weight.data#.clone()
        layer2.bias.data = layer1.bias.data#.clone()
    import itertools
    opt=torch.optim.SGD(itertools.chain(layer1.parameters(), layer2.parameters()), lr=0.001)
    
    # clone: False, ä¸ä½¿ç”¨ cloneï¼šFalse
    layer2.weight.data is layer1.weight.data
    # å¤šæ¬¡è¿è¡Œ, è¿™ä¸¤ä¸ªå€¼çš„idä¸€ç›´åœ¨å˜, ä½†æ— è®ºä½¿ç”¨cloneè¿˜æ˜¯ä¸ä½¿ç”¨, ä¸¤è€…çš„å€¼ç›¸åŒ
    id(layer2.weight.data), id(layer1.weight.data)
    
    x = torch.rand(4, 3)
    z = layer2(layer1(x)).sum()
    y = torch.rand(())
    loss = z - y
    loss.backward()

    # clone: ä¸åŒ, ä¸ä½¿ç”¨ cloneï¼šç›¸åŒ
    layer1.weight.grad, layer2.weight.grad

    opt.step()

    # clone: ä¸åŒ, ä¸ä½¿ç”¨ clone: ç›¸åŒ
    layer1.weight, layer2.weight
    ```

**signature**
```python
tied_names: List[List[str]] = find_tied_parameters(model: nn.Module)
```

**å…¥å‚**

**å‡ºå‚**

- `tied_names`:
    ```python
    tied_names=[
        ["layer1.weight", "layer2.weight", "layer3.weight"],  # è¿™ä¸‰ä¸ªweightè¢«tiedäº†
        ["layer1.bias", "layer2.bias", "layer3.bias"] # è¿™ä¸‰ä¸ªbiasè¢«tiedäº†
    ]
    ```

**å†…éƒ¨é€»è¾‘**

æœ¬è´¨ä¸Š, æ˜¯ä½¿ç”¨ `nn.Module.named_parameters()`, ç„¶åä¸¤ä¸¤ä½¿ç”¨ `parameter1 is parameter 2` åˆ¤æ–­æ˜¯å¦ä¸º tied_weight. å› æ­¤å®Œå…¨ç›¸åŒçš„ submodule ç”±äº `nn.Module.named_parameters()` ä¼šè¢«é¢„å…ˆæ’é™¤æ‰, å› æ­¤ä¸ä¼šå‡ºç°åœ¨ç»“æœé‡Œã€‚

**ä¾‹å­**

```python
import torch.nn as nn
import torch
from accelerate.utils.modeling import find_tied_parameters
class ModelA(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(3, 3)
        self.layer2 = self.layer1
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        return out
    
class ModelB(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(3, 3)
        self.layer2 = nn.Linear(3, 3)
        self.layer2.weight = self.layer1.weight
        self.layer2.bias = self.layer1.bias
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        return out

model_a = ModelA()
model_b = ModelB()
# è¿™é‡Œè°ƒç”¨çš„æ˜¯ nn.Module çš„æ–¹æ³•, å®ƒä¼šå»é™¤é‡å¤çš„module, å› æ­¤åªä¼šæ‰“å°å‡º ModelA å’Œ ModelA.layer1
for module in model_a.modules():
    print(module)
find_tied_parameters(model_a)  # è¿”å›ä¸ºç©ºåˆ—è¡¨!!!

# è¿™é‡Œè°ƒç”¨çš„æ˜¯ nn.Module çš„æ–¹æ³•, å› ä¸ºlayer1å’Œlayer2æ˜¯åˆ†åˆ«å®šä¹‰çš„, åªæ˜¯å‚æ•°è¢«ç»‘å®š
for module in model_b.modules():
    print(module)
find_tied_parameters(model_a)  # è¿”å›ä¸º[['layer1.weight', 'layer2.weight'], ['layer1.bias', 'layer2.bias']]
```


### get_max_layer_size


**å†…éƒ¨é€»è¾‘**

é€’å½’éå†æ•´ä¸ªæ¨¡å‹, ç›´è‡³æœ€å°ä¸å¯åˆ†å‰²çš„ Module æˆ–è€…å•ç‹¬çš„ Parameter/Buffer, è®¡ç®—è¿™äº›ä¸å¯åˆ†å‰²çš„éƒ¨åˆ†æ‰€éœ€è¦çš„å†…å­˜çš„æœ€å¤§å€¼, åˆ¤æ–­ä¸å¯åˆ†å‰²çš„ Module çš„ä¾æ®ä¸º:

```python
module.__class__.__name__ in no_split_module_classes
# å‡è®¾ module æ˜¯ä¸€ä¸ª torch.nn.modules.linear.Linear å±‚
# module.__class__.__name__: "Linear"
# no_split_module_classes: ["Linear", "T5Block"]
```

### accelerate.utils.modeling.check_device_map

**signature**
```python
check_device_map(model: nn.Module, device_map)
```

**è¾“å…¥**

ç•¥

**è¾“å‡º**

æ— , åªå¯èƒ½ raise é”™è¯¯ä¿¡æ¯

**å†…éƒ¨é€»è¾‘**

æ£€æŸ¥ `device_map` æ˜¯å¦ä¼šè¦†ç›–ä½ `model` çš„æ‰€æœ‰å‚æ•°, æ³¨æ„: è¿™ä¸ªå‡½æ•°**ä¸æ£€æŸ¥** model å½“å‰å®é™…çš„å‚æ•°ä½ç½®æ˜¯å¦ä¸ device_map ä¸€è‡´ã€‚å†…éƒ¨çš„å®ç°é€»è¾‘æ˜¯å…ˆè·å– `all_names = model.state_dict().keys()`, ç„¶åå¯¹äº `device_map` ä¸­çš„æ¯ä¸€ä¸ª key, éƒ½åœ¨ `all_names` å°†å…¶æ’é™¤(æ’é™¤æ‰ key ä»¥åŠä»¥ `f"{key}."` å¼€å¤´çš„é”®), æœ€åæ–­è¨€ `all_names` ä¸ºç©ºåˆ—è¡¨ã€‚

å¤‡æ³¨: è¿™ä¸ªæ£€æŸ¥å…¶å®æœ‰ä¸€å®šçš„ç¼ºé™·ï¼Œä¾‹å¦‚ `device_map` æœ¬èº«æœ‰å¯èƒ½â€œä¸åˆæ³•â€ï¼Œä¾‹å¦‚ï¼š
```python
device_map = {"fc": 0, "fc.weight": 0, "fc.bias": "cpu"}
```

### accelerate.utils.offload.OffloadedWeightsLoader

```python
from collections.abc import Mapping
class OffloadedWeightsLoader(Mapping):
    def __init__(self,
        state_dict,
        save_folder=None,
        index=None,
        device=None
    ):
        self.all_keys = ... # list(state_dict.keys()) + index ä¸­çš„é”®
        self.device = device
        ...
    
    def __getitem__(self, key):
        if key in self.state_dict:
            return self.state_dict[key]
        else:
            weight = np.memmap(weight_file, dtype, shape, mode="r")
            weight = torch.tensor(weight)
            return weight
    
    ...
```

è¯´æ˜: ä» `dispatch_model` ä¸­å¯¹è¿™ä¸ªå‡½æ•°çš„ä½¿ç”¨é‡Œ:

- å½“ main_device ä¸º gpu æ—¶, å®ä¾‹åŒ–å‚æ•° state_dict ä¸ºæ‰€æœ‰åœ¨ cpu ä¸Šçš„å‚æ•°, save_folder ä¸º offload æ–‡ä»¶å¤¹, device ä¸º None
- å½“ main_device ä¸º cpu æ—¶, å®ä¾‹åŒ–å‚æ•° state_dict ä¸º None, save_folder ä¸º offload æ–‡ä»¶å¤¹, device ä¸º None

`__getitem__` æ–¹æ³•:
- å½“ main_device ä¸º gpu æ—¶, æ— è®ºåŸæœ¬çš„å‚æ•° offload åˆ° disk è¿˜æ˜¯æœ¬å°±åœ¨ cpu ä¸Š, éƒ½ä¼šå°† tensor è½¬ç§»åˆ° cpu ä¸Šè¿”å›, å·²ç¡®è®¤ï¼ŒåŸæœ¬åœ¨ disk ä¸Šçš„å‚æ•°ï¼Œè¿”å›çš„tensorå ç”¨å†…å­˜
- å½“ main_device ä¸º cpu æ—¶, åŸæœ¬ offload åˆ° disk ä¸Šçš„å‚æ•°ä¼šè½¬ç§»åˆ° cpu ä¸Šè¿”å›, å·²ç¡®è®¤: è¿”å›çš„tensorå ç”¨å†…å­˜

```python
# éªŒè¯ä»£ç 
import numpy as np
import psutil
import torch
nrows, ncols = 100000, 1000  # éœ€è¦å ç”¨å¤§çº¦ 400MB

def show_memory():
    mem = psutil.virtual_memory()
    print("Used", mem.used / 1024 / 1024, "MB")
    print("Free", mem.free / 1024 / 1024, "MB")
show_memory()                # Used: 602MB, Free: 2586MB
# f = np.memmap('memmapped.dat', dtype=np.float32, mode='w+', shape=(nrows, ncols))
# for i in range(nrows):
#     f[i, :] = np.random.rand(ncols)
# f.flush()
# del f
# show_memory()

f = np.memmap('memmapped.dat', dtype=np.float32, mode='r', shape=(nrows, ncols))
show_memory()                # Used: 602MB, Free: 2586MB
tensor = torch.tensor(f)
show_memory()                # Used: 985MB, Free: 2204MB, è¯´æ˜æ„å»º tensor éœ€è¦å ç”¨å¤§é‡å†…å­˜
```

### accelerte.hooks.attach_align_device_hook_on_blocks

**signature**
```python
attach_align_device_hook_on_blocks(
    module: nn.Module,
    execution_device: Optional[Union[torch.device, Dict[str, torch.device]]] = None,
    offload: Union[bool, Dict[str, bool]] = False,
    weights_map: Mapping = None,
    offload_buffers: bool = False,
    module_name: str = "",
    skip_keys: Optional[Union[str, List[str]]] = None,
    preload_module_classes: Optional[List[str]] = None,
)  # ç»™ module åŠ ä¸Š hook, æ­¤å‡½æ•°æœ¬èº«æ— è¿”å›
```

`PretrainedModel.from_pretrain` ä¸­è°ƒç”¨ `dispath_model` æ–¹æ³•, åœ¨å†…éƒ¨è°ƒç”¨ `attach_align_device_hook_on_blocks` æ‰€ä¼ å…¥çš„å‚æ•°æœ‰å¦‚ä¸‹è¯´æ˜ï¼š

- main_device ä¸º gpu æ—¶, å‡è®¾ `device_map` å¦‚ä¸‹
    ```python
    device_map = {
        'embed': 0                              # Module, åŒ…å«æƒé‡ embed.weight
        'feed_forward.layers.0': 0,             # Module, åŒ…å«æƒé‡ feed_forward.layers.0.weight ä¸ feed_forward.layers.0.bias
        'feed_forward.layers.1.weight': 1,      # Parameter
        'feed_forward.layers.1.bias': "cpu",    # Parameter
        'feed_forward.layers.2': "cpu",         # Module
        'feed_forward.layers.3': "disk",        # Module
        'head': "disk"                          # Module, åŒ…å« head.out Module
    }
    ```
    offload_buffersã€module_nameã€skip_keysã€preload_module_classes é€šå¸¸ä¸ºé»˜è®¤å€¼, å‰©ä½™å…¶ä»–å‚æ•°ä¸º:
    ```python
    # ä¼ªä»£ç 
    zip(execution_device, offload) = {
        'embed.weight': (0, False),  # æ³¨æ„, key å¿…é¡»æ˜¯ tensor çš„ name 
        'feed_forward.layers.0.weight': (0, False),
        'feed_forward.layers.0.bias': (0, False),
        'feed_forward.layers.1.weight': (1, False),
        # weight_map åŒ…å«ä»¥ä¸‹çš„ tensor
        'feed_forward.layers.1.bias': (0, True),   # åŸæœ¬device_mapåœ¨cpuä¸Š, execution_device å°†å…¶æ˜ å°„åˆ°main_deviceä¸Š
        'feed_forward.layers.2.weight': (0, True), # åŸæœ¬device_mapåœ¨cpuä¸Š, execution_device å°†å…¶æ˜ å°„åˆ°main_deviceä¸Š
        'feed_forward.layers.2.bias': (0, True),
        'feed_forward.layers.3.weight': (0, True), # åŸæœ¬device_mapåœ¨diskä¸Š, execution_device å°†å…¶æ˜ å°„åˆ°main_deviceä¸Š
        'feed_forward.layers.3.bias': (0, True),
        "head.out.weight": (0, True),
        "head.out.bias": (0, True)
    }
    ```

- main_device ä¸º cpu æ—¶, å‡è®¾ `device_map` å¦‚ä¸‹
    ```python
    device_map = {
        'embed': "cpu"                         # Module, åŒ…å«æƒé‡ embed.weight
        'feed_forward.layers.0': "cpu",        # Module, åŒ…å«æƒé‡ feed_forward.layers.0.weight ä¸ feed_forward.layers.0.bias
        'feed_forward.layers.1.weight': "cpu", # Parameter
        'feed_forward.layers.1.bias': "cpu",   # Parameter
        'feed_forward.layers.2': "cpu",        # Module
        'feed_forward.layers.3': "disk",       # Module
        'head': "disk"                         # Module, åŒ…å« head.out Module
    }
    ```
    offload_buffersã€module_nameã€skip_keysã€preload_module_classes é€šå¸¸ä¸ºé»˜è®¤å€¼, å‰©ä½™å…¶ä»–å‚æ•°ä¸º:
    ```python
    # ä¼ªä»£ç 
    zip(execution_device, offload) = {
        'embed.weight': ("cpu", False),  # æ³¨æ„, key å¿…é¡»æ˜¯ tensor çš„ name 
        'feed_forward.layers.0.weight': ("cpu", False),
        'feed_forward.layers.0.bias': ("cpu", False),
        'feed_forward.layers.1.weight': ("cpu", False),
        'feed_forward.layers.1.bias': ("cpu", False),
        'feed_forward.layers.2.weight': ("cpu", False),
        'feed_forward.layers.2.bias': ("cpu", False),
        # weight_map åŒ…å«ä»¥ä¸‹çš„ tensor
        'feed_forward.layers.3.weight': ("cpu", True), # åŸæœ¬device_mapåœ¨diskä¸Š, execution_device å°†å…¶æ˜ å°„åˆ°main_deviceä¸Š
        'feed_forward.layers.3.bias': ("cpu", True),
        "head.out.weight": ("cpu", True),
        "head.out.bias": ("cpu", True)
    }
    ```

## ç®€åŒ–ç‰ˆå®ç°

```python
def from_pretrain(cls, pretrained_name_or_path, device_map: Dict):
    # åªè€ƒè™‘device_mapå«æœ‰gpu,cpu,disk, ä¸”æ¨¡å‹æ–‡ä»¶ä¸ºshardçš„æƒ…å½¢, å¹¶å‡å®šloadçš„å‚æ•°çš„keyä¸æ¨¡å‹å®Œå…¨åŒ¹é…
    
    config = ...
    # step 1: åˆå§‹åŒ–ç©ºæ¨¡å‹æ–‡ä»¶
    with init_empty_weights():
        model = cls(config, ...)
    # step 2: é€ä¸ªload, å¹¶åˆå§‹åŒ–æ¨¡å‹æ–‡ä»¶
    ...
    # æ­¤æ—¶, æ¨¡å‹çš„æƒé‡çš„device, ä¸€éƒ¨åˆ†åœ¨gpuä¸Š, ä¸€éƒ¨åˆ†åœ¨cpuä¸Š, ä¸€éƒ¨åˆ†ä»ä¸º meta
    # step 3:
    model.tie_weights()
    model.eval()
    # step 4: dispatch (add hook)
```