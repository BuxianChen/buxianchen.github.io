---
layout: post
title: "(WIP) Memory efficient inference (ğŸ¤—'s dispatch and offload)"
date: 2023-06-01 17:20:04 +0800
labels: [transformers]
---

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- ğŸ¤— transformer/accelerate çš„ `init_empty_weight` å’Œ `dispatch_model` ç­‰ API

å‚è€ƒèµ„æ–™

- [https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling)
- [https://huggingface.co/blog/accelerate-large-models](https://huggingface.co/blog/accelerate-large-models)

æ¶‰åŠå†…å®¹

- ä½¿ç”¨æ–¹æ³•ã€æ ·ä¾‹ã€‘
- æºç åˆ†æ

## æ ·ä¾‹ä»£ç 

```
# requirements.txt
torch >= 1.10.0
transformers==4.29.2
```

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausualLM, AutoConfig
from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch

pretrained_name_or_path = "./fnlp/moss-moon-003-sft"

# method 1: è°ƒç”¨ç›¸å…³çš„è¾…åŠ©å‡½æ•°(æœ‰åˆ©äºäº†è§£æ•´ä¸ªè¿‡ç¨‹)
config = AutoConfig.from_pretrained(path, trust_remote_code=True)
with init_empty_weights():
    model = AutoModelForCausalLM.from_config(
        config,
        # åœ¨æ‰§è¡Œ cls(...) ä¹‹å‰, ä¼šåˆ©ç”¨ torch.set_default_dtype(torch_dtype)å…¨å±€è®¾å®šé»˜è®¤æµ®ç‚¹æ•°ç±»å‹
        torch_dtype=torch.float16,
        trust_remote_code=True)
# !!!è¿™ä¸€æ­¥æ˜¯å¿…é¡»çš„!!!
model.tie_weights()

max_memory = {
    0: "10GB",
    1: "10GB",
    "cpu": "20GB",
}

device_map = infer_auto_device_map(
    model,
    max_memory=max_memory,
    dtype=None,  # ä½¿ç”¨modelä¸­çš„tensorç±»å‹ä¼°ç®—æ‰€éœ€çš„å†…å­˜/æ˜¾å­˜
    no_split_module_classes=model._no_split_modules,  # æŸäº›submoduleå¿…é¡»æ”¾åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Š
)

# å‰é¢çš„ä¸¤ä¸ªæ­¥éª¤æ˜¯ä¸ºäº†è·å–åˆé€‚çš„ device_map

# å†™æ³•1: å¾…ç¡®è®¤
# model = AutoModelForCausalLM.from_pretrained(
#     pretrained_name_or_path,
#     trust_remote_code=True,
#     # åœ¨æ‰§è¡Œ cls(...) ä¹‹å‰, ä¼šåˆ©ç”¨ torch.set_default_dtype(torch_dtype)å…¨å±€è®¾å®šé»˜è®¤æµ®ç‚¹æ•°ç±»å‹
#     torch_dtype=torch.float16,  
#     device_map=device_map,
#     offload_folder="offload",
#     offload_state_dict=True,  # ??
#     # low_cpu_memory_usage=True,  # æŸäº›æƒ…å†µä¸‹ä¼šæ ¹æ®å…¶ä»–å‚æ•°è‡ªåŠ¨è®¾å®šä¸ºTrue
#     # _fast_init=True,  # é»˜è®¤å€¼ä¸ºTrue
# )

# å†™æ³•2: å®˜ç½‘æ¨èçš„å†™æ³•(https://huggingface.co/docs/accelerate/usage_guides/big_modeling)
model = load_checkpoint_and_dispatch(
    model,
    pretrained_name_or_path,
    device_map=device_map
)


# method 2: (ç›´æ¥ä¸€æ­¥åˆ°ä½), ã€å¾…ç¡®è®¤ã€‘
model = AutoModelForCausalLM.from_pretrained(
    pretrained_name_or_path,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    # low_cpu_mem_usage=True,
    max_memory=max_memory,
    device_map="sequential",  # ä¼¼ä¹æ˜¯å¿…é¡»è®¾ç½®ä¸ºè¿™ä¸ª, æ‰ä¼šè®©max_memoryç”Ÿæ•ˆ
    offload_folder="offload",
    # offload_state_dict=True
)


tokenizer = AutoTokenizer.from_pretrained(
    pretrained_name_or_path,
    trust_remote_code=True
)

inputs = tokenizer(["Alice and Bob"], return_tensors="pt")
output = model.generate(
    inputs=inputs["input_ids"],
    temperature=0.7,
    top_k=0,  # 0 è¡¨ç¤ºä¸ä½¿ç”¨ top_k å¯¹ logits è¿›è¡Œåå¤„ç†
    top_p=0.8,
    repetition_penalty=1.02,
    max_length=50,
    do_sample=True,
    return_dict_in_generate=True,
    attention_mask=inputs["attention_mask"],
)

texts = tokenizer.batch_decode(output["sequences"])
```


å¤‡æ³¨ï¼šã€ä»¥ä¸‹APIé—´çš„å…³ç³»æœ‰äº›æ··ä¹±ï¼Œå¾…ç ”ç©¶ã€‘

- `PretrainedModel.from_pretrained(...)`
- `AutoModelForXXX.from_config(...)`
- `YYYModelForXXX._from_config(...)`
- `accelerate.init_empty_weight(...)`
- `accelerate.infer_auto_device_map(...)`
- `accelerate.get_balanced_memory(...)`
- `accelerate.dispatch_model(...)`
- `accelerate.load_checkpoint_and_dispatch(...)`

è¿™äº›å‚æ•°ä¹‹é—´æ€ä¹ˆé…åˆçš„:

- `torch_dtype`, `model.config.torch_dtype`
- `device_map`: None, auto, balanced, balanced_low_0, sequential, dict(...)
- `offload_state_dict`: 
- `low_cpu_mem_usage`:
- `_fast_init`:

