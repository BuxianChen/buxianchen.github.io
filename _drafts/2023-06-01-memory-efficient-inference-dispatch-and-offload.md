---
layout: post
title: "(WIP) Memory efficient inference (ğŸ¤—'s dispatch and offload)"
date: 2023-06-01 17:20:04 +0800
labels: [transformers]
---

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- ğŸ¤— transformer/accelerate çš„ `init_empty_weight` å’Œ `dispatch_model` ç­‰ API

å‚è€ƒèµ„æ–™

- [https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling)
- [https://huggingface.co/blog/accelerate-large-models](https://huggingface.co/blog/accelerate-large-models)

æ¶‰åŠå†…å®¹

- ä½¿ç”¨æ–¹æ³•
- æ–‡æ¡£è¡¥å……ï¼šæŸäº›è¾…åŠ©å‡½æ•°å®˜æ–¹æ–‡æ¡£ç¼ºå¤±ï¼ŒæŸäº›å¯¹å¤–APIçš„å‚æ•°è¯´æ˜ä¸æ˜ç¡®
- æºç åˆ†æ

## ç®€ä»‹

é’ˆå¯¹å¤§æ¨¡å‹çš„**æ¨ç†è¿‡ç¨‹** ğŸ¤— Transformer/accelerate å¼•å…¥äº†ä¸€äº› API, ä½¿å¾—å¤§æ¨¡å‹èƒ½åœ¨èµ„æºæœ‰é™çš„æœåŠ¡å™¨(å³æ˜¾å­˜æœ‰é™,å†…å­˜æœ‰é™)ä¸Šè¿è¡Œ, è¿è¡Œçš„å”¯ä¸€çš„è¦æ±‚æ˜¯æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ä»¥åŠæ—¶é—´, æ³¨æ„æ¨¡å‹ä»ç„¶æ˜¯åœ¨ raw pytorch çš„ç¯å¢ƒä¸‹è¿è¡Œçš„, è€Œéé€šè¿‡é‡åŒ–åŠå¹¶è¡ŒæŠ€æœ¯è¿›è¡ŒèŠ‚çœæ˜¾å­˜ä»¥åŠåŠ é€Ÿ

## æ ·ä¾‹ä»£ç 

```
# requirements.txt
torch >= 1.10.0
transformers==4.29.2
accelerate==0.19.0
```

å†™æ³•ä¸€: **æ³¨æ„: æœ¬ç¯‡åšå®¢åç»­å‡ ä¹å…¨éƒ¨çš„å†…å®¹éƒ½æ˜¯ä¸ºäº†è§£é‡Šè¿™ä¸€è¡Œåˆå§‹åŒ–æ¨¡å‹çš„ä»£ç **

```python
pretrained_name_or_path = "./fnlp/moss-moon-003-sft"
max_memory = {0: "12GB", 1: "20GB", "cpu": "20GB"}
model = AutoModelForCausalLM.from_pretrained(
    pretrained_name_or_path,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    # low_cpu_mem_usage=True,  # è®¾ç½®äº†device_mapålow_cpu_mem_usageä¼šè¢«é»˜è®¤è®¾ç½®ä¸ºTrue
    max_memory=max_memory,
    device_map="sequential",  # ä¼¼ä¹æ˜¯å¿…é¡»è®¾ç½®ä¸ºè¿™ä¸ª, æ‰ä¼šè®©max_memoryç”Ÿæ•ˆï¼ˆè§¦å‘infer_auto_device_mapçš„è°ƒç”¨ï¼‰
    offload_folder="offload",
    offload_state_dict=True  # cpuä¸Šçš„å‚æ•°æš‚æ—¶offload, å†é‡æ–°åŠ è½½å›cpu, ä»…ç”¨äºå‡å°‘åŠ è½½æ¨¡å‹æ—¶çš„å†…å­˜å ç”¨, ä½†åŠ è½½é€Ÿåº¦ä¼šå˜æ…¢ä¸€äº›
)
# device_map çš„å®é™…å†…å®¹å¯ç”± model.hf_device_map è·å–
```

å†™æ³•äºŒ:


```python
import torch
from transformers import AutoTokenizer, AutoModelForCausualLM, AutoConfig
from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch

pretrained_name_or_path = "./fnlp/moss-moon-003-sft"
config = AutoConfig.from_pretrained(path, trust_remote_code=True)
with init_empty_weights():
    model = AutoModelForCausalLM.from_config(
        config,
        # åœ¨æ‰§è¡Œ cls(...) ä¹‹å‰, ä¼šåˆ©ç”¨ torch.set_default_dtype(torch_dtype)å…¨å±€è®¾å®šé»˜è®¤æµ®ç‚¹æ•°ç±»å‹
        torch_dtype=torch.float16,
        trust_remote_code=True)
# !!!è¿™ä¸€æ­¥æ˜¯å¿…é¡»çš„!!!(empty weight model ä¹Ÿå¯ä»¥tie weight?)
model.tie_weights()

max_memory = {
    0: "10GB",
    1: "10GB",
    "cpu": "20GB",
}

device_map = infer_auto_device_map(
    model,
    max_memory=max_memory,
    dtype=None,  # ä½¿ç”¨modelä¸­çš„tensorç±»å‹ä¼°ç®—æ‰€éœ€çš„å†…å­˜/æ˜¾å­˜
    no_split_module_classes=model._no_split_modules,  # æŸäº›submoduleå¿…é¡»æ”¾åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Š
)

# å‰é¢çš„ä¸¤ä¸ªæ­¥éª¤æ˜¯ä¸ºäº†è·å–åˆé€‚çš„ device_map
# https://huggingface.co/docs/accelerate/usage_guides/big_modeling
model = load_checkpoint_and_dispatch(
    model,
    pretrained_name_or_path,
    device_map=device_map
)
```

ç”¨æ³•:

```python
tokenizer = AutoTokenizer.from_pretrained(
    pretrained_name_or_path,
    trust_remote_code=True
)

inputs = tokenizer(["Alice and Bob"], return_tensors="pt")
output = model.generate(
    inputs=inputs["input_ids"],
    temperature=0.7,
    top_k=0,  # 0 è¡¨ç¤ºä¸ä½¿ç”¨ top_k å¯¹ logits è¿›è¡Œåå¤„ç†
    top_p=0.8,
    repetition_penalty=1.02,
    max_length=50,
    do_sample=True,
    return_dict_in_generate=True,
    attention_mask=inputs["attention_mask"],
)

texts = tokenizer.batch_decode(output["sequences"])
```

## <font color=red>åŸç†æ€»ç»“</font>

è¿™é‡Œå…ˆå¯¹ä¸Šè¿°ä»£ç çš„åŸç†åšä¸€ä¸ªä»‹ç», ä¸æƒ³å¤ªè¿‡æ·±å…¥åœ°å­¦ä¹ æºç åªéœ€è¦çœ‹è¿™é‡Œçš„æè¿°æˆ–è€…å‚è€ƒ [å®˜æ–¹æ–‡æ¡£]([https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling]) å’Œ [å®˜æ–¹åšå®¢](https://huggingface.co/blog/accelerate-large-models) å³å¯, åœ¨ä¸Šè¿° `from_pretrained` çš„è¿‡ç¨‹ä¸­, å®é™…ä¸Šå‘ç”Ÿäº†å¦‚ä¸‹å‡ ä»¶äº‹æƒ…:

- é¦–å…ˆåˆå§‹åŒ–ä¸€ä¸ªç©ºæ¨¡å‹
  
  æ³¨æ„è¿™ä¸ªç©ºæ¨¡å‹æ²¡æœ‰éšæœºåˆå§‹åŒ–çš„æƒé‡, æŠ€æœ¯ä¸Šæ¥è¯´, å®ç°ä¸Šæœ¬è´¨æ¥æºäº `pytorch>=1.9.0` å¯ä»¥å°† tensor çš„ device è®¾ç½®ä¸º `torch.device("meta")`, ğŸ¤— accelerate å¼•å…¥äº†ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ `init_empty_weights`, åœ¨ä¸Šä¸‹æ–‡èŒƒå›´å†…, å°† `nn.Module.register_parameter` ä¸ `nn.Module.register_buffer` åšäº†æ›¿æ¢, ä½¿å¾—æ‰€æœ‰æ¨¡å‹çš„æ‰€æœ‰ parameter ä¸ buffer çš„è®¾å¤‡éƒ½è¢«è®¾ç½®ä¸ºäº† `torch.device("meta")`
  ```python
  with init_empty_weights():
    model = cls(config, *model_args, **model_kwargs)
  ```
- ç„¶åæ ¹æ® model ä¸­æ¯ä¸ªå‚æ•°çš„ dtype ä»¥åŠ shape å¯ä»¥ä¼°ç®—æ¯ä¸ªå‚æ•°æ‰€éœ€è¦çš„å­˜å‚¨ç©ºé—´, ä¾æ®ç»™å®šçš„ max_memory å†³å®šæ¯ä¸ªæ¨¡å‹çš„æ¯ä¸ªå‚æ•°åº”è¯¥æ”¾åœ¨å“ªä¸ª device ä¸Š, å³å¾—åˆ° device_map
  
  è¿™éƒ¨åˆ†æ¶‰åŠåˆ°çš„ API ä¸»è¦æ˜¯ ğŸ¤— accelerate çš„ `infer_auto_device_map`, ä»¥åŠæ‰€ä¾èµ–çš„ `compute_module_sizes`, `get_max_layer_size` ç­‰

- æ¥ä¸‹æ¥å°†å‚æ•°æ–‡ä»¶é€ä¸ªåŠ è½½è¿›cpu, å¹¶å°†å‚æ•°é€ä¸ªæ”¾ç½®åœ¨ä¸Šä¸€æ­¥éª¤æ‰€ç¡®å®šçš„è®¾å¤‡ä¸Š

  è¿™ä¸€æ­¥éª¤ä¸»è¦æ¶‰åŠåˆ°ä¸€äº›å°ç»†èŠ‚, é¦–å…ˆè¦ä¿è¯ cpu çš„å†…å­˜èƒ½è£…ä¸‹æœ€å¤§çš„å‚æ•°æ–‡ä»¶, å³ `state_dict=torch.load("pytorch_model-00001-of-00004.bin")`, ç„¶åå°† `state_dict` æ”¾ç½®åˆ° gpu/cpu/disk ä¸Š: å¦‚æœéœ€è¦æ”¾ç½®åœ¨ gpu ä¸Š, åˆ™éœ€è¦æ‹·è´ä¸€ä»½ tensor è‡³ gpu, å¦‚æœéœ€è¦æ”¾ç½®åœ¨ cpu ä¸Š, å®é™…ä¸Šä¸ä¼šå‘ç”Ÿæ‹·è´, å†…å­˜ä¸ä¼šå¢åŠ , ä»…ä»…æ˜¯ tensor çš„å¼•ç”¨è®¡æ•°åŠ  1, å¦‚æœéœ€è¦æ”¾ç½®åœ¨ disk ä¸Š, åˆ™ä¼šåˆ©ç”¨åˆ° `numpy.memmap` çš„åŠŸèƒ½å°†æ•°æ®ä¿å­˜åœ¨ç¡¬ç›˜ä¸Š. ä¸ºäº†è¿›ä¸€æ­¥èŠ‚çœ cpu å†…å­˜(`offload_state_dict=True`), å¯ä»¥å…ˆå°†æœ¬åº”è¯¥æ”¾ç½®åœ¨ cpu ä¸Šçš„ tensor ä¹Ÿå…ˆæ”¾ç½®åœ¨ç¡¬ç›˜ä¸Š, å¹¶è®°å½•å¥½æ¯ä¸ª tensor ä¸ç›¸åº”çš„ç¡¬ç›˜æ–‡ä»¶çš„æ˜ å°„å…³ç³», ç­‰å…¨éƒ¨çš„å‚æ•°æ–‡ä»¶å¤„ç†å®Œæˆå, å†é‡æ–°å°†æœ¬åº”è¯¥æ”¾åœ¨ cpu ä¸Šçš„ tensor åŠ è½½å› cpu, å¹¶åˆ é™¤æ‰è¿™éƒ¨åˆ†çš„ç¡¬ç›˜æ–‡ä»¶

- ä¿®æ”¹ model åŠå…¶å­æ¨¡å—çš„ forward å‡½æ•°, ä¿è¯æ¯ä¸ªå­æ¨¡å—åœ¨ä½¿ç”¨ forward å‡½æ•°è¿›è¡Œè¿ç®—æ—¶, ç›¸åº”çš„ tensor æ“ä½œæ•°(è¾“å…¥åŠæ¨¡å‹çš„æƒé‡)éƒ½åœ¨ç›¸åŒçš„è®¾å¤‡ä¸Š

  è¿™ä¸€æ­¥éª¤çš„å®ç°æ‰€ä½¿ç”¨çš„ç»†èŠ‚åœ¨å®˜æ–¹æ–‡æ¡£ä¸­è¢«ç§°ä¸º hook, ä½†ä¸ªäººè®¤ä¸ºè¿™ä¸ torch.nn.Module ä¸­çš„ hook è¿˜æ˜¯æœ‰ä¸€å®šçš„åŒºåˆ«, ç§°ä¸ºä¿®æ”¹ forward å‡½æ•°å¯èƒ½æ›´ä¸ºå‡†ç¡®. éœ€è¦æ³¨æ„çš„ç»†èŠ‚æœ‰: é¦–å…ˆä¼šç¡®å®šä¸€ä¸ªæ‰€è°“çš„ main_device, main_device ä¸€èˆ¬ä¸ºæŸä¸€ä¸ª gpu.

  - åœ¨è¿›è¡Œè¿ç®—å‰: å¯¹äºæ¨¡å‹ä¸­åŸæœ¬åœ¨ disk ä¸Šä¿å­˜çš„ tensor, åœ¨è¿›è¡Œè¿ç®—æ—¶, ä¼šé¦–å…ˆå°† disk ä¸Šçš„æ•°æ®è¯»å…¥ cpu, ç„¶åå°†å…¶è½¬ç§»åˆ° main_device ä¸Š; å¯¹äºæ¨¡å‹ä¸­åŸæœ¬åœ¨ cpu ä¸Šä¿å­˜çš„ tensor, åˆ™å°†å…¶è½¬ç§»åˆ° main_device ä¸Š; å¯¹äºæ¨¡å‹ä¸­åŸæœ¬å°±åœ¨ gpu ä¸Šçš„ tensor, åˆ™ä¸è¿›è¡Œ tensor çš„è®¾å¤‡è½¬ç§». ç„¶åå°†è¾“å…¥çš„ tensor ä¹Ÿè½¬ç§»åˆ°ç›¸åŒ¹é…çš„ gpu ä¸Š
  - è¿ç®—: å³æ™®é€šçš„ CUDA ä¸Šçš„ç®—å­è¿ç®—
  - è¿ç®—ç»“æŸå: å¯¹äºæ¨¡å‹ä¸­åŸæœ¬åœ¨ disk ä¸Šä¿å­˜çš„ tensor, åˆ™é‡æ–°å°† tensor ä» main_device è½¬ç§»å› cpu å†ä¿å­˜ä¼š disk, å¯¹äºæ¨¡å‹ä¸­åŸæœ¬åœ¨ cpu ä¸Šä¿å­˜çš„ tensor, åˆ™ tensor ä» main_device è½¬ç§»å› cpu, å¯¹äºæ¨¡å‹ä¸­åŸæœ¬å°±åœ¨ gpu ä¸Šçš„ tensor, åˆ™ä¸è¿›è¡Œ tensor çš„è®¾å¤‡è½¬ç§».

  å¦ä¸€ä¸ªç»†èŠ‚æ˜¯: ä¸Šè¿°çš„ tensor è½¬ç§»è¿‡ç¨‹æ¯æ¬¡çš„è½¬ç§»èŒƒå›´æ˜¯æ¯æ¬¡è½¬ç§»ä¸€ä¸ª submodule ä¸‹æ‰€æœ‰ç›´æ¥çš„ paramter ä¸ buffer. submodule é‡Œçš„ submodule çš„ paramter å’Œ buffer åˆ™ä¼šåœ¨è§¦å‘å®ƒæœ¬èº«çš„ forward å‡½æ•°æ—¶è¢«è½¬ç§»è®¾å¤‡

  ä¸Šè¿°æ­¥éª¤æ‰€æ¶‰åŠçš„ API ä¸»è¦æ˜¯ ğŸ¤— accelerate çš„ `dispatch_model`, ä»¥åŠæ‰€ä¾èµ–çš„ `OffloadedWeightsLoader`, `AlignDeviceHook` ç­‰

é€šè¿‡ä»¥ä¸Šæ­¥éª¤, ğŸ¤— å®ç°äº†å°†å¤§æ¨¡å‹çš„**æ¨ç†è¿‡ç¨‹**è¿è¡Œåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡çš„ç›®æ ‡

å…¶ä»–ç»†èŠ‚: å¯¹äºå­˜åœ¨ tie_weight æƒ…å†µçš„æ¨¡å‹, å¯¹è¿™éƒ¨åˆ†å‚æ•°åšäº†ä¸€äº›é¢å¤–çš„ç‰¹æ®Šå¤„ç†

å¤‡æ³¨ï¼šã€ä»¥ä¸‹APIé—´çš„å…³ç³»æœ‰äº›æ··ä¹±ï¼Œå¾…ç ”ç©¶ã€‘

- `PretrainedModel.from_pretrained(...)`
- `AutoModelForXXX.from_config(...)`
- `YYYModelForXXX._from_config(...)`
- `accelerate.init_empty_weight(...)`: ä¸Šä¸‹æ–‡ç®¡ç†å™¨, åœ¨ä¸Šä¸‹æ–‡èŒƒå›´å†…, ä¿®æ”¹ `nn.Module` çš„ `register_parameter` ä¸ `register_buffer` å‡½æ•°, ä½¿å¾— `nn.Module` ä¸­çš„ tensor æ€»æ˜¯åœ¨ `torch.device("meta")` ä¸Š
- `accelerate.get_balanced_memory(...)`: åœ¨è®¾å®š `device_map` ä¸º `auto/balanced/balanced_low_0` æ—¶, å¾—åˆ° `max_memory` å­—å…¸
- `accelerate.infer_auto_device_map(...)`: æ ¹æ® `max_memory` å­—å…¸å¾—åˆ° `device_map` å­—å…¸, ç¡®å®šæ¯ä¸ª module æˆ– parameter/buffer çš„è®¾å¤‡
- `accelerate.dispatch_model(...)`: æ ¹æ® `device_map` å­—å…¸å¢åŠ  `forward` çš„ hook
- `accelerate.load_checkpoint_and_dispatch(...)`: å¹¶æœªåœ¨ ğŸ¤— Transformer è¢«ä½¿ç”¨ (ä¸ from_pretrain æœ‰äº›é‡å¤ä»£ç , çŒœæƒ³åº”è¯¥æ˜¯ä»£ç è¿˜éœ€è¦é‡æ„å¥½)

è¿™äº›å‚æ•°ä¹‹é—´æ€ä¹ˆé…åˆçš„:

- `torch_dtype`, `model.config.torch_dtype`
- `device_map`: None, auto, balanced, balanced_low_0, sequential, dict(...)
- `offload_state_dict`: å¦‚æœä¸º False, é‚£ä¹ˆ device_map ä¸º cpu çš„ tensor ä¼šæ”¾åœ¨cpuä¸Š, å› æ­¤åœ¨åŠ è½½è¿‡ç¨‹ä¸­ï¼Œcpuçš„å†…å­˜è¦åŒ…å«ä¸€ä»½shardçš„state_dictä¸åº”è¯¥æ”¾åœ¨cpuä¸Šçš„modelé‡Œçš„tensorã€‚å¦‚æœè®¾ç½® `offload_state_dict` ä¸º Trueï¼Œé‚£ä¹ˆå¯¹äº device_map ä¸º cpu çš„ tensor ä¼šå…ˆä¸´æ—¶ offload åˆ° disk ä¸Šï¼Œè¿™æ ·å­ä¿è¯ cpu çš„å†…å­˜åªè¦èƒ½è£…ä¸‹æœ€å¤§çš„ shard å³å¯ï¼Œåœ¨æ‰€æœ‰ shard å¤„ç†å®Œæˆåï¼Œåœ¨å°† offload çš„ tensor å…¨éƒ¨ç§»å›åˆ° cpu ä¸Šï¼ˆoffloadç›®å½•ä¹Ÿå°†è¢«åˆ é™¤ï¼‰ã€‚ç”±æ­¤ cpu çš„å†…å­˜åªè¦è¶…è¿‡æœ€å¤§shardã€ä»¥åŠèƒ½è£…ä¸‹æ”¾åœ¨ cpu ä¸Šçš„ model çš„ tensor å³å¯ï¼Œè€Œä¸æ˜¯éœ€è¦èƒ½è£…ä¸‹è¿™ä¸¤è€…ä¹‹å’Œã€‚
- `low_cpu_mem_usage`:
- `_fast_init`:


## ä¸»è¦APIåŠåŠŸèƒ½

### `PretrainedModel.from_pretrained`

#### low_cpu_mem_usage å‚æ•°

æŒ‰ç…§ [ğŸ¤— blog](https://huggingface.co/blog/accelerate-large-models) çš„æè¿°, low_cpu_mem_usage ä¸º False ä»¥åŠ True åˆ†åˆ«å¯¹åº”å¦‚ä¸‹ä¸¤ç§æµç¨‹

æµç¨‹ä¸€ï¼š
> 1. Create the model
> 2. Load in memory its weights (in an object usually called state_dict)
> 3. Load those weights in the created model
> 4. Move the model on the device for inference

æµç¨‹äºŒï¼š
> 1. Create an empty (e.g. without weights) model
> 2. Decide where each layer is going to go (when multiple devices are available)
> 3. Load in memory parts of its weights
> 4. Load those weights in the empty model
> 5. Move the weights on the device for inference
> 6. Repeat from step 3 for the next weights until all the weights are loaded

ç»è¿‡å¯¹æºç çš„ä»”ç»†åˆ†æ, å°†ä¸Šè¿°è¿‡ç¨‹ç»†åŒ–å¦‚ä¸‹:

low_cpu_mem_usage çš„é»˜è®¤å€¼ä¸º False, ä½†å½“å…¶ä»–ä¸€äº›å‚æ•°è¢«è®¾å®šçš„æƒ…å†µä¸‹, low_cpu_mem_usage ä¼šè¢«è‡ªåŠ¨è®¾ç½®ä¸º True

- device_map ä¸ä¸º None æ—¶, low_cpu_mem_usage ä¼šè¢«è‡ªåŠ¨è®¾ç½®ä¸º True
- ...


**low_cpu_mem_usage=False**

å½“æ¨¡å‹çš„å‚æ•°æ–‡ä»¶åªæœ‰ä¸€ä¸ªæ—¶, ä¸æµç¨‹ä¸€çš„æè¿°ä¸€è‡´ï¼Œå› æ­¤åœ¨æµç¨‹ä¸€çš„ç¬¬ 2 æ­¥ç»“æŸæ—¶ï¼Œcpu ä¸­ä¼šæœ‰ç€ä¸¤ä»½æƒé‡

- åˆå§‹åŒ–ä¸€ä¸ªéšæœºå‚æ•°çš„æ¨¡å‹ï¼ˆåœ¨CPUä¸Šï¼‰
- load `pytorch_model.bin`
- å°†æƒé‡æ”¾å…¥æ¨¡å‹ä¸­ï¼ˆCPUä¸Šï¼‰

å½“æ¨¡å‹çš„å‚æ•°æ–‡ä»¶æœ‰å¤šä¸ªæ—¶ï¼ˆå³æ‰€è°“çš„ "shard checkpoint"ï¼‰

- åˆå§‹åŒ–ä¸€ä¸ªéšæœºå‚æ•°çš„æ¨¡å‹ï¼ˆåœ¨CPUä¸Šï¼‰
- é€ä¸ª `pytorch_model-0000x-of-0000y.bin` æ–‡ä»¶, å¹¶ç”¨äºåŠ è½½è‡³æ¨¡å‹ï¼ˆåœ¨CPUä¸Šï¼‰ä¸­, æ¯ä¸ªæ–‡ä»¶ load è¿›æ¨¡å‹å load çš„å‚æ•°å­—å…¸

å› æ­¤å®é™…ä¸Šåªéœ€è¦ â€œæ¨¡å‹æ–‡ä»¶å¤§å°+1ä¸ªshardå‚æ•°â€ çš„ CPU å†…å­˜ã€‚

**low_cpu_mem_usage=True**

å½“æ¨¡å‹çš„å‚æ•°æ–‡ä»¶åªæœ‰ä¸€ä¸ªæ—¶:

- åˆå§‹åŒ–ä¸€ä¸ªç©ºæ¨¡å‹(device="meta")
- å¦‚æœæ¨¡å‹çš„å‚æ•°æ–‡ä»¶åªæœ‰ä¸€ä¸ª, åˆ™é€šè¿‡ load `pytorch_model.bin` ï¼ˆload åœ¨ cpuä¸Šï¼‰æ¥è·å–ä¿å­˜çš„æ¨¡å‹å‚æ•°çš„ key åˆ—è¡¨, éšåå°† load çš„å‚æ•°å­—å…¸å…ˆé‡Šæ”¾
- æ ¹æ® device_map, max_memory, torch_dtype ç¡®å®šæ¯ä¸ªå‚æ•°æœ€ç»ˆéœ€è¦å­˜æ”¾çš„è®¾å¤‡
- é‡æ–° load `pytorch_model.bin`ï¼ˆload åœ¨ cpu ä¸Šï¼‰
- å°†æƒé‡æ”¾å…¥ç›¸åº”çš„è®¾å¤‡ä¸­ã€‚æ³¨æ„ï¼Œè¿™ä¸€æ­¥éª¤å¯èƒ½ä¼šæœ‰å¦‚ä¸‹å‡ ç§æƒ…å½¢å¯¼è‡´éœ€è¦æ›´å¤šçš„å†…å­˜èµ„æº
    - å¦‚æœæŸä¸ªæƒé‡éœ€è¦æ”¾ç½®åœ¨ GPU ä¸Šï¼Œé‚£ä¹ˆè¿™ä»½æƒé‡åœ¨ CPU å’Œ GPU ä¸Šéƒ½æœ‰ä¸€ä»½
    - å¦‚æœæŸä¸ªæƒé‡éœ€è¦çš„ dtype ä¸ load å‡ºæ¥çš„æƒé‡ä¸ç›¸åŒï¼Œé‚£ä¹ˆè¿™ä¸ªæƒé‡éœ€è¦å­˜å‚¨ä¸¤ä»½
    - å¦‚æœæŸä¸ªæƒé‡éœ€è¦çš„ dtype ä¸ load å‡ºæ¥çš„æƒé‡ç›¸åŒï¼Œé‚£ä¹ˆè¿™ä¸ªæƒé‡åªä¼šå ç”¨ä¸€ä»½ç©ºé—´
    
    æ³¨æ„åœ¨å°†æ¨¡å‹çš„æƒé‡åˆå§‹åŒ–å¥½ä¹‹å‰ï¼Œload çš„æƒé‡éƒ½ä¸ä¼šè¢«é‡Šæ”¾
- å°† load çš„æƒé‡ä¸€æ¬¡æ€§é‡Šæ”¾


å½“æ¨¡å‹çš„å‚æ•°æ–‡ä»¶æœ‰å¤šä¸ªæ—¶

- åˆå§‹åŒ–ä¸€ä¸ªç©ºæ¨¡å‹(device="meta")
- å¦‚æœæ¨¡å‹çš„å‚æ•°æ–‡ä»¶æœ‰å¤šä¸ª, åˆ™é€šè¿‡ load `pytorch_model.bin.index.json` æ–‡ä»¶è·å–ä¿å­˜çš„æ¨¡å‹å‚æ•°çš„ key åˆ—è¡¨
- æ ¹æ® device_map, max_memory, torch_dtype ç¡®å®šæ¯ä¸ªå‚æ•°æœ€ç»ˆéœ€è¦å­˜æ”¾çš„è®¾å¤‡
- æ¯æ¬¡ load ä¸€ä¸ª `pytorch_model-0000x-of-0000y.bin` æ–‡ä»¶, åˆå§‹åŒ–æ¨¡å‹çš„ä¸€éƒ¨åˆ†å‚æ•°, ç„¶åé‡Šæ”¾æ‰è¿™ä¸€ä¸ª `pytorch_model-0000x-of-0000y.bin` çš„å‚æ•°æ–‡ä»¶


ä»¥ä¸‹æ˜¯ç®€åŒ–åçš„ä¼ªä»£ç : `low_cpu_mem_usage=True`ï¼Œä¸”ä¸ºå•ä¸ªæƒé‡æ–‡ä»¶çš„æƒ…å½¢

```python
assert model.device == torch.device("meta")
state_dict = torch.load("pytorch_model.bin")
device_map = {
    "layer1.weight": "cuda:0",
    "layer1.bias": "cuda:0",
    "layer2.weight": "cpu",
    "layer2.bias": "cpu",
}
for name, param in state_dict.items():
    # submodule = model.layer1 æˆ– model.layer2
    submodule = get_submodule(model, name)
    tensor_name = name.split(".")[-1]
    with torch.no_grad():
        # æ³¨æ„å¦‚æœnew_valueçš„deviceä¸dtypeä¸paramç›¸åŒæ—¶, ä¸éœ€è¦é¢å¤–å ç”¨å†…å­˜ç©ºé—´
        new_value = torch.tensor(param.to(device=device_map[name], dtype=...))
        if is_buffer:
            submodule._buffers[tensor_name] = new_value
        else:
            submodule._parameters[tensor_name] = new_value
del state_dict
gc.collect()
```

#### _fast_init

è¿™ä¸ªå‚æ•°ä¼¼ä¹ä½œç”¨å¹¶ä¸ç®—å¤ªå¤§

#### device_map, max_memory, torch_dtype

è¿™ä¸¤ä¸ªå‚æ•°ä¸€èµ·å†³å®šæ¯ä¸ªå‚æ•°çš„ device
- `device_map: str/Dict`
- `max_memory: None/Dict`

é€»è¾‘å¦‚ä¸‹: 
- å¦‚æœ `device_map` æœ¬èº«æ˜¯ Dict æ—¶, åˆ™ç›´æ¥ç¡®å®šæ¯ä¸ªå‚æ•°çš„ device
- å¦‚æœ `device_map` å–å€¼ä¸ºå­—ç¬¦ä¸², è€Œ `max_memory=None`, é‚£ä¹ˆå°±ã€å¾…ç¡®è®¤ã€‘å‡å®šæ‰€æœ‰çš„ GPU ä¸ CPU çš„å†…å­˜éƒ½å¯ä»¥ä½¿ç”¨, ç”±æ­¤å¾—åˆ°å­—å…¸å½¢å¼çš„ `max_memory`, ç„¶åå†æ ¹æ® `device_map` çš„å…·ä½“å–å€¼æ¥ç¡®å®šæ¯ä¸ªå‚æ•°çš„ device:
    - `auto/balanced`
    - `balanced_low_0`
    - `sequential`
- å¦‚æœ `max_memory` ä¸º Dict, `device_map` å¿…é¡»å–å€¼ä¸º `"sequential"`

è¿™ä¸ªå‚æ•°ä»¥åŠconfigå†³å®šæ¯ä¸ªå‚æ•°dtype
- `torch_dtype: None/torch.dtype`

### accelerate.big_modeling.init_empty_weights

ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
init_empty_weights(include_buffers: bool = False)
```

### accelerate.utils.modeling.infer_auto_device_mapã€é€»è¾‘æ¯”è¾ƒå¤æ‚, ä¸”æœ‰ä¸€äº›bugã€‘

ä¾èµ–çš„è¾…åŠ© API å¦‚ä¸‹:

- accelerate.utils.modeling.compute_module_sizes
- accelerate.utils.modeling.get_max_layer_size
- accelerate.utils.modeling.find_tied_parameters, retie_parameters


**signature**

```python
device_map: Dict[str, torch.dtype] = infer_auto_device_map(
    model: nn.Module,
    max_memory: Optional[Dict]=None,
    no_split_module_classes: List[str]=None,
    dtype: Optional[Union[torch.dtype, str]]=None,
    special_dtypes: Optional[Dict] = None,
    verbose: bool = False
)
```

**å…¥å‚**
- `model`: åªè¦æ±‚æ˜¯ `nn.Module` å³å¯, å› æ­¤è¿™ä¸ªå‡½æ•°**ä¸ä¼š**å°† `model._no_split_module` è‡ªåŠ¨çº³å…¥ `no_split_module_classes` ä¸­
- `max_memory`: è®¾å®šæ¯ä¸ªè®¾å¤‡æœ€å¤§å¯ä½¿ç”¨çš„CPUå†…å­˜/GPUæ˜¾å­˜
    - Dict:
        ```python
        max_memory={0: "10GB", 1: "20GB", "cpu": "200MB"}
        ```
    - None:
- `no_split_module_classes`: ä¸å¯åˆ†å‰²çš„å­æ¨¡å—å, ä¾‹å¦‚å‡è®¾å®šä¹‰åœ¨ `transformers.models.t5.modeling_t5.py` ä¸­çš„ `T5Block` è¢«è®¤ä¸ºæ˜¯ä¸å¯åˆ†å‰²åˆ°ä¸åŒè®¾å¤‡ä¸Šçš„åŸºæœ¬å•å…ƒ, åˆ™ä¼ å…¥:
    ```python
    no_split_module_classes=["T5Block"]
    ```
    å…·ä½“é€»è¾‘å¯å‚è€ƒ `get_max_layer_size` ä¸­çš„è§£é‡Š
- `dtype`: 
    - None: è¡¨ç¤ºç”¨ model æœ¬èº«çš„æ¯ä¸ªå‚æ•°çš„ dtype è¿›è¡Œè®¡ç®—å†…å­˜å¤§å°
    - torch.float32: å¯¹äº model çš„æ¯ä¸ª tensor, tensor çš„ dtype ä¸ä¼ å…¥çš„ `dtype` çš„æœ€å°å€¼è®¡ç®—å†…å­˜å¤§å°
- `special_dtypes`:
    ```python
    # å­—å…¸çš„ key éœ€è¦åœ¨ model.state_dict().keys() ä¸­, å³å®ƒéœ€è¦ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„å¼ é‡
    special_dtypes = {"feed_forward.layers.0.weight": torch.float16}
    ```
**å‡ºå‚**

- `device_map` æ˜¯ä¸€ä¸ªå­—å…¸, key æ˜¯å­æ¨¡å—åæˆ–è€…æ˜¯æƒé‡å(å½“ key æ˜¯å­æ¨¡å—åæ—¶, è¡¨æ˜æ•´ä¸ªå­æ¨¡å—éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š), value æ˜¯è®¾å¤‡å(0/1/cpu/disk), ä¾‹å­:
    ```python
    device_map = {
        'embed': 0                              # Module, åŒ…å«æƒé‡ embed.weight
        'feed_forward.layers.0': 0,             # Module, åŒ…å«æƒé‡ feed_forward.layers.0.weight ä¸ feed_forward.layers.0.bias
        'feed_forward.layers.1.weight': 0,      # Parameter
        'feed_forward.layers.1.bias': "cpu",    # Parameter
        'feed_forward.layers.2': "cpu",         # Module
        'feed_forward.layers.3': "disk",        # Module
        'head': "disk"                          # Module, åŒ…å« head.out Module
    }
    ```

**ä¾‹å­**

```python
class ModelA(nn.Module):
    def __init__(self):
        super().__init__()
        self.a = nn.Parameter(torch.rand(1000, 1000))
        self.b = nn.Parameter(torch.rand(1000, 1000))
        self.layer = nn.Linear(1000, 1000)
    def forward(self, x):
        pass

print(infer_auto_device_map(ModelA(), max_memory={"cpu": 1000*1000*6}))  # {'': 'disk'}
print(infer_auto_device_map(ModelA(), max_memory={"cpu": 1000*1000*8+3999}))  # {'': 'disk'}
print(infer_auto_device_map(ModelA(), max_memory={"cpu": 1000*1000*8+4000}))  # {'a': 'cpu', 'b': 'disk', 'layer': 'disk'}
print(infer_auto_device_map(ModelA(), max_memory={"cpu": 1000*1000*10}))  # {'a': 'cpu', 'b': 'disk', 'layer': 'disk'}
```

æ³¨æ„è¿™é‡Œç¬¬ä¸€ä¸ªä¾‹å­é‡Œ, æŒ‰é€šå¸¸çš„æƒ³æ³•, è‡³å°‘ `self.a` åº”è¯¥å¯ä»¥æ”¾åœ¨ cpu ä¸Š, ä½†è¿™é‡Œè¦è€ƒè™‘ offload çš„å‚æ•° `self.b` å’Œ `self.layer` åœ¨è¿ç®—æ—¶éœ€è¦é‡æ–° load å› cpu, å› æ­¤å¦‚æœéœ€è¦å°† `self.a` ä¿ç•™åœ¨ cpu ä¸Š, åº”è¯¥è‡³å°‘è¦è¿™ä¹ˆå¤šå†…å­˜:

```python
1000*1000*4 # å­˜æ”¾ self.a
+ max(
    1000*1000*4,  # è¿ç®—æ—¶éœ€è¦ä¸´æ—¶åŠ è½½ self.b
    1000*1000*4 + 1000*4  # è¿ç®—æ—¶éœ€è¦ä¸´æ—¶åŠ è½½ self.layer
)
```

### accelerate.big_modeling.dispatch_model

ä¾èµ–çš„è¾…åŠ© API å¦‚ä¸‹:
- accelerate.utils.modeling.check_device_map
- accelerate.utils.offload.OffloadedWeightsLoader, accelerate.utils.offload.PrefixDataset
- accelerate.hooks.attach_align_device_hook_on_blocks
    - accelerate.hooks.ModelHook, SequentialHook, AlignDeviceHook
    - accelerate.hooks.add_hook_to_module, (remove_hook_from_module)
    - accelerate.hooks.attach_align_device_hook, attach_execution_device_hook


## è¾…åŠ©APIåŠåŠŸèƒ½


### accelerate.utils.modeling.compute_module_sizes

**signature**
```python
module_sizes: Dict[str, int] = compute_module_sizes(
    model: nn.Module,
    dtype: Optional[Union[torch.dtype, str]]=None,
    special_dtypes: Optional[Dict] = None
)
```

**å…¥å‚**
å‚æ•°ç±»å‹ä¸ `infer_auto_device_map` ç›¸åŒ, æ³¨æ„: **æ—¢ç»Ÿè®¡äº† Parameter ä¹Ÿç»Ÿè®¡äº† Buffer**

**å‡ºå‚**
- `module_sizes`: å„å±‚çº§çš„ Module/Parameter/Buffer çš„å†…å­˜å ç”¨å¤§å°, è§ç¤ºä¾‹

**ä¾‹å­**

```python
import torch.nn as nn
import torch

class FeedForward(nn.Module):
    def __init__(self, num_layer, in_dim, hidden_dim):
        super().__init__()
        assert num_layer > 2
        self.num_layer = num_layer
        layers = nn.Sequential()
        layers.append(nn.Linear(in_dim, hidden_dim))
        for i in range(num_layer - 2):
            layers.append(nn.Linear(hidden_dim, hidden_dim))
        layers.append(nn.Linear(hidden_dim, in_dim))
        self.layers = layers
        self.activate = nn.ReLU()
    def forward(self, x):
        x = self.layers(x)
        x = self.activate(x)
        return x
    
class Head(nn.Module):
    def __init__(self, in_dim, num_class):
        super().__init__()
        self.out = nn.Linear(in_dim, num_class)
        self.softmax = nn.Softmax(dim=-1)
    def forward(self, x):
        x = self.out(x)
        probs = self.softmax(x)
        return probs

class ExampleModel(nn.Module):
    def __init__(self, vocab_size, in_dim, num_layer, num_class):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, in_dim)
        self.feed_forward = FeedForward(num_layer, in_dim, 4*in_dim)
        self.head = Head(in_dim, num_class)

    def forward(self, x):
        # x: (B, L), out: (B, L, num_class)
        out = self.embed(x)
        out = self.feed_forward(out)
        out = self.head(out)
        return out
```

```python
from accelerate.utils.modeling import compute_module_sizes
model = ExampleModel(100, 16, 4, 3)
compute_module_sizes(
    model.half(),
    dtype=torch.float32,
    special_dtypes={'feed_forward.layers.0.weight': torch.float32},
)

# è¾“å‡ºç»“æœå¦‚ä¸‹
{
    '': 26246,                              # Module
    'embed': 3200,                          # Module
    'embed.weight': 3200,                   # Parameter
    'feed_forward': 22944,                  # Module
    'feed_forward.layers': 22944,           # Module
    'feed_forward.layers.0': 4224,          # Module
    'feed_forward.layers.0.weight': 4096,   # Parameter: 4byte*16*64=4096byte
    'feed_forward.layers.0.bias': 128,      # Parameter: min(2, 4)=2byte*64=128byte
    'feed_forward.layers.1': 8320,          # Module
    'feed_forward.layers.1.weight': 8192,   # Parameter: min(2, 4)=2byte*64*64=8192byte
    'feed_forward.layers.1.bias': 128,      # Parameter
    'feed_forward.layers.2': 8320,          # Module
    'feed_forward.layers.2.weight': 8192,   # Parameter
    'feed_forward.layers.2.bias': 128,      # Parameter
    'feed_forward.layers.3': 2080,          # Module
    'feed_forward.layers.3.weight': 2048,   # Parameter
    'feed_forward.layers.3.bias': 32,       # Parameter
    'head': 102,                            # Module
    'head.out': 102,                        # Module
    'head.out.weight': 96,                  # Parameter
    'head.out.bias': 6                      # Parameter
}
```

### accelerate.utils.modeling.find_tied_parameters

é¦–å…ˆåŒºåˆ†å‡ ç±»â€œç»‘å®šâ€

- å˜é‡ç»‘å®š: `self.layer2=self.layer1`, è¿™ç§åšæ³•æ˜¯å¯¹åŒä¸€ä»½å†…å­˜ç»‘å®šäº†ä¸¤ä¸ªå®ä¾‹å˜é‡å
    - `state_dict=self.state_dict()`: {"layer1.weight": xx, "layer2.weight": xx}, ä¸¤ä¸ªkeyå¯¹åº”çš„valueæ˜¯ç›¸åŒçš„(å¯¹åº”åŒä¸€ä»½å†…å­˜), åœ¨ä½¿ç”¨ `torch.save(sate_dict)` æ—¶å†…å­˜ä»…å ç”¨ä¸€ä»½
    - `self.modules()`: è¿­ä»£å™¨ä¼šå»é‡, **åªè¿­ä»£å‡ºä¸€ä¸ª**
    - `layer1.weight.grad` ç­‰å…³äºæ¢¯åº¦çš„æ“ä½œä¼šåŒæ—¶ä½œç”¨äºä¸¤è€…, å› æ­¤å¯ä»¥ä¸€ç›´ä¿æŒä¸€è‡´
    - `layer1.weight is layer2.weight`: True
    - `find_tied_parameters`: **æ— æ³•æ£€æµ‹**å‡ºè¿™ç§æƒ…å½¢, è¿”å›å€¼ä¸ºç©ºåˆ—è¡¨
- tied Parameter (**ğŸ¤— Transformer ä¸­çš„ tie_weight æ˜¯æŒ‡è¿™ç§æƒ…å½¢**): `self.layer2.weight=self.layer1.weight`
    - `state_dict=self.state_dict()`: {"layer1.weight": xx, "layer2.weight": xx}, ä¸¤ä¸ªkeyå¯¹åº”çš„valueæ˜¯ç›¸åŒçš„(å¯¹åº”åŒä¸€ä»½å†…å­˜), åœ¨ä½¿ç”¨ `torch.save(sate_dict)` æ—¶å†…å­˜ä»…å ç”¨ä¸€ä»½
    - `self.modules()`: è¿­ä»£å™¨ä¼šå»é‡, ä½†layer1ä¸layer2æ˜¯ä¸åŒçš„, **è¿­ä»£å‡ºä¸¤ä¸ª**
    - `layer1.weight.grad` ç­‰å…³äºæ¢¯åº¦çš„æ“ä½œä¼šåŒæ—¶ä½œç”¨äºä¸¤è€…, å› æ­¤å¯ä»¥ä¸€ç›´ä¿æŒä¸€è‡´
    - `layer1.weight is layer2.weight`: True
    - `find_tied_parameters`: **å¯ä»¥æ£€æµ‹**å‡ºè¿™ç§æƒ…å½¢, è¿”å›å€¼ä¸º `[["layer1.weight", "layer2.weight"]]`
- copy Parameter data ã€è¿™ä¸¤ç§ä¼¼ä¹æ˜¯å®Œå…¨çš„è¡¨ç°,æœ‰äº›å¥‡æ€ªã€‘:
    - `layer2.weight.data=layer1.weight.data.clone()`
    - `layer2.weight.data=layer1.weight.data`

    ä¾‹å­ã€å¾…ç ”ç©¶æ¸…æ¥šã€‘
    ```python
    layer1 = nn.Linear(3, 3)
    layer2 = nn.Linear(3, 3)
    with torch.no_grad():
        layer2.weight.data = layer1.weight.data#.clone()
        layer2.bias.data = layer1.bias.data#.clone()
    import itertools
    opt=torch.optim.SGD(itertools.chain(layer1.parameters(), layer2.parameters()), lr=0.001)
    
    # clone: False, ä¸ä½¿ç”¨ cloneï¼šFalse
    layer2.weight.data is layer1.weight.data
    # å¤šæ¬¡è¿è¡Œ, è¿™ä¸¤ä¸ªå€¼çš„idä¸€ç›´åœ¨å˜, ä½†æ— è®ºä½¿ç”¨cloneè¿˜æ˜¯ä¸ä½¿ç”¨, ä¸¤è€…çš„å€¼ç›¸åŒ
    id(layer2.weight.data), id(layer1.weight.data)
    
    x = torch.rand(4, 3)
    z = layer2(layer1(x)).sum()
    y = torch.rand(())
    loss = z - y
    loss.backward()

    # clone: ä¸åŒ, ä¸ä½¿ç”¨ cloneï¼šç›¸åŒ
    layer1.weight.grad, layer2.weight.grad

    opt.step()

    # clone: ä¸åŒ, ä¸ä½¿ç”¨ clone: ç›¸åŒ
    layer1.weight, layer2.weight
    ```

**signature**
```python
tied_names: List[List[str]] = find_tied_parameters(model: nn.Module)
```

**å…¥å‚**

**å‡ºå‚**

- `tied_names`:
    ```python
    tied_names=[
        ["layer1.weight", "layer2.weight", "layer3.weight"],  # è¿™ä¸‰ä¸ªweightè¢«tiedäº†
        ["layer1.bias", "layer2.bias", "layer3.bias"] # è¿™ä¸‰ä¸ªbiasè¢«tiedäº†
    ]
    ```

**å†…éƒ¨é€»è¾‘**

æœ¬è´¨ä¸Š, æ˜¯ä½¿ç”¨ `nn.Module.named_parameters()`, ç„¶åä¸¤ä¸¤ä½¿ç”¨ `parameter1 is parameter 2` åˆ¤æ–­æ˜¯å¦ä¸º tied_weight. å› æ­¤å®Œå…¨ç›¸åŒçš„ submodule ç”±äº `nn.Module.named_parameters()` ä¼šè¢«é¢„å…ˆæ’é™¤æ‰, å› æ­¤ä¸ä¼šå‡ºç°åœ¨ç»“æœé‡Œã€‚

**ä¾‹å­**

```python
import torch.nn as nn
import torch
from accelerate.utils.modeling import find_tied_parameters
class ModelA(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(3, 3)
        self.layer2 = self.layer1
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        return out
    
class ModelB(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(3, 3)
        self.layer2 = nn.Linear(3, 3)
        self.layer2.weight = self.layer1.weight
        self.layer2.bias = self.layer1.bias
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        return out

model_a = ModelA()
model_b = ModelB()
# è¿™é‡Œè°ƒç”¨çš„æ˜¯ nn.Module çš„æ–¹æ³•, å®ƒä¼šå»é™¤é‡å¤çš„module, å› æ­¤åªä¼šæ‰“å°å‡º ModelA å’Œ ModelA.layer1
for module in model_a.modules():
    print(module)
find_tied_parameters(model_a)  # è¿”å›ä¸ºç©ºåˆ—è¡¨!!!

# è¿™é‡Œè°ƒç”¨çš„æ˜¯ nn.Module çš„æ–¹æ³•, å› ä¸ºlayer1å’Œlayer2æ˜¯åˆ†åˆ«å®šä¹‰çš„, åªæ˜¯å‚æ•°è¢«ç»‘å®š
for module in model_b.modules():
    print(module)
find_tied_parameters(model_a)  # è¿”å›ä¸º[['layer1.weight', 'layer2.weight'], ['layer1.bias', 'layer2.bias']]
```


### accelerate.utils.modeling.get_max_layer_size


**å†…éƒ¨é€»è¾‘**

é€’å½’éå†æ•´ä¸ªæ¨¡å‹, ç›´è‡³æœ€å°ä¸å¯åˆ†å‰²çš„ Module æˆ–è€…å•ç‹¬çš„ Parameter/Buffer, è®¡ç®—è¿™äº›ä¸å¯åˆ†å‰²çš„éƒ¨åˆ†æ‰€éœ€è¦çš„å†…å­˜çš„æœ€å¤§å€¼, åˆ¤æ–­ä¸å¯åˆ†å‰²çš„ Module çš„ä¾æ®ä¸º:

```python
module.__class__.__name__ in no_split_module_classes
# å‡è®¾ module æ˜¯ä¸€ä¸ª torch.nn.modules.linear.Linear å±‚
# module.__class__.__name__: "Linear"
# no_split_module_classes: ["Linear", "T5Block"]
```

### accelerate.utils.modeling.check_device_map

**signature**
```python
check_device_map(model: nn.Module, device_map)
```

**è¾“å…¥**

ç•¥

**è¾“å‡º**

æ— , åªå¯èƒ½ raise é”™è¯¯ä¿¡æ¯

**å†…éƒ¨é€»è¾‘**

æ£€æŸ¥ `device_map` æ˜¯å¦ä¼šè¦†ç›–ä½ `model` çš„æ‰€æœ‰å‚æ•°, æ³¨æ„: è¿™ä¸ªå‡½æ•°**ä¸æ£€æŸ¥** model å½“å‰å®é™…çš„å‚æ•°ä½ç½®æ˜¯å¦ä¸ device_map ä¸€è‡´ã€‚å†…éƒ¨çš„å®ç°é€»è¾‘æ˜¯å…ˆè·å– `all_names = model.state_dict().keys()`, ç„¶åå¯¹äº `device_map` ä¸­çš„æ¯ä¸€ä¸ª key, éƒ½åœ¨ `all_names` å°†å…¶æ’é™¤(æ’é™¤æ‰ key ä»¥åŠä»¥ `f"{key}."` å¼€å¤´çš„é”®), æœ€åæ–­è¨€ `all_names` ä¸ºç©ºåˆ—è¡¨ã€‚

å¤‡æ³¨: è¿™ä¸ªæ£€æŸ¥å…¶å®æœ‰ä¸€å®šçš„ç¼ºé™·ï¼Œä¾‹å¦‚ `device_map` æœ¬èº«æœ‰å¯èƒ½â€œä¸åˆæ³•â€ï¼Œä¾‹å¦‚ï¼š
```python
device_map = {"fc": 0, "fc.weight": 0, "fc.bias": "cpu"}
```

### accelerate.utils.offload.OffloadedWeightsLoader

```python
from collections.abc import Mapping
class OffloadedWeightsLoader(Mapping):
    def __init__(self,
        state_dict,
        save_folder=None,
        index=None,
        device=None
    ):
        self.all_keys = ... # list(state_dict.keys()) + index ä¸­çš„é”®
        self.device = device
        ...
    
    def __getitem__(self, key):
        if key in self.state_dict:
            return self.state_dict[key]
        else:
            weight = np.memmap(weight_file, dtype, shape, mode="r")
            weight = torch.tensor(weight)
            return weight
    
    ...
```

è¯´æ˜: ä» `dispatch_model` ä¸­å¯¹è¿™ä¸ªå‡½æ•°çš„ä½¿ç”¨é‡Œ:

- å½“ main_device ä¸º gpu æ—¶, å®ä¾‹åŒ–å‚æ•° state_dict ä¸ºæ‰€æœ‰åœ¨ cpu ä¸Šçš„å‚æ•°, save_folder ä¸º offload æ–‡ä»¶å¤¹, device ä¸º None
- å½“ main_device ä¸º cpu æ—¶, å®ä¾‹åŒ–å‚æ•° state_dict ä¸º None, save_folder ä¸º offload æ–‡ä»¶å¤¹, device ä¸º None

`__getitem__` æ–¹æ³•:
- å½“ main_device ä¸º gpu æ—¶, æ— è®ºåŸæœ¬çš„å‚æ•° offload åˆ° disk è¿˜æ˜¯æœ¬å°±åœ¨ cpu ä¸Š, éƒ½ä¼šå°† tensor è½¬ç§»åˆ° cpu ä¸Šè¿”å›, å·²ç¡®è®¤ï¼ŒåŸæœ¬åœ¨ disk ä¸Šçš„å‚æ•°ï¼Œè¿”å›çš„tensorå ç”¨å†…å­˜
- å½“ main_device ä¸º cpu æ—¶, åŸæœ¬ offload åˆ° disk ä¸Šçš„å‚æ•°ä¼šè½¬ç§»åˆ° cpu ä¸Šè¿”å›, å·²ç¡®è®¤: è¿”å›çš„tensorå ç”¨å†…å­˜

```python
# éªŒè¯ä»£ç 
import numpy as np
import psutil
import torch
nrows, ncols = 100000, 1000  # éœ€è¦å ç”¨å¤§çº¦ 400MB

def show_memory():
    mem = psutil.virtual_memory()
    print("Used", mem.used / 1024 / 1024, "MB")
    print("Free", mem.free / 1024 / 1024, "MB")
show_memory()                # Used: 602MB, Free: 2586MB
# f = np.memmap('memmapped.dat', dtype=np.float32, mode='w+', shape=(nrows, ncols))
# for i in range(nrows):
#     f[i, :] = np.random.rand(ncols)
# f.flush()
# del f
# show_memory()

f = np.memmap('memmapped.dat', dtype=np.float32, mode='r', shape=(nrows, ncols))
show_memory()                # Used: 602MB, Free: 2586MB
tensor = torch.tensor(f)
show_memory()                # Used: 985MB, Free: 2204MB, è¯´æ˜æ„å»º tensor éœ€è¦å ç”¨å¤§é‡å†…å­˜
```

### accelerte.hooks.attach_align_device_hook_on_blocksã€é‡Œé¢çš„é€’å½’é€»è¾‘åé¢å†è¡¥å……ã€‘

**signature**
```python
attach_align_device_hook_on_blocks(
    module: nn.Module,
    execution_device: Optional[Union[torch.device, Dict[str, torch.device]]] = None,
    offload: Union[bool, Dict[str, bool]] = False,
    weights_map: Mapping = None,
    offload_buffers: bool = False,
    module_name: str = "",
    skip_keys: Optional[Union[str, List[str]]] = None,
    preload_module_classes: Optional[List[str]] = None,
)  # ç»™ module åŠ ä¸Š hook, æ­¤å‡½æ•°æœ¬èº«æ— è¿”å›
```

`PretrainedModel.from_pretrain` ä¸­è°ƒç”¨ `dispath_model` æ–¹æ³•, åœ¨å†…éƒ¨è°ƒç”¨ `attach_align_device_hook_on_blocks` æ‰€ä¼ å…¥çš„å‚æ•°æœ‰å¦‚ä¸‹è¯´æ˜ï¼š

- main_device ä¸º gpu æ—¶, å‡è®¾ `device_map` å¦‚ä¸‹
    ```python
    device_map = {
        'embed': 0                              # Module, åŒ…å«æƒé‡ embed.weight
        'feed_forward.layers.0': 0,             # Module, åŒ…å«æƒé‡ feed_forward.layers.0.weight ä¸ feed_forward.layers.0.bias
        'feed_forward.layers.1.weight': 1,      # Parameter
        'feed_forward.layers.1.bias': "cpu",    # Parameter
        'feed_forward.layers.2': "cpu",         # Module
        'feed_forward.layers.3': "disk",        # Module
        'head': "disk"                          # Module, åŒ…å« head.out Module
    }
    ```
    offload_buffersã€module_nameã€skip_keysã€preload_module_classes é€šå¸¸ä¸ºé»˜è®¤å€¼, å‰©ä½™å…¶ä»–å‚æ•°ä¸º:
    ```python
    zip(execution_device, offload) = {
        "": 0,                                  # è¿™ä¸ªæ˜¯ dispath å‡½æ•°åœ¨è°ƒç”¨ attach_align_device_hook_on_blocks ä¹‹å‰è¿½åŠ çš„ä¸€ä¸ªé”®
        'embed': (0, False)                         # Module, åŒ…å«æƒé‡ embed.weight
        'feed_forward.layers.0': (0, False),        # Module, åŒ…å«æƒé‡ feed_forward.layers.0.weight ä¸ feed_forward.layers.0.bias
        'feed_forward.layers.1.weight': (1, False), # Parameter
        # weight_map åŒ…å«ä»¥ä¸‹æ¨¡å—é‡Œçš„æ‰€æœ‰ tensor, æ³¨æ„ weight_map çš„é”®æ˜¯ tensor name
        'feed_forward.layers.1.bias': (0, True),    # Parameter, åŸæœ¬device_mapåœ¨cpuä¸Š, execution_device å°†å…¶æ˜ å°„åˆ°main_deviceä¸Š
        'feed_forward.layers.2': (0, True),         # Module
        'feed_forward.layers.3': (0, True),         # Module
        'head': (0, True)                           # Module, åŸæœ¬device_mapåœ¨diskä¸Š, execution_device å°†å…¶æ˜ å°„åˆ°main_deviceä¸Š
    }
    weights_map = OffloadedWeightsLoader(cpu_state_dict, offload_folder)
    ```

- main_device ä¸º cpu æ—¶, å‡è®¾ `device_map` å¦‚ä¸‹
    ```python
    device_map = {
        'embed': "cpu"                         # Module, åŒ…å«æƒé‡ embed.weight
        'feed_forward.layers.0': "cpu",        # Module, åŒ…å«æƒé‡ feed_forward.layers.0.weight ä¸ feed_forward.layers.0.bias
        'feed_forward.layers.1.weight': "cpu", # Parameter
        'feed_forward.layers.1.bias': "cpu",   # Parameter
        'feed_forward.layers.2': "cpu",        # Module
        'feed_forward.layers.3': "disk",       # Module
        'head': "disk"                         # Module, åŒ…å« head.out Module
    }
    ```
    offload_buffersã€module_nameã€skip_keysã€preload_module_classes é€šå¸¸ä¸ºé»˜è®¤å€¼, å‰©ä½™å…¶ä»–å‚æ•°ä¸º:
    ```python
    zip(execution_device, offload) = {
        "": "cpu",                                  # è¿™ä¸ªæ˜¯ dispath å‡½æ•°åœ¨è°ƒç”¨ attach_align_device_hook_on_blocks ä¹‹å‰è¿½åŠ çš„ä¸€ä¸ªé”®
        'embed': ("cpu", False)                         # Module, åŒ…å«æƒé‡ embed.weight
        'feed_forward.layers.0': ("cpu", False),        # Module, åŒ…å«æƒé‡ feed_forward.layers.0.weight ä¸ feed_forward.layers.0.bias
        'feed_forward.layers.1.weight': ("cpu", False), # Parameter
        'feed_forward.layers.1.bias': ("cpu", True),    # Parameter, åŸæœ¬device_mapåœ¨cpuä¸Š, execution_device å°†å…¶æ˜ å°„åˆ°main_deviceä¸Š
        # weight_map åŒ…å«ä»¥ä¸‹æ¨¡å—é‡Œçš„æ‰€æœ‰ tensor, æ³¨æ„ weight_map çš„é”®æ˜¯ tensor name
        'feed_forward.layers.2': ("cpu", True),         # Module
        'feed_forward.layers.3': ("cpu", True),         # Module
        'head': ("cpu", True)                           # Module, åŸæœ¬device_mapåœ¨diskä¸Š, execution_device å°†å…¶æ˜ å°„åˆ°main_deviceä¸Š
    }
    weights_map = OffloadedWeightsLoader(offload_folder)
    ```

### `torch.nn.Module` çš„ hook æœºåˆ¶ä¸ `torch.nn.Module.__call__`


[torch.nn.Module æºç ](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html), ç”±äºè¿™é‡Œæˆ‘ä»¬åªå…³æ³¨ forward hook, æ‰€ä»¥ç®€åŒ–å¦‚ä¸‹:

```python
class Module:
    def __call__(self, *args, **kwargs):
        for hook_id, hook in (*_global_forward_pre_hooks.items(), *self._forward_pre_hooks.items()):
            if hook_id in self._forward_pre_hooks_with_kwargs:
                result = hook(self, args, kwargs)  # type: ignore[misc]
                if result is not None:
                    args, kwargs = result
            else:
                result = hook(self, args)
                if result is not None:
                    if not isinstance(result, tuple):
                        result = (result,)
                    args = result
        result = self.forward(*args, **kwargs)
        for hook_id, hook in (*_global_forward_hooks.items(), *self._forward_hooks.items()):
            if hook_id in self._forward_hooks_with_kwargs:
                hook_result = hook(self, args, kwargs, result)
            else:
                hook_result = hook(self, args, result)

            if hook_result is not None:
                result = hook_result
        return result
```

ç®€è¨€ä¹‹, å¯¹äº `torch.nn.Module` çš„ä¸¤ç±» forward hook è€Œè¨€, æœ‰å¦‚ä¸‹è¯´æ˜, å‡è®¾ `result=forward(*args, **kwargs)`

- åªæœ‰åœ¨è°ƒç”¨ `__call__` æ—¶, hook æ‰ä¼šèµ·ä½œç”¨, ç›´æ¥è°ƒç”¨ `forward` hook ä¸ä¼šèµ·ä½œç”¨
- forward_pre_hook çš„å‡ºå…¥å‚å¯ä»¥æ˜¯å¦‚ä¸‹ï¼ˆæ³¨æ„ `args` å’Œ `kwargs` **å¯èƒ½ä¼šè¢«ä¿®æ”¹**ï¼‰:
  - å…¥å‚: **é™¤å»moduleæœ¬èº«å¤–**, åªæ¥æ”¶ä¸€ä¸ªå…ƒç»„å½¢å¼çš„å‚æ•° `args`, å‡ºå‚å¯ä»¥æ˜¯ None, tuple, å…¶ä»–(æœ€ç»ˆè½¬åŒ–ä¸ºå•ä¸ªå…ƒç´ çš„tuple), å¦‚æœå‡ºå‚ä¸º tuple, åˆ™ä¸‹ä¸€ä¸ª hook çš„å…¥å‚å°†ä½¿ç”¨å½“å‰ hook çš„å‡ºå‚
  - å…¥å‚: **é™¤å»moduleæœ¬èº«å¤–**, æ¥æ”¶å…ƒç»„å½¢å¼çš„å‚æ•° `args` å’Œå­—å…¸å½¢å¼å‚æ•° `kwargs`, å‡ºå‚å¯ä»¥æ˜¯ None, (tuple, dict), å¦‚æœå‡ºå‚ä¸º (tuple, dict), åˆ™ä¸‹ä¸€ä¸ª hook çš„å…¥å‚å°†ä½¿ç”¨å½“å‰ hook çš„å‡ºå‚
- forward_hook çš„å‡ºå…¥å‚å¯ä»¥æ˜¯å¦‚ä¸‹ï¼ˆæ³¨æ„ `args` å’Œ `kwargs` **ä¸ä¼šè¢«ä¿®æ”¹**ï¼‰:
  - å…¥å‚: **é™¤å»moduleæœ¬èº«å¤–**, ä¸€ä¸ªå…ƒç»„å½¢å¼çš„å‚æ•° `args` å’Œ `result`, å‡ºå‚å¯ä»¥æ˜¯ None, å…¶ä»–, å¦‚æœå‡ºå‚ä¸ä¸º None, åˆ™ä¸‹ä¸€ä¸ª hook æ‰€ä½¿ç”¨çš„ `result` å…¥å‚å°†ä½¿ç”¨å½“å‰ hook çš„å‡ºå‚
  - å…¥å‚: **é™¤å»moduleæœ¬èº«å¤–**, ä¸€ä¸ªå…ƒç»„å½¢å¼çš„å‚æ•° `args`ã€å­—å…¸å½¢å¼å‚æ•° `kwargs` å’Œ `result`, å‡ºå‚å¯ä»¥æ˜¯ None, å…¶ä»–, å¦‚æœå‡ºå‚ä¸ä¸º None, åˆ™ä¸‹ä¸€ä¸ª hook æ‰€ä½¿ç”¨çš„ `result` å…¥å‚å°†ä½¿ç”¨å½“å‰ hook çš„å‡ºå‚


æ³¨å†Œ hook çš„æ–¹å¼ä¸º:
```python
# æ³¨æ„: è€ç‰ˆæœ¬çš„pytorch(ä¾‹å¦‚torch==1.12.0), æ²¡æœ‰with_kwargså‚æ•°, å³ä¸èƒ½ä½¿ç”¨å…³é”®å­—å‚æ•°
register_forward_pre_hook(
    self,
    hook: Union[
        Callable[[T, Tuple[Any, ...]], Optional[Any]],
        Callable[[T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]],
    ],
    *,
    prepend: bool = False,  # å¦‚æœä¸º True, åˆ™æŠŠä¼˜å…ˆçº§æåˆ°æœ€é«˜, æœ€å…ˆæ‰§è¡Œ
    with_kwargs: bool = False  # hookçš„å…¥å‚æ˜¯å¦æœ‰å…³é”®å­—å‚æ•°
)

def register_forward_hook(
    self,
    hook: Union[
        Callable[[T, Tuple[Any, ...], Any], Optional[Any]],
        Callable[[T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]],
    ],
    *,
    prepend: bool = False,
    with_kwargs: bool = False,
)
```

ç¤ºä¾‹

```python
# è¿™ä¸ªä¾‹å­é€‚åˆæ–°è€ç‰ˆæœ¬torch
import torch.nn as nn
import torch
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(20, 10)
        self.act = nn.ReLU()
        self.layer2 = nn.Linear(10, 5)
    def forward(self, x):
        x = self.layer1(x)
        x = self.act(x)
        x = self.layer2(x)
        return x
model = MLP()
for name, module in model.named_modules():
    module.register_forward_pre_hook(
        lambda self, args: print(self.__class__.__name__, len(args), type(args), args[0].shape),
        )
x = torch.rand((4, 20))
y = model(x)
# è¾“å‡º:
# MLP 1 <class 'tuple'> torch.Size([4, 20])
# Linear 1 <class 'tuple'> torch.Size([4, 20])
# ReLU 1 <class 'tuple'> torch.Size([4, 10])
# Linear 1 <class 'tuple'> torch.Size([4, 10])
```


### accelerate.hooks.ModelHook, SequentialHook


### accelerate.hooks.AlignDevicesHook


```python
class AlignDevicesHook:
    def __init__(
        self,
        execution_device: Optional[Union[int, str, torch.device]] = None,
        offload: bool = False,
        io_same_device: bool = False,
        weights_map: Optional[Mapping] = None,  # é€šå¸¸æƒ…å†µä¼šæŒ‡å®š
        offload_buffers: bool = False,
        place_submodules: bool = False,
        # skip_keys: Optional[Union[str, List[str]]] = None,  # accelerate 0.20 å¢åŠ çš„å‚æ•°, ä¸ºç®€å•èµ·è§å¿½ç•¥
    ):

    def init_hook(self, module):
        pass
```

é¦–å…ˆé‡å¤ä¸€ä¸‹ `OffloadedWeightsLoader` çš„å†…å®¹: `self.weights_map` åªåŒ…å«éœ€è¦ offload çš„å‚æ•°: å¦‚æœ main_device ä¸º gpu, `self.weights_map` å‚æ•°æ—¢åŒ…å« cpu ä¸Šçš„å‚æ•°, ä¹ŸåŒ…å« disk ä¸Šçš„å‚æ•°; å¦‚æœ main_device ä¸º cpu, åˆ™ `self.device_map` åªåŒ…å« disk ä¸Šçš„å‚æ•°. åœ¨æ‰§è¡Œ `self.weights_map["tensor_name"]` æ—¶, å¦‚æœåŸæœ¬çš„å‚æ•°åœ¨ cpu ä¸Šä¿å­˜, åˆ™ç›´æ¥å–å‡ºè¿›è¡Œè¿”å›, å¦‚æœåŸæœ¬çš„å‚æ•°æ˜¯ä½¿ç”¨ `np.memmap` ä¿å­˜åœ¨ disk ä¸Š, åˆ™é€šè¿‡ `np.memmap` load å› cpu, ç„¶åå†è½¬åŒ–ä¸º cpu ä¸Šçš„ tensor è¿›è¡Œè¿”å›, æ€»ä¹‹è¿”å›çš„æ€»æ˜¯ cpu ä¸Šçš„ tensor.


è¦ç†è§£å…¶ä½™å‚æ•°çš„å«ä¹‰, è¿™é‡Œå…ˆå¯¹ `nn.Module` ä¸­çš„â€œtensorâ€è¿›è¡Œæ¾„æ¸…ï¼š

```python
class SubModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.c = nn.Parameter(torch.rand(40, 40))
        # è¿™ç§è¡¨ç¤ºä¸æŒä¹…åŒ–ä¿å­˜çš„buffer, ä½¿ç”¨é€šå¸¸torch.save(model.state_dict(), "x.pth")å°†ä¸ä¼šä¿å­˜è¿™éƒ¨åˆ†buffer
        self.register_buffer("d", torch.ones(10, 10), persistent=False)

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.a = nn.Parameter(torch.rand(100, 100))
        self.register_buffer("b", torch.rand(50, 50))
        self.layer = SubModule()

model = Model()
model.state_dict()                          # åŒ…å« a, b, layer.c
list(model.named_parameters())              # åŒ…å« a, layer.c
list(model.named_parameters(recurce=False)) # åŒ…å« a
list(model.named_buffers())                 # åŒ…å« b, layer.d
list(model.named_buffers(recurse=False))    # åŒ…å« b
```

æ€»çš„æ¥è¯´, ä¸€ä¸ªmoduleçš„å‚æ•°åŒ…å«å¦‚ä¸‹å‡ ç±»(åé¢çš„æè¿°çº¦å®šé‡‡ç”¨è¿™äº›æœ¯è¯­)

- parameter
- buffer
- child

**init_hook**

`init_hook` çš„å…¥å‚ `module` åº”è¯¥ä¸ `self.weights_map`ã€`self.offload_buffers`ã€`self.offload`ã€`self.place_submodules` å‚æ•°åŒ¹é… (æ„Ÿè§‰è¿™é‡Œçš„APIè®¾è®¡ç¡®å®ä¸å¤ªåˆç†), ä¹Ÿå°±æ˜¯è¯´ `module` é‡ŒåŒ…å«ä¸å­˜æ”¾åœ¨ meta device ä¸Šçš„å‚æ•°ä¸ `self.weights_map` åˆå¹¶èµ·æ¥åº”è¯¥è¦åŒ…å«å…¨éƒ¨çš„å‚æ•°:

`self.offload=False`: åˆ™å°† parameter/buffer ç§»åŠ¨åˆ° `self.execution_device` ä¸Š, å¦‚æœ `self.place_submodules=True`, åˆ™é€’å½’å°† child çš„ parameter/buffer ä¹Ÿç§»åŠ¨åˆ° `self.execution_device` ä¸Š

`self.offload=True`: `self.place_submodules=True` è¡¨ç¤ºå¦‚ä¸‹æ“ä½œä¹Ÿå¯¹ child é€’å½’è¿›è¡Œ
    - å¯¹bufferçš„å¤„ç†: å¦‚æœ `self.offload_buffers=True`, åˆ™è¡¨æ˜ buffer ä¹Ÿè¢«åŒ…å«åœ¨ `self.device_map` é‡Œ, åŒæ—¶æ„å‘³ç€ `module` é‡Œçš„ buffer åœ¨ meta device ä¸Š, æ­¤æ—¶å°† `module` çš„ buffer ç§»è‡³ meta device ä¸Š; å¦‚æœ `self.offload_buffers=False`, åˆ™è¡¨æ˜ buffer ä¸è¢«åŒ…å«åœ¨ `self.device_map` é‡Œ, åŒæ—¶æ„å‘³ç€ `module` é‡Œçš„ buffer å­˜æ”¾äº†å®é™…æ•°æ®, æ­¤æ—¶å°† `module` çš„ buffer ç§»è‡³ `self.execution_device` ä¸Š
    - å¯¹parameterçš„å¤„ç†: å°† `module` çš„ parameter ç§»è‡³ meta device ä¸Š

**pre_forward/post_forward**

`self.io_same_device=True`: å°†å…¥å‚å…ˆè½¬ç§»åˆ° `self.execution_device` ä¸Š, ç„¶åé€’å½’/ä¸é€’å½’åœ°å°† `module` çš„ parameter/buffer å…¨éƒ¨è½¬ç§»åˆ° `self.execution_device`, ç„¶åæ­£å¸¸è¿›è¡Œ `forward`, ç„¶åå°†æ‰€æœ‰éœ€è¦ offload çš„å‚æ•°å…¨éƒ¨è½¬ç§»åˆ° meta device ä¸Š, æœ€åå°†å‡ºå‚è½¬ç§»åˆ°è·Ÿå…¥å‚ç›¸åŒçš„ device ä¸Š

`self.io_same_device=False`: å°†å…¥å‚å…ˆè½¬ç§»åˆ° `self.execution_device` ä¸Š, ç„¶åé€’å½’/ä¸é€’å½’åœ°å°† `module` çš„ parameter/buffer å…¨éƒ¨è½¬ç§»åˆ° `self.execution_device`, ç„¶åæ­£å¸¸è¿›è¡Œ `forward`, ç„¶åå°†æ‰€æœ‰éœ€è¦ offload çš„å‚æ•°å…¨éƒ¨è½¬ç§»åˆ° meta device ä¸Š, æœ€åå‡ºå‚ä¿ç•™åœ¨ `self.execution_device` ä¸Š

**detach_hook**

å°† `module` çš„å‚æ•°çš„ device å…¨éƒ¨æ¢å¤è‡³ `init_hook` ä¹‹å‰çš„çŠ¶æ€


### accelerate.hooks.add_hook_to_module, remove_hook_from_module

ä½¿ç”¨ accelerate çš„ hook æœºåˆ¶, ä¼šæ“çºµåŸæœ¬çš„ `nn.Module` çš„ä¸€äº›å±æ€§/æ–¹æ³•

- `model._hf_hook: ModelHook`
- `model._old_forward: Callable`
- `model.forward`

### accelerate.hooks.attach_align_device_hook, attach_execution_device_hook


## ç®€åŒ–ç‰ˆå®ç°

```python
def from_pretrain(cls, pretrained_name_or_path, device_map: Dict):
    # åªè€ƒè™‘device_mapå«æœ‰gpu,cpu,disk, ä¸”æ¨¡å‹æ–‡ä»¶ä¸ºshardçš„æƒ…å½¢, å¹¶å‡å®šloadçš„å‚æ•°çš„keyä¸æ¨¡å‹å®Œå…¨åŒ¹é…
    
    config = ...
    # step 1: åˆå§‹åŒ–ç©ºæ¨¡å‹æ–‡ä»¶
    with init_empty_weights():
        model = cls(config, ...)
    # step 2: é€ä¸ªload, å¹¶åˆå§‹åŒ–æ¨¡å‹æ–‡ä»¶
    ...
    # æ­¤æ—¶, æ¨¡å‹çš„æƒé‡çš„device, ä¸€éƒ¨åˆ†åœ¨gpuä¸Š, ä¸€éƒ¨åˆ†åœ¨cpuä¸Š, ä¸€éƒ¨åˆ†ä»ä¸º meta
    # step 3:
    model.tie_weights()
    model.eval()
    # step 4: dispatch (add hook)
```

## å…¶ä»–æœ‰ç”¨çš„ API

### `accelerate.big_modeling.cpu_offload_with_hook`, `accelerate.hooks.CpuOffload`, `accelerate.hooks.UserCpuOffloadHook`

æ³¨æ„å‰é¢æåˆ°çš„æ‰®æ¼”é‡è¦è§’è‰²çš„ `accelerate.hooks.AlignDevicesHook`, å½“å®ƒä½œä¸ºæŸä¸ª `nn.Module` çš„ hook æ—¶, å‡è®¾å®ƒåœ¨ä¸æ‰§è¡Œ `forward` å‡½æ•°æ—¶, æƒé‡å­˜å‚¨åœ¨ CPU ä¸Š (è¿™ç§æƒ…å†µç§°ä¸º CPU offload). åœ¨æ‰§è¡Œ `forward` å‡½æ•°ä¹‹å‰, hook è¢«è§¦å‘, å°†æ‰€æœ‰çš„å…¥å‚åŠå…¶æœ¬èº«çš„æƒé‡å‚æ•°éƒ½è¢«è½¬ç§»åˆ°äº† `execution_device` ä¸Š (é€šå¸¸æ˜¯æŸä¸€å— GPU ä¸Š), è€Œåœ¨æ‰§è¡Œå®Œ `forward` å, hook ä¼šå†æ¬¡è¢«è§¦å‘, å°†å…¶æœ¬èº«çš„æƒé‡å…¨éƒ¨è½¬ç§»åˆ° CPU ä¸Š (æ ¹æ®é…ç½®çš„ä¸åŒ, å‡ºå‚ä¹Ÿè®¸ä¼šè¢«è½¬ç§»åˆ°ä¸å…¥å‚ç›¸åŒçš„è®¾å¤‡ä¸Š).

ä½†åœ¨æŸäº›ç‰¹æ®Šæƒ…å†µä¸‹, ä¸Šè¿°æ‰§è¡Œé€»è¾‘æ•ˆç‡æ¯”è¾ƒä½, ä¾‹å¦‚:

```python
# module_a, module_b å‡è¢«åŠ ä¸Šäº† AlignDevicesHook, åœ¨ä¸è¿è¡Œæ—¶, æƒé‡ä¿å­˜åœ¨ CPU ä¸Š, è€Œè¿è¡Œè®¾å¤‡ä¸º GPU
for i in range(3):
    x = module_a(x)
for i in range(4):
    x = module_b(x)
```

åœ¨ä¸Šè¿°æƒ…å†µä¸‹, `module_a` æœ¬èº«çš„æƒé‡ä¼šç»å† 3 æ¬¡ CPU åˆ° GPU çš„æ‹·è´ä»¥åŠ 3 æ¬¡ GPU åˆ° CPU çš„æ‹·è´, `module_b` æœ¬èº«çš„æƒé‡ä¼šç»å† 4 æ¬¡ CPU åˆ° GPU çš„æ‹·è´ä»¥åŠ 4 æ¬¡ GPU åˆ° CPU çš„æ‹·è´, ç„¶è€Œæ›´ä¸ºä¼˜ç§€çš„ç­–ç•¥æ˜¯, åœ¨è¿›å…¥ç¬¬ä¸€ä¸ª `for` å¾ªç¯ä¹‹æ—¶, `module_a` çš„æƒé‡è¿›è¡Œä¸€æ¬¡ CPU åˆ° GPU çš„æ‹·è´, è€Œåæƒé‡ä¸€ç›´ä¿æŒåœ¨ GPU ä¸Š, ç›´è‡³ç¬¬äºŒä¸ª `for` å¾ªç¯å¼€å§‹æ—¶, é¦–å…ˆå°† `module_a` çš„æƒé‡ä» GPU æ‹·è´å› CPU, è€Œåå°† `module_b` çš„æƒé‡ä» CPU è½¬ç§»è‡³ GPU, è€Œåæƒé‡ä¸€ç›´ä¿æŒåœ¨ GPU ä¸Š, åœ¨ç¬¬äºŒä¸ªå¾ªç¯ç»“æŸæ—¶, å°† `module_b` çš„æƒé‡ä» GPU æ‹·è´å› CPU. è¿™æ ·æˆ‘ä»¬å°±æœ€å¤§é™åº¦åœ°é™ä½äº† CPU ä¸ GPU ä¹‹é—´åœ°å†…å­˜æ‹·è´æ¬¡æ•°, ä»è€Œæå‡äº†è¿è¡Œæ•ˆç‡.

é‚£ä¹ˆå¦‚ä½•å®ç°å‘¢? é¦–å…ˆæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªåŠè‡ªåŠ¨åŒ–çš„ `AlignDevicesHook`: å®ƒåªä¿è¯åœ¨ `forward` å‡½æ•°æ‰§è¡Œä¹‹å‰, æ¨¡å‹å‚æ•°è¢«æ­£ç¡®åœ°è½¬ç§»åˆ°è¿è¡Œè®¾å¤‡ä¸Š, å¹¶æä¾›ä¸€ä¸ª `offload` æ–¹æ³•ç”¨ä»¥å°†å‚æ•°é‡æ–°ä»è¿è¡Œè®¾å¤‡è½¬ç§»å› CPU, ä½† `offload` åº”è¯¥äº¤ç”±ç”¨æˆ·æ‰‹åŠ¨è§¦å‘, è¿™æ ·ä»¥æ¥, æˆ‘ä»¬å¯èƒ½å¯ä»¥ç”¨ç±»ä¼¼äºå¦‚ä¸‹çš„ä¼ªä»£ç å®ç°ä¹‹å‰æåˆ°çš„ä¼˜åŒ–é€»è¾‘:

```python
attach_hook_to_module(module_a, hook_a)
attach_hook_to_moduel(module_b, hook_b)
for i in range(3):
    x = module_a(x)
hook_a.offload()
for i in range(4):
    x = module_b(x)
hook_b.offload()
```

ç°åœ¨æ¥çœ‹ ğŸ¤— accelerate çš„å®ç°æ–¹æ¡ˆ, å…ˆçœ‹ä½¿ç”¨ä¾‹å­(å®Œå…¨å€Ÿç”¨è‡ªæºç  [docstring](https://github.com/huggingface/accelerate/blob/v0.20.3/src/accelerate/big_modeling.py#L194)):

```python
model_1, hook_1 = cpu_offload_with_hook(model_1, cuda_device)
model_2, hook_2 = cpu_offload_with_hook(model_2, cuda_device, prev_module_hook=hook_1)
model_3, hook_3 = cpu_offload_with_hook(model_3, cuda_device, prev_module_hook=hook_2)

hid_1 = model_1(input)
for i in range(50):
    # model1 is offloaded on the CPU at the first iteration, model 2 stays on the GPU for this whole loop.
    hid_2 = model_2(hid_1)
# model2 is offloaded to the CPU just before this forward.
hid_3 = model_3(hid_3)

# For model3, you need to manually call the hook offload method.
hook_3.offload()
```

æ³¨æ„åˆ°, è¿™é‡Œçš„ä½¿ç”¨æ–¹å¼ä¸æˆ‘ä»¬æœ€åˆå‡æƒ³çš„ä½¿ç”¨æ–¹å¼çš„æœ€å¤§åŒºåˆ«åœ¨äº `model_1` ä¸ `model_2` çš„ `hook_1` ä¸ `hook_2` ä¸éœ€è¦æ‰‹åŠ¨ç”¨ `offload` å‡½æ•°è§¦å‘. å®ç°é€»è¾‘å¦‚ä¸‹, `cpu_offload_with_hook` å‡½æ•°çš„æºç ä¸º:

```python
def cpu_offload_with_hook(
    model: torch.nn.Module,
    execution_device: Optional[Union[int, str, torch.device]] = None,
    prev_module_hook: Optional[UserCpuOffloadHook] = None,
):
    hook = CpuOffload(execution_device=execution_device, prev_module_hook=prev_module_hook)
    add_hook_to_module(model, hook, append=True)
    user_hook = UserCpuOffloadHook(model, hook)
    return model, user_hook
```

æ­¤å¤„çš„ `CpuOffload` æ˜¯å‰é¢ç« èŠ‚æåˆ°çš„ `accelerate.hooks.ModelHook` çš„å­ç±», å…¶ä»…åŒ…å« `pre_forward`, ç”¨äºä¿è¯æ‰§è¡Œ `forward` å‡½æ•°ä¹‹å‰, å…¥å‚åŠæ¨¡å‹å‚æ•°è¢«è½¬ç§»åˆ°æ‰§è¡Œè®¾å¤‡ä¸Š. è€Œ `UserCpuOffloadHook` **ä¸æ˜¯** `accelerate.hooks.ModelHook` çš„å­ç±», å®ƒä»…åŒ…å« `offload` å‡½æ•°. æ³¨æ„, è¿™é‡Œçš„ trick åœ¨äº `CpuOffload` åœ¨æ‰§è¡Œ `forward` å‡½æ•°è¢«è§¦å‘æ—¶, é™¤äº†è½¬ç§»æœ¬æ¨¡å‹çš„å‚æ•°, è¿˜è‡ªåŠ¨è§¦å‘äº† `prev_module_hook.offload` å‡½æ•°, ç”±æ­¤å®ç°è‡ªåŠ¨åŒ–, å”¯ä¸€éœ€è¦æ‰‹åŠ¨è§¦å‘ `offload` çš„æ˜¯æœ€åä¸€ä¸ª `nn.Module`. è¿™ä¸¤ä¸ªç±»çš„å®ç°æºç å¦‚ä¸‹:

```python
class CpuOffload(ModelHook):
    def __init__(
        self,
        execution_device: Optional[Union[str, int, torch.device]] = None,
        prev_module_hook: Optional["UserCpuOffloadHook"] = None,
    ):
        self.prev_module_hook = prev_module_hook

        if execution_device is not None:
            self.execution_device = execution_device
        elif is_mps_available():
            self.execution_device = torch.device("mps")
        elif torch.cuda.is_available():
            self.execution_device = torch.device(0)
        else:
            self.execution_device = torch.device("cpu")

    def init_hook(self, module):
        return module.to("cpu")

    def pre_forward(self, module, *args, **kwargs):
        module.to(self.execution_device)
        if self.prev_module_hook is not None:
            self.prev_module_hook.offload()
        return send_to_device(args, self.execution_device), send_to_device(kwargs, self.execution_device)


class UserCpuOffloadHook:
    def __init__(self, model, hook):
        self.model = model
        self.hook = hook

    def offload(self):
        self.hook.init_hook(self.model)

    def remove(self):
        remove_hook_from_module(self.model)
```