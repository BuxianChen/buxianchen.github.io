---
layout: post
title: "(WIP) 资源与计划"
date: 2023-11-18 11:10:04 +0800
---

## 动机、参考资料、涉及内容

梳理学习资源及计划

## 资源

**Pytorch 与分布式训练相关**

- Pytorch tutorial: [https://pytorch.org/tutorials](https://pytorch.org/tutorials)
- 博客园(罗西的思考), 包含了一些关于分布式机器学习的博客(最大的优点是注明了原文的出处), 博主还出了本书: [https://www.cnblogs.com/rossiXYZ/](https://www.cnblogs.com/rossiXYZ/)
- [deepspeed](https://github.com/microsoft/DeepSpeed)
- [ColossalAI](https://github.com/hpcaitech/ColossalAI)

**大模型部署相关**

- [text-generation-inference](https://huggingface.co/docs/text-generation-inference/quicktour): huggingface 出品
- [vLLM](https://github.com/vllm-project/vllm)
- [deepspeed](https://www.deepspeed.ai/tutorials/inference-tutorial/): Microsoft 出品
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII): Microsoft 出品, 也许是目前最快的?
- [lmdeploy](https://github.com/InternLM/lmdeploy): mmlab 出品
- [FastTransformer](https://github.com/NVIDIA/FasterTransformer): Nvidia 出品
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM): Nvidia 出品, 似乎是 fastertransformer 的替代, 也许是目前最快的?
- [triton-inference-server](https://developer.nvidia.com/triton-inference-server): Nvidia 出品

**量化**

`W8A8` 指模型权重和激活值都量化到 8 bit int; `W4A16` 指模型权重量化到 4 bit int, 激活值保持为 FP 16

- [GPTQ](https://github.com/IST-DASLab/gptq), [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
- [exllama](https://github.com/turboderp/exllama): GPTQ 量化后模型推理的算子优化
- [AWQ](https://github.com/mit-han-lab/llm-awq), [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)

pytorch 原生量化

pytorch 中的[量化公式](https://pytorch.org/docs/stable/quantization-support.html#quantized-dtypes-and-quantization-schemes)

- blog (2020/3/26, pytorch 1.4): [https://pytorch.org/blog/introduction-to-quantization-on-pytorch/](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)
- docs:
  - [https://pytorch.org/docs/stable/quantization.html](https://pytorch.org/docs/stable/quantization.html)
  - [pytorch wiki](https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor)
- API: [https://pytorch.org/docs/stable/quantization-support.html](https://pytorch.org/docs/stable/quantization-support.html)
- tutorial:
  - [https://pytorch.org/tutorials/recipes/quantization.html](https://pytorch.org/tutorials/recipes/quantization.html)
  - [https://leimao.github.io/article/Neural-Networks-Quantization/](https://leimao.github.io/article/Neural-Networks-Quantization/)
  - [https://leimao.github.io/blog/PyTorch-Dynamic-Quantization/](https://leimao.github.io/blog/PyTorch-Dynamic-Quantization/)

**rapidapi**
[rapidapi](https://docs.rapidapi.com/docs/what-is-rapidapi): [ToolLLM paper](https://github.com/OpenBMB/ToolBench#data)

## 计划

- 线路1 (优先): Rust 入门 + text-generation-inference/huggingface Tokenizer 库
- 线路2: 《网络是怎样连接的》
- 线路3 (torch.fx 优先): torch.fx + torchscript + torch.compile
- 线路4 (优先): torch 原生支持的量化 + QAT + AWQ
- 线路5: lmdeploy 的组装 batch 的一些细节
- 线路6: DDP 与 FSDP
- 线路7: 自动微分 + 陈天奇 dlsyscourse 课程
- 线路8: Tensor Parallel + Pipeline Parallel (lmdeploy 等框架)