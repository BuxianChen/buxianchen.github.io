---
layout: post
title: "(WIP) Huggingface PEFT"
date: 2023-10-20 10:20:24 +0800
labels: [huggingface]
---

## 动机、参考资料、涉及内容

## 使用


注意, 只分析 v0.5.0 的结构, github 上 main 分支是计划对 v0.6.0 做目录结构的[大改](https://github.com/huggingface/peft/pull/807), 这个改动基本上是:

```
peft/tuners/lora.py 
->
peft/tuners/lora
  - __init__.py
  - config.py # LoraConfig
  - layer.py  # lora 的一些 Layer 定义
  - model.py  # LoraModel
  - bnb.py    # bitandbytes 的一些 Layer 定义
  - gptq.py   # GPTQ 的一些 Layer 定义
```

没有实质的代码变动, 且对外接口实际上通过 `__init__.py` 之后实际上也是无感知的, 出于本文

Config 类的继承关系:

```
PushToHubMixin: transformers.utils.PushToHubMixin, 相当于只有 `push_to_hub` 方法
  - PeftConfigMixin: peft.utils.config.PeftConfigMixin, 相当于只增加了 `save_pretrained` 和 `from_pretrained` 方法
    - PeftConfig: peft.utils.config.PeftConfig, 只是增加了一些属性, 比较关键的是: `peft_type`, `base_model_name_or_path`, `task_type`, `inference_mode`
      - LoraConfig: peft.tuners.lora.LoraConfig, 设定 `peft_type=PeftType.LORA`, 再增加了一些其他属性
      - PromptLearningConfig: peft.utils.config.PromptLearningConfig, 只增加了一些属性: prefix_tuning, prompt_tuning, prompt_encoder 三个算法共用的一些属性
        - PrefixTuningConfig: peft.tuners.prefix_tuning.PrefixTuningConfig, 设定 `peft_type=PeftType.PREFIX_TUNING`, 再增加了一些其他属性
```

总之, `peft/utils/{peft_type}.py` 文件中定义了相应的 Config 类, 而这个类本质上就是附带了 `push_to_hub`, `save_pretrained`, `from_pretrained` 的“字典”


`PeftModel` 是对外的核心类, 其内部包含 `LoraModel`, `PrefixEncoder` 这种特殊的层, 而 `PeftModelForSequenceClassification` 是 `PeftModel` 的子类. 

`PeftModel` 做了可以切换多个 adapter 的设计:

```python
lora_model = PeftModel(model, peft_config: PeftConfig, adapter_name="default")
lora_model.active_adapter  # default
lora_model.peft_config  # {"default": LoraConfig(...)}
lora_model.active_peft_config == lora_model.peft_config[lora_model.active_adapter]

lora_model.get_base_model()  # 必定是 transformers 里的 PretrainedModel
lora_model.base_model        # 取决于具体的算法, 如果是 lora, 则是 LoraModel, 如果是 prefix_tuning, 则为 PretrainedModel
```

下面主要解释[这里](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/image_classification/image_classification_peft_lora.ipynb#scrollTo=vhvCQpP-isJr)发生了什么? 我们按重要顺序关注如下几点:

- `PeftModel.__init__`
- `PeftModel.forward`
- 保存模型相关
- 插拔 lora 相关内容

```python
from transformers import AutoModelForImageClassification, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model

model_checkpoint = "google/vit-base-patch16-224-in21k"

model = AutoModelForImageClassification.from_pretrained(
    model_checkpoint,
    # label2id=label2id,
    # id2label=id2label,
    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
)
print_trainable_parameters(model)

config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["query", "value"],
    lora_dropout=0.1,
    bias="none",
    modules_to_save=["classifier"],
)
lora_model = get_peft_model(model, config)  # 在这种情况下, 基本等价于下面这一行
# lora_model = PeftModel(model, config, adapter_name="defalut")
print_trainable_parameters(lora_model)
```

在上面的例子里, `PeftModel(model, config)` 中最为关键的步骤是执行 `self.base_model = LoraModel(model, config)`

`peft/tuners/lora.py` 实质上就是这几个类的定义

```python
LoraConfig(PeftConfig)
BaseTunerLayer(ABC)  # merge/unmerge 抽象方法
  - LoraLayer(BaseTunerLayer)
    - Linear(nn.Linear, LoraLayer)
    - Embedding(nn.Embedding, LoraLayer)
    - Conv2D(nn.Conv2D, LoraLayer)
    - Linear8bitLt(bnb.nn.Linear8bitLt, LoraLayer)  # 与 bnb 相关
    - Linear4bit(bnb.nn.Linear4bit, LoraLayer)      # 与 bnb 相关
    - QuantLinear(torch.nn.Module, LoraLayer)       # 似乎与 GPTQ 相关

BaseTuner(ABC, nn.Module)
LoraModel(BaseTuner)
```

首先回顾 LoRA 的细节:

- Linear: 
- Conv2d: 假设原本的层为 `layer=Conv2d(in_feat, out_feat, conv_size, stride, ...)`, 使用 LoRA 的流程是
  ```python
  layer = Conv2d(in_feat, out_feat, conv_size, stride, ...)
  lora_a = Conv2d(in_feat, r, conv_size, stride, ...)
  lora_b = Conv2d(r, out_feat, conv_size=(1, 1))

  def forward(x):
    layer(x) + lora_a()

  ```