---
layout: post
title: "(P0) GGUF,GGML,llama.cpp"
date: 2024-10-09 13:00:04 +0800
labels: [llm]
---

## 动机、参考资料、涉及内容

涉及内容: GGUF 文件格式, llama.cpp, GGML

## 简介

[GGML](https://github.com/ggerganov/ggml)(GPT-Generated Model Language) 是一个开源项目(可以类比 pytorch), 用于高效的利用 CPU 进行张量计算, 项目定义了一种 GGUF(GPT-Generated Unified Format) 文件格式. GGML 的作者也同样是 [llama.cpp](https://github.com/ggerganov/llama.cpp) 的作者, 而 llama.cpp 用于高效的利用 CPU 进行市面上流行的许多开源大模型的推理.

目前看 GGML 和 llama.cpp 项目没有依赖关系, 因此可能有代码会重复?

如果只希望快速部署一些开源大模型, 一般只需要关心 llama.cpp 即可, 具体例子见下节

## llama.cpp Quick Start

普通的算法工程师一般只需要知道按如下方式部署一个现成的模型

参考: [https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF), 步骤如下:

```bash
git clone --depth=1 -b master https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
# 编译可执行文件
make -j8
# 下载 gguf 文件
huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct-GGUF qwen2.5-1.5b-instruct-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False
# 运行一个命令行交互的简单 demo
./llama-cli -m qwen2.5-1.5b-instruct-q5_k_m.gguf -co -cnv -p "You are Qwen, created by Alibaba Cloud. You are a helpful assistant." -fa -ngl 80 -n 512
```

更深入的情况是:

- Q1: 如果只有 `pytorch_model.bin` 这种形式的权重, 怎么转换为 GGUF 格式?
- A1: 如果模型结构被 [convert_hf_to_gguf.py](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) 脚本支持, 那么直接用脚本转化即可
- Q2: 如果模型结构不被支持呢?
- A2: 可以修改 `convert_hf_to_gguf.py` 文件, 其实如果你这么做了, 你事实上可以为 llama.cpp 提 PR, 实际做起来你可能需要参考 [HOWTO-add-model.md](https://github.com/ggerganov/llama.cpp/blob/master/docs/development/HOWTO-add-model.md)
- Q3: 如果你很想知道 GGUF 文件的具体格式, 以探索用 GGUF 文件存储别的东西, 或者做更多的事情
- A3: 请看下文

通常情况下, 至多只需要知道 A1 即可: 一般是你对模型做了微调, 需要自己转换格式, 但一般来说也就是用 huggingface transformers 进行微调, 而它大概也继承了这个转换脚本, 因此甚至于不需要知道 A1.

## GGUF 文件格式

- GGML 文档 (最官方的描述): [https://github.com/ggerganov/ggml/blob/master/docs/gguf.md](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- huggingface hub 文档: [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

`llama.cpp` 项目下包含一个子项目: gguf-py ([pypi:gguf](https://pypi.org/project/gguf/)), 这个子项目是纯 python 实现 GGUF 文件的读写, 因此可以用来了解 GGUF 的文件格式.

TODO: llama.cpp 里实际使用的应该是 C 语言的实现, 待搜寻

### 初体验: 读取一个实际的 GGUF 文件

```bash
cd /path/to/llama.cpp/gguf-py
pip install -e .
python examples/reader.py ../qwen/qwen2.5-1.5b-instruct-q5_k_m.gguf
```

可以得到这样的输出(省略了许多层)

```
Key-Value Pairs:
GGUF.version                           : [3]
GGUF.tensor_count                      : [339]
GGUF.kv_count                          : [26]
general.architecture                   : [113 119 101 110  50]
general.type                           : [109 111 100 101 108]
general.name                           : [113 119 101 110  50  46  53  45  49  46  53  98  45 105 110 115 116 114 117  99 116]
general.version                        : [118  48  46  49]
general.finetune                       : [113 119 101 110  50  46  53  45  49  46  53  98  45 105 110 115 116 114 117  99 116]
general.size_label                     : [49 46 56 66]
qwen2.block_count                      : [28]
qwen2.context_length                   : [32768]
qwen2.embedding_length                 : [1536]
qwen2.feed_forward_length              : [8960]
qwen2.attention.head_count             : [12]
qwen2.attention.head_count_kv          : [2]
qwen2.rope.freq_base                   : [1000000.]
qwen2.attention.layer_norm_rms_epsilon : [1.e-06]
general.file_type                      : [17]
tokenizer.ggml.model                   : [103 112 116  50]
tokenizer.ggml.pre                     : [113 119 101 110  50]
tokenizer.ggml.tokens                  : [33]
tokenizer.ggml.token_type              : [1]
tokenizer.ggml.merges                  : [196 160  32 196 160]
tokenizer.ggml.eos_token_id            : [151645]
tokenizer.ggml.padding_token_id        : [151643]
tokenizer.ggml.bos_token_id            : [151643]
tokenizer.ggml.add_bos_token           : [False]
tokenizer.chat_template                : [123  37  45 ...  37 125  10]
general.quantization_version           : [2]
----
Tensors:
Tensor Name                    | Shape: Shape           | Size: Size         | Quantization: Quantization
--------------------------------------------------------------------------------
output.weight                  | Shape: 1536x151936     | Size: 233373696    | Quantization: Q6_K
token_embd.weight              | Shape: 1536x151936     | Size: 233373696    | Quantization: Q5_K
blk.0.attn_norm.weight         | Shape: 1536            | Size: 1536         | Quantization: F32
blk.0.ffn_down.weight          | Shape: 8960x1536       | Size: 13762560     | Quantization: Q6_K
blk.0.ffn_gate.weight          | Shape: 1536x8960       | Size: 13762560     | Quantization: Q5_K
blk.0.ffn_up.weight            | Shape: 1536x8960       | Size: 13762560     | Quantization: Q5_K
blk.0.ffn_norm.weight          | Shape: 1536            | Size: 1536         | Quantization: F32
blk.0.attn_k.bias              | Shape: 256             | Size: 256          | Quantization: F32
blk.0.attn_k.weight            | Shape: 1536x256        | Size: 393216       | Quantization: Q5_K
blk.0.attn_output.weight       | Shape: 1536x1536       | Size: 2359296      | Quantization: Q5_K
blk.0.attn_q.bias              | Shape: 1536            | Size: 1536         | Quantization: F32
blk.0.attn_q.weight            | Shape: 1536x1536       | Size: 2359296      | Quantization: Q5_K
blk.0.attn_v.bias              | Shape: 256             | Size: 256          | Quantization: F32
blk.0.attn_v.weight            | Shape: 1536x256        | Size: 393216       | Quantization: Q6_K
...
output_norm.weight             | Shape: 1536            | Size: 1536         | Quantization: F32
```

这些输出可以与[huggingface-hub](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/tree/main?show_file_info=qwen2.5-1.5b-instruct-q5_k_m.gguf)的显示进行对照, 两者本质是完全一致的, 只是 huggingface-hub 上的显示更为友好一些, 例如:

```python
# general.architecture: [113 119 101 110  50]
x = [113, 119, 101, 110, 50]
assert "".join([chr(_) for _ in x]) == "qwen2"
```

GGUF 与 Pytorch 相互转化, 可以参考: [https://huggingface.co/docs/transformers/v4.45.2/en/gguf](https://huggingface.co/docs/transformers/v4.45.2/en/gguf)

```python
# 利用 transformers 将 GGUF 转化为普通的 Pytorch Module 的 state_dict: 本质上也是利用 gguf python 包: from gguf import GGUFReader, dequantize
from transformers import AutoModelForCausalLM
import os
path = "/path/to/qwen/qwen2.5-1.5b-instruct-q5_k_m.gguf"
model = AutoModelForCausalLM.from_pretrained(os.path.dirname(path), gguf_file=path)  # 转化后是普通版本 float 权重的模型

# 将 transformers 的 Pytorch Module 权重文件转化为 GGUF 格式: 本质上是一个模型一个模型去与 huggingface 对齐的, 实质上也是使用了 GGUFWriter
https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py
```

接下来, 我们将深入理解读取过程, 从而理解 GGUF 的数据排布方式

### 前置知识

numpy 相关

```python
import numpy as np
import struct

# memmap
nrows, ncols = 2, 3
arr = np.memmap('arr.dat', dtype=np.int32, mode='w', shape=(nrows, ncols))
arr[0][0] = 1
arr[0][1] = 2
arr[0][2] = 3
arr[1][0] = 4
arr[1][1] = 5
arr[1][2] = 6
arr.flush()

with open('arr.dat', "rb") as fr:
    x = fr.read()

struct.unpack("6I", x)  # (1, 2, 3, 4, 5, 6)


# npy 文件
with open("arr.npy", "wb") as fw:
    np.save(fw, np.array([1, 2, 3, 4], dtype=np.int32))
with open("arr.npy", "rb") as fr:
    x = fr.read()  # b"\x93NUMPY\x01\x00v\x00{'descr': '<i4', 'fortran_order': False, 'shape': (4,), }
len(x)  # 144
```

以上示例代码有如下要点:

- `np.memmap` 写入的内容只包含纯数据, 而 `.npy` 文件还包含了一些额外信息
- `numpy` 数组是行优先存储的

### GGUF 文件数据排布

`reader.py` 详解: GGUF 文件的内容格式如下图所示

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/gguf-spec.png)

以下为文件内容的 Overview (精确描述)

- 文件的前 4 个字节以 uint32 按 Little Endian 读取必须是 GGUF
- 接下来的 4 个字节以 uint32 按系统默认的字节序读取, 得到版本号(目前是3), 并同时得知后续内容中文件的字节序是否与系统的一致
- 接下来的 8 个字节代表 metadata 总共有几个
- 接下来的 8 个字节代表 tensor 总共有几个
- metadata 的实际数据内容
- tensor 的元信息, 例如数据类型, 数据个数, 起始位置
- padding: 字节对齐, 目的是为了读 tensor 时速度更快
- tensor 的实际数据内容

### GGUFReader 详解

先看头部的 `4+4+8+8=24` 个字节

然后看 metadata 的部分

```python
class GGUFValueType(IntEnum):
    UINT8   = 0
    INT8    = 1
    UINT16  = 2
    INT16   = 3
    UINT32  = 4
    INT32   = 5
    FLOAT32 = 6
    BOOL    = 7
    STRING  = 8
    ARRAY   = 9
    UINT64  = 10
    INT64   = 11
    FLOAT64 = 12

class ReaderField(NamedTuple):
    offset: int  # 起始地址
    name: str    # key的字符串表示
    parts: list[npt.NDArray[Any]] = []  # 将字节拆分为多个部分, 将 parts 转化为字节合并在一起即为文件中原始的字节表示
    data: list[int] = [-1]  # parts 的下标数组, 真实的数据所对应的实际下标
    types: list[GGUFValueType] = []  # value的数据类型描述, 对于简单的标量, types 仅包含一个元素, 对于嵌套情形, types 包含多个元素
```


```python
import numpy as np
from gguf.constants import GGUFValueType


with open(path, "rb") as fr:
    offset = 0
    
    x = fr.read(4)  # b'GGUF': 魔术数字
    offset += 4
    
    x = fr.read(4)   # b'\x03\x00\x00\x00'
    np.frombuffer(x, dtype=np.uint32)  # array([3], dtype=uint32): GGUF 版本号
    offset += 4

    tensor_count = fr.read(8)
    tensor_count = int(np.frombuffer(tensor_count, dtype=np.uint64))  # 339, 表示有 339 个 tensor
    offset += 8

    kv_count = fr.read(8)
    kv_count = int(np.frombuffer(kv_count, dtype=np.uint64)[0])  # 26, 表示后续有 26 个 metadata
    offset += 8

    # value 的类型仅支持数字,字符串, Array. 但 Array 里必须是同类型的, 且允许嵌套(嵌套情形后面再看)

    # metadata 的数据组织形式
    # uint64: key 的长度
    # key
    # uint32: value 的类型
    # 以下排布方式与 value 类型相关:
    # (1) string:
    # uint64: value 的长度
    # value
    # 例子: [20, general.architecture, 8, 5, qwen2], GGUFValueType.STRING=8
    # (2) uint8,int8,uint16,int16,uint32,int32,uint64,int64,bool,float32,float64
    # value
    # 例子: [26, qwen2.attention.head_count, 4, 12], GGUFValueType.UINT32=4
    # (3) array
    # uint32: value 中每个元素的类型
    # value: 根据每个元素是字符串还是数值类型确定
    # 例子(array[string]): [21, tokenizer.ggml.tokens, 9, 8, 1, 33, 1, 34, ...]
    # 其中 GGUFValueType.ARRAY=8, GGUFValueType.STRING=8
    # 接下来每两个数据项为一组, 例如: [1, 33] 代表 `!`, [1, 34] 代表 `"`
    

    # ReaderField: NamedTuple
    # offest: int, 起始地址
    # name: str, key 的字符串形式
    # parts: List[np.array], 以 value 是字符串类型为例, parts 是一个五元组
    #     [
    #         np.array([20], dtype=np.uint64),
    #         np.array([103, 101, 110, 101, 114,  97, 108,  46,  97, 114,  99, 104, 105, 116, 101,  99, 116, 117, 114, 101], dtype=np.uint8),
    #         np.array([8], dtype=np.uint32),
    #         np.array([5], dtype=np.uint64),
    #         np.array([113, 119, 101, 110,  50], dtype=np.uint8)
    #     ]
    # data: List[int], parts 中哪些项是数据, value 是字符串类型为例, data 是 [4], 代表 parts[4] 是 value 的值 
    # types: List[GGUFValueType], 以 value 是字符串类型为例, types = [GGUFValueType.STRING]
    # types 与 data 的长度不一定一致, 例如 data=[6,8,...], type = [GGUFValueType.ARRAY, GGUFValueType.STRING]
    
    key_len = fr.read(8)
    key_len = int(np.frombuffer(key_len, dtype=np.uint64)[0])
    offset += 8
    
    key_data = fr.read(key_len)
    key_data = np.frombuffer(key_data, dtype=np.uint8)
    offset += key_len

    raw_kv_type = fr.read(4)
    raw_kv_type = int(np.frombuffer(raw_kv_type, dtype=np.uint32)[0])  # 8, 代表 String
```


metadata 中的 value 可以是 Array, 且允许一定程度的嵌套

```python
from gguf import GGUFWriter, GGUFReader
import numpy as np

def writer_example() -> None:
    gguf_writer = GGUFWriter("example.gguf", "llama")
    
    gguf_writer.add_array("arr", [[1, 2, 3], [4, 5, 6]])  # ok
    # gguf_writer.add_array("arr", [[1, 2, 3], ["abc", "def"]])  # ok, 嵌套时允许不等长, 也允许基本元素类型不一致
    # gguf_writer.add_array("arr", [[1, 2, 3], ["abc", "def", 3]])  # error
    
    tensor1 = np.ones((32,), dtype=np.float32) * 100.0
    gguf_writer.add_tensor("tensor1", tensor1)
    gguf_writer.write_header_to_file()
    gguf_writer.write_kv_data_to_file()
    gguf_writer.write_tensors_to_file()
    gguf_writer.close()

writer_example()
reader = GGUFReader("example.gguf")

reader.fields['arr'].parts

# [[1, 2, 3], [4, 5, 6]] 的情形
# 输出如下
# [memmap([3], dtype=uint64),
#  memmap([ 97, 114, 114], dtype=uint8),
#  memmap([9], dtype=uint32),
#  memmap([9], dtype=uint32),
#  memmap([2], dtype=uint64),
#  memmap([5], dtype=uint32),
#  memmap([3], dtype=uint64),
#  memmap([1], dtype=int32),
#  memmap([2], dtype=int32),
#  memmap([3], dtype=int32),
#  memmap([5], dtype=uint32),
#  memmap([3], dtype=uint64),
#  memmap([4], dtype=int32),
#  memmap([5], dtype=int32),
#  memmap([6], dtype=int32)]
# 解释如下
# 3(key的字节数), "arr", 9(ARRAY), 9(ARRAY), 2(外层数组长度), 5(INT32) 3(内层数组长度), 1, 2, 3, 5(INT32), 3(内层数组长度), 2, 3, 4

# [[1, 2, 3], ["abc", "def"]] 的情形
# 输出如下:
# [memmap([3], dtype=uint64),
#  memmap([ 97, 114, 114], dtype=uint8),
#  memmap([9], dtype=uint32),
#  memmap([9], dtype=uint32),
#  memmap([2], dtype=uint64),
#  memmap([5], dtype=uint32),
#  memmap([3], dtype=uint64),
#  memmap([1], dtype=int32),
#  memmap([2], dtype=int32),
#  memmap([3], dtype=int32),
#  memmap([8], dtype=uint32),
#  memmap([2], dtype=uint64),
#  memmap([3], dtype=uint64),
#  memmap([97, 98, 99], dtype=uint8),
#  memmap([3], dtype=uint64),
#  memmap([100, 101, 102], dtype=uint8)]
# 解释如下:
# 3(key的字节数), "arr", 9(ARRAY), 9(ARRAY), 2(外层数组长度), 5(INT32), 3(内层数组长度), 1, 2, 3, 8(STRING), 2(内层数组长度), 3(字符串字节长度), "abc", 3(字符串字节长度), "def"
```
