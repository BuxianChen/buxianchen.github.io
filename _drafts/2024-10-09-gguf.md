---
layout: post
title: "(P0) GGUF,GGML,llama.cpp"
date: 2024-10-09 13:00:04 +0800
labels: [llm]
---

## 动机、参考资料、涉及内容

涉及内容: GGUF 文件格式, llama.cpp, GGML

## 简介

[GGML](https://github.com/ggerganov/ggml)(GPT-Generated Model Language) 是一个开源项目(可以类比 pytorch), 用于高效的利用 CPU 进行张量计算, 项目定义了一种 GGUF(GPT-Generated Unified Format) 文件格式. GGML 的作者也同样是 [llama.cpp](https://github.com/ggerganov/llama.cpp) 的作者, 而 llama.cpp 用于高效的利用 CPU 进行市面上流行的许多开源大模型的推理.

目前看 GGML 和 llama.cpp 项目没有依赖关系, 因此可能有代码会重复?

如果只希望快速部署一些开源大模型, 一般只需要关心 llama.cpp 即可, 具体例子见下节

## llama.cpp Quick Start

参考: [https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF), 步骤如下:

```bash
git clone --depth=1 -b master https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
# 编译可执行文件
make -j8
# 下载 gguf 文件
huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct-GGUF qwen2.5-1.5b-instruct-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False
# 运行一个命令行交互的简单 demo
./llama-cli -m qwen2.5-1.5b-instruct-q5_k_m.gguf -co -cnv -p "You are Qwen, created by Alibaba Cloud. You are a helpful assistant." -fa -ngl 80 -n 512
```

## GGUF 文件格式

- GGML 文档 (最官方的描述): [https://github.com/ggerganov/ggml/blob/master/docs/gguf.md](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- huggingface hub 文档: [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

`llama.cpp` 项目下包含一个子项目: gguf-py ([pypi:gguf](https://pypi.org/project/gguf/)), 这个子项目是纯 python 实现 GGUF 文件的读写, 因此可以用来了解 GGUF 的文件格式.

TODO: llama.cpp 里实际使用的应该是 C 语言的实现, 待搜寻

```bash
cd /path/to/llama.cpp/gguf-py
pip install -e .
python examples/reader.py ../qwen/qwen2.5-1.5b-instruct-q5_k_m.gguf
```

可以得到这样的输出(省略了许多层)

```
Key-Value Pairs:
GGUF.version                           : [3]
GGUF.tensor_count                      : [339]
GGUF.kv_count                          : [26]
general.architecture                   : [113 119 101 110  50]
general.type                           : [109 111 100 101 108]
general.name                           : [113 119 101 110  50  46  53  45  49  46  53  98  45 105 110 115 116 114
 117  99 116]
general.version                        : [118  48  46  49]
general.finetune                       : [113 119 101 110  50  46  53  45  49  46  53  98  45 105 110 115 116 114
 117  99 116]
general.size_label                     : [49 46 56 66]
qwen2.block_count                      : [28]
qwen2.context_length                   : [32768]
qwen2.embedding_length                 : [1536]
qwen2.feed_forward_length              : [8960]
qwen2.attention.head_count             : [12]
qwen2.attention.head_count_kv          : [2]
qwen2.rope.freq_base                   : [1000000.]
qwen2.attention.layer_norm_rms_epsilon : [1.e-06]
general.file_type                      : [17]
tokenizer.ggml.model                   : [103 112 116  50]
tokenizer.ggml.pre                     : [113 119 101 110  50]
tokenizer.ggml.tokens                  : [33]
tokenizer.ggml.token_type              : [1]
tokenizer.ggml.merges                  : [196 160  32 196 160]
tokenizer.ggml.eos_token_id            : [151645]
tokenizer.ggml.padding_token_id        : [151643]
tokenizer.ggml.bos_token_id            : [151643]
tokenizer.ggml.add_bos_token           : [False]
tokenizer.chat_template                : [123  37  45 ...  37 125  10]
general.quantization_version           : [2]
----
Tensors:
Tensor Name                    | Shape: Shape           | Size: Size         | Quantization: Quantization
--------------------------------------------------------------------------------
output.weight                  | Shape: 1536x151936     | Size: 233373696    | Quantization: Q6_K
token_embd.weight              | Shape: 1536x151936     | Size: 233373696    | Quantization: Q5_K
blk.0.attn_norm.weight         | Shape: 1536            | Size: 1536         | Quantization: F32
blk.0.ffn_down.weight          | Shape: 8960x1536       | Size: 13762560     | Quantization: Q6_K
blk.0.ffn_gate.weight          | Shape: 1536x8960       | Size: 13762560     | Quantization: Q5_K
blk.0.ffn_up.weight            | Shape: 1536x8960       | Size: 13762560     | Quantization: Q5_K
blk.0.ffn_norm.weight          | Shape: 1536            | Size: 1536         | Quantization: F32
blk.0.attn_k.bias              | Shape: 256             | Size: 256          | Quantization: F32
blk.0.attn_k.weight            | Shape: 1536x256        | Size: 393216       | Quantization: Q5_K
blk.0.attn_output.weight       | Shape: 1536x1536       | Size: 2359296      | Quantization: Q5_K
blk.0.attn_q.bias              | Shape: 1536            | Size: 1536         | Quantization: F32
blk.0.attn_q.weight            | Shape: 1536x1536       | Size: 2359296      | Quantization: Q5_K
blk.0.attn_v.bias              | Shape: 256             | Size: 256          | Quantization: F32
blk.0.attn_v.weight            | Shape: 1536x256        | Size: 393216       | Quantization: Q6_K
...
output_norm.weight             | Shape: 1536            | Size: 1536         | Quantization: F32
```

这些输出可以与[huggingface-hub](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/tree/main?show_file_info=qwen2.5-1.5b-instruct-q5_k_m.gguf)的显示进行对照, 两者本质是完全一致的, 只是 huggingface-hub 上的显示更为友好一些, 例如:

```python
# general.architecture: [113 119 101 110  50]
x = [113 119 101 110  50]
assert "".join([chr(_) for _ in x]) == "qwen2"
```

TODO: 详解上面的 `reader.py`


GGUF 与 Pytorch 相互转化, 参考: [https://huggingface.co/docs/transformers/v4.45.2/en/gguf](https://huggingface.co/docs/transformers/v4.45.2/en/gguf)

```python
# 利用 transformers 将 GGUF 转化为普通的 Pytorch Module
from transformers import AutoModelForCausalLM
import os
path = "/path/to/qwen/qwen2.5-1.5b-instruct-q5_k_m.gguf"
model = AutoModelForCausalLM.from_pretrained(os.path.dirname(path), gguf_file=path)

# 将 transformers 的 Pytorch Module 权重文件转化为 GGUF 格式:
https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py
```
