---
layout: post
title: "(P0) tiktoken 详解"
date: 2024-05-15 10:05:04 +0800
labels: [tokenizer,tiktoken,bpe]
---

## 动机、参考资料、涉及内容

- tiktoken 的实现细节, 包括 python 实现与 rust 实现, 完全以学习的角度看: [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)
- 为什么 tiktoken 快过 huggingface 的实现

## tiktoken

### 使用

tiktoken 是 BPE 算法的实现, 最重要的特点是 encode 与 decode 是无损的

**简单使用**

```python
from tiktoken import get_encoding
from typing import List
enc = get_encoding("cl100k_base")
text = "hello world"
tokens: List[int] = enc.encode("hello world")
decode_text: str = enc.decode(tokens)
assert text == decode_text
```

**关于 special_token 的处理**

```python
enc.encode("<|endofprompt|>", allowed_special=set(), disallowed_special="all")  # 默认值, 触发 Error
enc.encode("<|endofprompt|>", allowed_special="all", disallowed_special="all")  # [100276]

# 源码中的处理逻辑如下:
if allowed_special == "all":
    allowed_special = self.special_tokens_set
if disallowed_special == "all":
    disallowed_special = self.special_tokens_set - allowed_special
# 只要 disallowed_special 不为空, 输入的 text 就不能包含 disallowed_special 中的字符
if disallowed_special:
    if match := _special_token_regex(disallowed_special).search(text):
        raise_disallowed_special_token(match.group())
# 这个 _core_bpe 的类型是 Rust 绑定到 Python 的
return self._core_bpe.encode(text, allowed_special)
```

### 前置说明

下面以 `cl100k_base` 为例来分析 `enc = get_encoding("cl100k_base")` 的具体流程 (gpt3.5, gpt4, text-embedding-ada-002 这几个模型都是使用这个 tokenizer)

```python
# tiktoken.get_encoding("cl100k_base") 的实际调用流程

ENDOFTEXT = "<|endoftext|>"
FIM_PREFIX = "<|fim_prefix|>"
FIM_MIDDLE = "<|fim_middle|>"
FIM_SUFFIX = "<|fim_suffix|>"
ENDOFPROMPT = "<|endofprompt|>"

def cl100k_base():
    # cl100k_base.tiktoken 本身是一个文本文件, 每一行是 base64 表示的 token 表示, 以及相应的 token 序号
    mergeable_ranks: dict[bytes, int] = load_tiktoken_bpe(
        "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken",
        expected_hash="223921b76ee99bde995b7ff738513eef100fb51d18c93597a113bcffe865b2a7",
    )
    # load_tiktoken_bpe 的实际过程就是下载文件(或者从缓存中读取), 然后像这样解析
    # lines = open("cl100k_base.tiktoken", "rb").read().splitlines()
    # mergeable_ranks = {
    #     base64.b64decode(token): int(rank)
    #     for token, rank in (line.split() for line in lines if line)
    # }
    special_tokens = {
        ENDOFTEXT: 100257,
        FIM_PREFIX: 100258,
        FIM_MIDDLE: 100259,
        FIM_SUFFIX: 100260,
        ENDOFPROMPT: 100276,
    }
    return {
        "name": "cl100k_base",
        "pat_str": r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+""",
        "mergeable_ranks": mergeable_ranks,
        "special_tokens": special_tokens,
    }

tokenizer = Encoding(**cl100k_base())
```

值得注意点主要包括 `mergeable_ranks`, `pat_str`, `Encoding` 这几处

(1) 这里先探索一下 `mergeable_ranks`

- 前 256 个 token 完全覆盖了 `\x00` ~ `\xff` 这 `2^8=256` 个单字节字符
  ```python
  import base64
    lines = open("cl100k_base.tiktoken", "rb").read().splitlines()
    mergeable_ranks = {
        base64.b64decode(token): int(rank) for token, rank in (line.split() for line in lines if line)
    }
    a = set([ord(token) for token, num in list(mergeable_ranks.items())[:256]])
    b = set([i for i in range(256)])
    a == b  # True
  ```
- `mergeable_ranks` 长度为 100256, 因此特殊 token 的序号刚好错开
  ```python
  all([i == num for i, (token, num) in enumerate(list(mergeable_ranks.items()))]) == True # True
  len(mergeable_ranks)  # 100256
  ```

(2) 这里的 `Encoding` 主要是对 rust 实现的 tokenizer 的封装, 相应的代码如下

```python
# tiktoken/core.py
class Encoding:
    def __init__(self, ...):
        # 其余代码从略 ...
        self._special_tokens = special_tokens
        # _tiktoken 是一个 rust 编译出的的Python扩展动态链接库, 在 pip install tiktoken 后在硬盘上大约位于
        # site-packages/tiktoken/_tiktoken.cpython-39-x86_64-linux-gnu.so
        self._core_bpe = _tiktoken.CoreBPE(mergeable_ranks, special_tokens, pat_str)

    @functools.cached_property
    def special_tokens_set(self) -> set[str]:
        return set(self._special_tokens.keys())

    def encode(
        self,
        text: str,
        *,
        allowed_special: Union[Literal["all"], AbstractSet[str]] = set(),  # noqa: B006
        disallowed_special: Union[Literal["all"], Collection[str]] = "all",
    ) -> list[int]:
        if allowed_special == "all":
            allowed_special = self.special_tokens_set
        if disallowed_special == "all":
            disallowed_special = self.special_tokens_set - allowed_special
        if disallowed_special:  # 如果 disallowed_special = set("")
            if not isinstance(disallowed_special, frozenset):
                disallowed_special = frozenset(disallowed_special)
            if match := _special_token_regex(disallowed_special).search(text):
                raise_disallowed_special_token(match.group())

        if isinstance(allowed_special, frozenset):
            allowed_special = set(allowed_special)

        try:
            # 大多数情况是进入这个分支
            return self._core_bpe.encode(text, allowed_special)
        except UnicodeEncodeError:  # 如果有兴趣, 这个分支可参考源码说明
            text = text.encode("utf-16", "surrogatepass").decode("utf-16", "replace")
            return self._core_bpe.encode(text, allowed_special)
    def decode(self, tokens: list[int], errors: str = "replace") -> str:
        return self._core_bpe.decode_bytes(tokens).decode("utf-8", errors=errors)
```

由此可以看出工作完全是交接给 Rust 实现的 CoreBPE, 这便是本文的目标

(3) 这个 `pat_str` 用在预切分, 这里以 gpt-2 举例, 这个正则的特征是**能将原始文本完全切分**, 也就是切分完后可以重新精确恢复原始文本.

```python
import regex
pat = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
text = "a's 1,123  abc  中国人"
parts = regex.findall(pat, text)
# ['a', "'s", ' 1', ',', '123', ' ', ' abc', ' ', ' 中国人']
# 可以看出对英文比较友好, 而连续的汉字会切分不开
"".join(parts) == text  # True
```

总的来说就是一堆“或”关系的匹配, 具体可以拆解为:

- `'(?:[sdmt]|ll|ve|re)`: 实际上就是代表匹配 `'s|'d|'m|'t|'ll|'ve|'re`, 基本上就是常见的缩写
- ` ?\p{L}+`: 多个连续的 Unicode 文字
- ` ?\p{N}+`: 多个连续的 Unicode 数字
- ` ?[^\s\p{L}\p{N}]+`: 非空白, 非 Unicode 文字, 
- `\s+(?!\S)`: 多个空白, 但不包含最后一个
- `\s+`: 多个空白

备注: Unicode 字符集除了 `p{L}`(文字) 和 `p{N}`(数字) 外, 还有符号, 空白符, emoji 等等

备注: 最前面的 `?:` 只是代表是非捕获组, 解释如下:
```python
import re
text = "today is 2024-05-15, tomorrow is 2024-05-16"
pat1 = r"(\d{4})-(\d{2})-(\d{2})"
pat2 = r"(?:\d{4})-(\d{2})-(\d{2})"
print("捕获组")
for result in re.findall(pat1, text):
    print(result)
print("非捕获组")
for result in re.findall(pat2, text):
    print(result)
```
输出结果如下(其实只是结果里不包含非捕获组而已)
```
捕获组
('2024', '05', '15')
('2024', '05', '16')
非捕获组
('05', '15')
('05', '16')
```

cl100k_base 的这个 `pat_str` 如下

```python
pat_str = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""
```

值得注意的是, 这个 “正则” 不符合 python 的标注: 例如里面包含这种子序列 `?+`, 这对于 python 正则是不合法的, 而像 `\p{L}` 代表匹配任意的 Unicode 文本字符, 这也同样不能用 python 的 re 模块所解析 (可以借助第三方包 `regex`). 后续会重新看看这个正则(从 Rust 的实现来看).

### bpe 算法

完整实现请直接参考 `tiktoken/_education.py`

关于 `bytes` 的注解:

```python
bytes(3)  # 表示 3 个 \x00, 即: b'\x00\x00\x00'
bytes([3, 4])  # 表示 b'\x03\x04'
b''.join([bytes([3]), bytes([4])])  # [b'\x03', b'\x04'] -> b'\x03\x04'
```

**train**

1) 初始化:

```python
import regex
pat_str = r"""'s|'t|'re|'ve|'m|'ll|'d| ?[\p{L}]+| ?[\p{N}]+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
data = "中国人, haven't"

words: list[list[bytes]] = [
    [bytes([b]) for b in word.encode("utf-8")] for word in regex.findall(pat_str, data)
]

# 具体过程为:
# regex.findall(pat_str, data)  # ['中国人', ',', ' haven', "'t"]
# 然后再将每个部分分解为字节:
# words = [
#     [b'\xe4', b'\xb8', b'\xad', b'\xe5', b'\x9b', b'\xbd', b'\xe4', b'\xba', b'\xba'],
#     [b','],
#     [b' ', b'h', b'a', b'v', b'e', b'n'],
#     [b"'", b't']
# ]
# assert b''.join(words[0]).decode() == "中国人"
```

2) 迭代:

```python
while len(ranks) < vocab_size:  # vocab_size 是期望的词表大小
    # step 1: 找到最常见的紧邻的字节
    stats = collections.Counter()
    for piece in words:
        for pair in zip(piece[:-1], piece[1:]):
            stats[pair] += 1

    most_common_pair = max(stats, key=lambda x: stats[x])
    token_bytes = most_common_pair[0] + most_common_pair[1]
    token = len(ranks)
    ranks[token_bytes] = token

    # step 2: 用新添加的字节 pair 对 words 合并
    new_words = []
    for word in words:
        new_word = []
        i = 0
        while i < len(word) - 1:
            if (word[i], word[i + 1]) == most_common_pair:
                # We found our pair! Merge it
                new_word.append(token_bytes)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        if i == len(word) - 1:
            new_word.append(word[i])
        new_words.append(new_word)
    words = new_words
```

**推理**

```python
def encode(self, text: str, visualise: Optional[str] = "colour") -> list[int]:
    words = self._pat.findall(text)
    tokens = []
    for word in words:
        word_bytes = word.encode("utf-8")
        word_tokens = bpe_encode(self.mergeable_ranks, word_bytes)
        tokens.extend(word_tokens)
    return tokens

def bpe_encode(mergeable_ranks: dict[bytes, int], input: bytes) -> list[int]:
    parts = [bytes([b]) for b in input]
    while True:
        # 每次进入都只会合并一次
        min_idx = None
        min_rank = None
        for i, pair in enumerate(zip(parts[:-1], parts[1:])):
            rank = mergeable_ranks.get(pair[0] + pair[1])
            # 这里 rank < min_rank 的作用是保证每次合并的是 rank 最小的 pair (rank 越小表示这是 train 过程越早发现的 pair)
            if rank is not None and (min_rank is None or rank < min_rank):
                min_idx = i
                min_rank = rank

        if min_rank is None:
            break
        assert min_idx is not None

        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2 :]

    tokens = [mergeable_ranks[part] for part in parts]
    return tokens
```

关于 `rank < min_rank` 的进一步解释

```python
mergeable_ranks = {k.encode("utf-8"): v for k, v in {"a": 1, "b": 2, "c": 3, "bc": 89, "ab": 100}.items()}
bpe_encode(mergeable_ranks, 'abc'.encode('utf-8'))  # [1, 89]
# 注意这里有两种合并规则: ['a', 'bc'] 和 ['ab', 'c'], 由于 {'bc': 89, 'ab': 100}, 所以 'bc' 更优先 
```

### tiktoken 的 Rust 实现 (TODO)

- [https://github.com/youkaichao/fast_bpe_tokenizer](https://github.com/youkaichao/fast_bpe_tokenizer): 这个仓库包含了一些对 tiktoken 的探索, 但注意无法保证作者的行文是否是统一的 (很可能像本文一样前后不一致)
- 上一节是 `tiktoken._education` 实现的精确描述, 但 Rust 实现不确定是否与它完全一致

encode 的核心代码如下, 笔者不熟悉 Rust, 只能尝试注解, 尽量解释成 Python 用户能理解

```rust
type Rank = u32;

fn _byte_pair_merge(ranks: &HashMap<Vec<u8>, Rank>, piece: &[u8]) -> Vec<(usize, Rank)> {
    // 这里的 ranks 对应于 Python 中的 Dict[bytes, int], 表示前文中的 mergeable_ranks

    // This is a vector of (start, rank).
    // The rank is of the pair starting at position start.
    let mut parts = Vec::with_capacity(piece.len() + 1);

    // Note that we hash bytes when indexing into `ranks`, not token pairs. As long as we train BPE
    // the way we currently do, this is equivalent. An easy way to break this would be to decouple
    // merge priority from token index or to prevent specific token merges.
    let mut min_rank: (Rank, usize) = (Rank::MAX, usize::MAX);
    for i in 0..piece.len() - 1 {
        // piece[i..i+2] 就表示取相邻的两个 byte, 如果找不到, rank=Rank::MAX, 否则返回对应的 rank
        let rank = *ranks.get(&piece[i..i + 2]).unwrap_or(&Rank::MAX);
        if rank < min_rank.0 {
            min_rank = (rank, i);
        }
        parts.push((i, rank));
    }
    parts.push((piece.len() - 1, Rank::MAX));
    parts.push((piece.len(), Rank::MAX));
    // 至此, parts 是一个列表: [(1, 500), (5, 400)] + [(9, Rank::MAX), (10, Rank::MAX)]
    // 代表 parts[1:3] 和 parts[5:7] 在 ranks 中出现, 且对应的 token id 为 500 及 400
    // 而 min_rank = (400, 5) 代表应该最优先合并的相邻字节
    // 这里假设 pieces 的长度为 10, 即 10 个字节

    // get_rank 是一个闭包(类似于局部函数), |parts: &Vec<(usize, Rank)>, i: usize| 是这个局部函数的输入
    let get_rank = {
        #[inline(always)]
        |parts: &Vec<(usize, Rank)>, i: usize| {
            if (i + 3) < parts.len() {
                // Similar to `piece[i..i + 2]` above. The +3 is because we haven't yet deleted
                // parts[i + 1], see comment in the main loop.
                *ranks
                    .get(&piece[parts[i].0..parts[i + 3].0])
                    .unwrap_or(&Rank::MAX)
            } else {
                Rank::MAX
            }
        }
    };

    // 这里是上面所谓的 main loop
    // If you have n parts and m merges, this does O(mn) work.
    // We could do something with a heap and do O(m log n) work.
    // n is often very small so considerations like cache-locality outweigh the algorithmic
    // complexity downsides of the `parts` vector.
    while min_rank.0 != Rank::MAX {
        let i = min_rank.1;
        // Update parts[i] and parts[i - 1] before removing parts[i + 1], since
        // `parts.remove(i + 1)` will thrash the cache.
        if i > 0 {
            parts[i - 1].1 = get_rank(&parts, i - 1);
        }
        parts[i].1 = get_rank(&parts, i);
        parts.remove(i + 1);

        min_rank = (Rank::MAX, usize::MAX);
        for (i, &(_, rank)) in parts[..parts.len() - 1].iter().enumerate() {
            if rank < min_rank.0 {
                min_rank = (rank, i);
            }
        }
    }
    parts
}
```

等价的 python 实现 (TODO)

```python
from typing import Dict, List, Tuple
def _byte_pair_merge(ranks: Dict[bytes, int], pieces: bytes):
    MAX_INT = int(1e8)
    parts: List[Tuple[int, int]] = []
    min_rank = (MAX_INT, MAX_INT)  # (rank, i)
    n = len(pieces)
    for i in range(n - 1):
        rank = ranks.get(pieces[i:i+1], MAX_INT)
        if rank < min_rank[0]:
            min_rank = (rank, i)
        parts.push((i, rank))
    parts.push(n-1, MAX_INT)
    parts.push(n, MAX_INT)

    def get_rank(parts, i):
        if (i+3) < len(parts):
            return MAX_INT
        return ranks.get(pieces[parts[i][0]:parts[i+3][0]], MAX_INT)

    while min_rank[0] != MAX_INT:
        i = min_rank[1]
        if i > 0:
            parts[i-1][1] = get_rank(parts, i-1)
        parts[i][1] = get_rank(parts, i)
        parts.pop(i+1)
        min_rank = (MAX_INT, MAX_INT)
        for (i, (_, rank)) in enumerate(parts[:len(parts)-1]):
            if rank < min_rank[0]:
                min_rank = (rank, 0)

    return parts
```
