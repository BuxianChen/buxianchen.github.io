

flash-attention ä½œä¸º bettertransformer çš„ä¸€éƒ¨åˆ†è¢«å®ç°åœ¨ pytorch ä¸­: `torch.nn.functional.scaled_dot_product_attention`

bettertransformer:
- [https://pytorch.org/blog/out-of-the-box-acceleration/](https://pytorch.org/blog/out-of-the-box-acceleration/)\
- [https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)

ğŸ¤— optimum ä¸­ç”¨è¿™æ ·çš„åšæ³•å°†æ¨¡å‹è¿›è¡Œè½¬æ¢

```python
from transformers import AutoModelForCausalLM
from optimum.bettertransformer import BetterTransformer
model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name).to("cuda:0")
bt_model = BetterTransformer.transform(model, keep_original_model=True)  # ä¸æ­£å¸¸çš„æ¨ç†è¿‡ç¨‹å”¯ä¸€å¢åŠ çš„ä¸€è¡Œ, å…¶ä½™åœ°æ–¹éƒ½ä¸åŠ¨
# input_ids, attention_mask= ...
model.generate(input_ids, attention_mask=masks, max_new_tokens=20, pad_token_id=model.config.eos_token_id)
```

å®ç°åŸç†å¦‚ä¸‹

```python
# optimum.bettertransformer.models.decoder_models.GPT2AttentionLayerBetterTransformer
class BetterTransformer:
    def transform(model, ...):
        # é€’å½’å‡½æ•°, æœ€ç»ˆæ‰§è¡Œçš„æ˜¯
        # bettertransformer_module = BetterTransformerManager.MODEL_MAPPING[config.model_type][target_class](module, config)
        # model._modules[name] = bettertransformer_module

        # è€Œ BetterTransformerManager.MODEL_MAPPING åŒ…å«:
        # "gpt2": {"GPT2Attention": GPT2AttentionLayerBetterTransformer}
    def reverse(...):
        ...

from transformer.models.gpt2 import GPT2Attention

# optimum.bettertransformer.models.base.BetterTransformerBaseLayer
class BetterTransformerBaseLayer:
    ...  # æ²¡æœ‰å¤ªå¤šæ¶‰åŠåˆ°forwardçš„ä¸œè¥¿

# optimum.bettertransformer.models.decoder_models.GPT2AttentionLayerBetterTransformer
class GPT2AttentionLayerBetterTransformer(BetterTransformerBaseLayer, GPT2Attention):
    _attn = gpt2_wrapped_scaled_dot_product  # æ­¤å¤„ä¸ºå…³é”®, å°† GPT2Attention ä¸­çš„ _attn å‡½æ•°æ›¿æ¢
    def forward(self, *args, **kwargs):
        super().forward_checker()
        return super().forward(*args, **kwargs)

def gpt2_wrapped_scaled_dot_product(...):  # è¿™ä¸ªå‡½æ•°ç”¨æ¥æ›¿æ¢ GPT2Attention._attn å‡½æ•°
    # æœ€ç»ˆä½¿ç”¨åˆ°äº† pytorch çš„ torch.nn.functional.scaled_dot_product_attention å‡½æ•°
```