---
layout: post
title: "(WIP) GPTQ 详解"
date: 2023-06-15 22:10:04 +0800
labels: [pytorch]
---

## 动机、参考资料、涉及内容

动机

- gptq 原理及源码解析: 理解一个具体的量化算法, 学习 torch cpp extension 的使用

参考资料

- 原始论文
- 原始代码仓库

涉及内容

- 原理解析及源码解析(涉及到关于torch的cpp的部分直接就地进行说明, 涉及到 CUDA C 的部分也直接就地说明)

不涉及内容

- 一般地量化方法介绍

## 前言

几个 repo 之间的联系

- gptq: 原始实现 [https://github.com/IST-DASLab/gptq](https://github.com/IST-DASLab/gptq)
- GPTQ-for-LLaMa: 应用于 Llama [https://github.com/qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa)
- AutoGPTQ: 更丰富的集成 [https://github.com/PanQiWei/AutoGPTQ.git](https://github.com/PanQiWei/AutoGPTQ.git)

本文主要研究原始实现 gptq

几篇论文间的关系, 参考 [知乎博客](https://zhuanlan.zhihu.com/p/616969812):

gptq 的原理来源于 OBQ, 而 OBQ 是对 OBS (一种经典的剪枝算法) 的魔改, 而 OBS 来源于 OBD (1990 年, 一作 Yann LeCun), 而 OBD 里更为详细的数学推导来源于 Yann LeCun 1987 年的博士论文(法文)

- [(2022.10) GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)
- [(2022.08) Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning](https://arxiv.org/abs/2208.11580v2)
- [(1993) Optimal Brain Surgeon and general network pruning](https://www.babak.caltech.edu/pubs/conferences/00298572.pdf)
- [(1990) Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf)
- [(1987) Modeles connexionnistes de l'apprentissage](https://www.persee.fr/doc/intel_0769-4113_1987_num_2_1_1804)

备注: GPTQ 与 OBQ 的第一作者是相同的

## 多元微积分回顾

### 局部偏导, 二阶偏导

考虑如下形式
$$ z = g(\mathbf{y}), y_i=f_i(\mathbf{x}) $$

备注: 考虑上述形式的动机是, $f_i$ 是某一个 layer 的计算, 向量 $\mathbf{x}$ 是这一层的输入, 向量 $\mathbf{y}$ 是这一层的输出, 标量 $z$ 是整个网络最终计算的 loss.

根据链式法则, $z$ 对 $x_j$ 的偏导数有如下计算公式

$$
\frac{\partial{z}}{\partial{x_j}}=\sum_{i}{\frac{\partial{g}}{\partial{y_i}}\frac{\partial{f_i}}{\partial{x_j}}}
$$

备注: 这条公式对于自动微分局部导数的概念的理解比较重要, 这里暂不做深究

而 $z$ 对 $\mathbf{x}$ 的二阶偏导数为

$$
\begin{align*}
\frac{\partial^2{z}}{\partial{x_j}\partial{x_k}} &= \sum_{i}{
  \frac{\partial{\frac{\partial{g}}{\partial{y_i}}}}{\partial{x_k}}\frac{\partial{f_i}}{\partial{x_j}}+
  \frac{\partial{g}}{\partial{y_i}}\frac{\partial{\frac{\partial{f_i}}{\partial{x_j}}}}{\partial{x_k}}
}\\
&=\sum_{i,l}{\frac{\partial{f_i}}{\partial{x_j}}\frac{\partial{f_l}}{\partial{x_k}}\frac{\partial^2{g}}{\partial{y_i}{y_l}}} + \sum_{i}{\frac{\partial{g}}{\partial{y_i}} \frac{\partial^2{f_i}}{\partial{x_j}{x_k}}}
\end{align*}
$$

**例子**

$$
\begin{align*}
z=g(\mathbf{y})&=y_1y_2^2, \\
y_1=f_1(\mathbf{x})&=\sin(x_1)+x_1+x_2, \\
y_2=f_2(\mathbf{x})&=x_1x_2
\end{align*}
$$

因此, 直接计算 $z$ 对 $\mathbf{x}$ 的一阶、二阶偏导数如下

$$
\begin{align*}
z&=(\sin(x_1)+x_1+x_2)x_1^2x_2^2 \\
\frac{\partial{z}}{\partial{x_1}} &= 2x_1x_2^2\sin(x_1)+x_1^2x_2^2\cos(x_1)+3x_1^2x_2^2+2x_1x_2^3 \\
\frac{\partial{z}}{\partial{x_2}} &= 2x_1^2x_2\sin(x_1)+2x_1^3x_2+3x_1^2x_2^2 \\
\frac{\partial^2{z}}{\partial{x_1^2}} &= 2x_2^2\sin(x_1)+4x_1x_2^2\cos(x_1)-x_1^2x_2^2\sin(x_1)+6x_1x_2^2+2x_2^3\\
\frac{\partial^2{z}}{\partial{x_2^2}} &= 2x_1^2\sin(x_1)+2x_1^3+6x_1^2x_2\\
\frac{\partial^2{z}}{\partial{x_1}\partial{x_2}} &= 4x_1x_2\sin(x_1)+2x_1^2x_2\cos(x_1)+6x_1^2x_2+6x_1x_2^2\\
\end{align*}
$$

采用局部导数的方式计算如下:

$$
\begin{align*}
\frac{\partial{z}}{\partial{y_1}} &=y_2^2, \frac{\partial{z}}{\partial{y_2}} =2y_1y_2, \frac{\partial^2{z}}{\partial{y_1^2}} =0, \frac{\partial^2{z}}{\partial{y_2^2}} =2y_1, \frac{\partial^2{z}}{\partial{y_1}\partial{y_2}} =2y_2 \\

\frac{\partial{y_1}}{\partial{x_1}} &=\cos(x_1)+1, \frac{\partial{y_1}}{\partial{x_2}}=1, \frac{\partial^2{y_1}}{\partial{x_1^2}} = -\sin(x_1), \frac{\partial^2{y_1}}{\partial{x_2^2}} = 0, \frac{\partial^2{y_1}}{\partial{x_1}\partial{x_2}}=0 \\

\frac{\partial{y_2}}{\partial{x_1}} &=x_2, \frac{\partial{y_2}}{\partial{x_2}}=x_1, \frac{\partial^2{y_2}}{\partial{x_1^2}} = 0, \frac{\partial^2{y_2}}{\partial{x_2^2}} = 0, \frac{\partial^2{y_2}}{\partial{x_1}\partial{x_2}}=1
\end{align*}
$$

因此

$$
\begin{align*}
\frac{\partial{z}}{\partial{x_1}}&={\frac{\partial{z}}{\partial{y_1}}\frac{\partial{y_1}}{\partial{x_1}}}+{\frac{\partial{z}}{\partial{y_2}}\frac{\partial{y_2}}{\partial{x_1}}} = y_2^2(\cos(x_1)+1)+2y_1y_2x_2 \\
\frac{\partial{z}}{\partial{x_2}}&={\frac{\partial{z}}{\partial{y_1}}\frac{\partial{y_1}}{\partial{x_2}}}+{\frac{\partial{z}}{\partial{y_2}}\frac{\partial{y_2}}{\partial{x_2}}} = y_2^2+2y_1y_2x_1 \\

\frac{\partial^2{z}}{\partial{x_1^2}}&=(
  \frac{\partial^2{z}}{\partial{y_1^2}}\frac{\partial{y_1}}{\partial{x_1}}\frac{\partial{y_1}}{\partial{x_1}}
  +\frac{\partial^2{z}}{\partial{y_2^2}}\frac{\partial{y_2}}{\partial{x_1}}\frac{\partial{y_2}}{\partial{x_1}}
  +2\frac{\partial^2{z}}{\partial{y_1}\partial{y_2}}\frac{\partial{y_1}}{\partial{x_1}}\frac{\partial{y_2}}{\partial{x_1}}
  )
  +(
  \frac{\partial{z}}{\partial{y_1}}\frac{\partial^2{y_1}}{\partial{x_1^2}}
  + \frac{\partial{z}}{\partial{y_2}}\frac{\partial^2{y_2}}{\partial{x_1^2}}
  ) \\
  &=0+2y_1x_2^2+4y_2x_2(\cos(x_1)+1)-y_2^2\sin(x_1)+0 \\

\frac{\partial^2{z}}{\partial{x_2^2}}&=(
  \frac{\partial^2{z}}{\partial{y_1^2}}\frac{\partial{y_1}}{\partial{x_2}}\frac{\partial{y_1}}{\partial{x_2}}
  +\frac{\partial^2{z}}{\partial{y_2^2}}\frac{\partial{y_2}}{\partial{x_2}}\frac{\partial{y_2}}{\partial{x_2}}
  +2\frac{\partial^2{z}}{\partial{y_1}\partial{y_2}}\frac{\partial{y_1}}{\partial{x_2}}\frac{\partial{y_2}}{\partial{x_2}}
  )
  +(
  \frac{\partial{z}}{\partial{y_1}}\frac{\partial^2{y_1}}{\partial{x_2^2}}
  +\frac{\partial{z}}{\partial{y_2}}\frac{\partial^2{y_2}}{\partial{x_2^2}}
  ) \\
  &=0+2y_1x_1^2+4y_2x_1+0+0 \\

\frac{\partial^2{z}}{\partial{x_1}\partial{x_2}}&=(
  \frac{\partial^2{z}}{\partial{y_1^2}}\frac{\partial{y_1}}{\partial{x_1}}\frac{\partial{y_1}}{\partial{x_2}}
  +\frac{\partial^2{z}}{\partial{y_1}\partial{y_2}}\frac{\partial{y_1}}{\partial{x_1}}\frac{\partial{y_2}}{\partial{x_2}}
  +\frac{\partial^2{z}}{\partial{y_2}\partial{y_1}}\frac{\partial{y_2}}{\partial{x_1}}\frac{\partial{y_1}}{\partial{x_2}}
  +\frac{\partial^2{z}}{\partial{y_2^2}}\frac{\partial{y_2}}{\partial{x_1}}\frac{\partial{y_2}}{\partial{x_2}}
  )
  +(
  \frac{\partial{z}}{\partial{y_1}}\frac{\partial^2{y_1}}{\partial{x_1}\partial{x_2}}
  +\frac{\partial{z}}{\partial{y_2}}\frac{\partial^2{y_2}}{\partial{x_1}\partial{x_2}}
  ) \\
  &=0+2y_2(\cos(x_1)+1)x_1+2y_2x_2+2y_1x_2x_1+0+2y_1y_2

\end{align*}
$$

可以验证两种计算方式结果一致

### 拉格朗日乘子法

TODO

## OBS: Optimal Brain Surgeon and general network pruning

此算法最初提出来用于做剪枝, 即将一个训练好的模型权重, 将其中的一部分置为 0 (同时也修改其余部分的参数值), 使得损失函数上升的尽可能地小.

假设模型权重为 $\mathbf{w}$, 假设参数总量为 $N$, 现在对 $\mathbf{w}$ 增加一个扰动 $\delta\mathbf{w}$, 那么损失 $E$ 的变化为:

$$
\delta E = (\frac{\partial{E}}{\partial{\mathbf{w}}})^T\delta\mathbf{w}+\frac{1}{2}(\delta\mathbf{w})^T\mathbf{H}\delta\mathbf{w}+O(\|\delta\mathbf{w}\|^3)
$$

我们假设参数已经优化完毕, 因此第一项的梯度都接近 0, 因此可以忽略, 第三项相比于第二项也可以忽略.

现在目标是希望**剪枝**, 即希望找到 $q\in\{1,...,N\}$ 以及最优的 $\delta\mathbf{w}$, 使得 $[\delta\mathbf{w}]_q=-[\mathbf{w}]_q$.

为此, 我们首先固定 $q$, 找到最优的 $\delta\mathbf{w}$, 即需要求解如下优化问题:

$$
\begin{align*}
&\min_{\delta\mathbf{w}}{\frac{1}{2}(\delta\mathbf{w})^T\mathbf{H}\delta\mathbf{w}}\\
s.t.\quad &e_q^T\delta\mathbf{w}+w_q=0
\end{align*}
$$

求解过程如下, 构造拉格朗日函数

$$
L(\delta\mathbf{w},\lambda)=\frac{1}{2}(\delta\mathbf{w})^T\mathbf{H}\delta\mathbf{w}+\lambda(\mathbf{e}_q^T\delta\mathbf{w}+w_q)
$$

因此需要求解如下方程组

$$
\begin{align*}
\mathbf{H}\delta{\mathbf{w}}+\lambda\mathbf{e}_q&=0\\
\mathbf{e}_q^T\delta\mathbf{w}+w_q&=0
\end{align*}
$$

由此计算可得:

$$
\begin{align*}
\delta\mathbf{w}&=-\mathbf{H}^{-1}\frac{w_q}{[H^{-1}]_{qq}}\mathbf{e}_q \\
L_{min}&=\frac{1}{2}\frac{w_q^2}{[H^{-1}]_{qq}}
\end{align*}
$$

因此 OBS 算法框架如下:

1. 首先对网络进行训练, 得到优化好的权重
2. 计算海塞矩阵的逆 $H^{-1}$
3. 根据 $L_{min}=\frac{1}{2}\frac{w_q^2}{[H^{-1}]_{qq}}$ 找出最小的 $q$, 如果由此得到的损失较小, 则进行第 4 步操作, 然后回到算法第 2 步继续; 否则进行第 5 步
4. 使用 $\delta\mathbf{w}$ 更新权重
5. 结束, 此时可能需要将剪枝部分的参数固定为 0, 重新训练整个网络

OBS 算法继续探讨了上述算法框架的一些细节, 例如计算海塞矩阵的逆等等, 因 gptq 算法没有采用这部分优化内容, 此处不再赘述. 这里只简单列出上述算法的几个重要缺陷:

- 每剪枝一个参数就需要计算一次海塞矩阵, 计算量极大, 并且对于参数量较大的情形, 存储海塞矩阵的逆本身就不可行
- 上述过程需要保证海塞矩阵的正定性, 基本上也无法保证
- 算法最后需要固定住剪枝部分的权重为 0 的情况下重新训练整个网络, 计算消耗较大

## OBQ

我们将模型的量化问题重新定义为:

在训练好一个网络的权重后, 在给定一些校准样本的情况下, 对网络逐层进行权重的量化, 我们假设只对全连接层的权重进行量化, 即优化如下目标:

$$
\min_{\hat{W}_l}\|W_lX-\hat{W}_lX\|_2^2
$$

其中 $W_l$ 的形状为 $(d_{out}, d_{in})$, $X$ 为校准数据在第 $l$ 层的输入, 形状为 $(d_{in}, n)$, $n$ 为校准数据量, 我们的目标是对 $\hat{W}_l$ 增加“量化约束”(即每个元素的取值是被量化的), 使得上面定义的损失尽量小.

注意到, 在不进行量化时, 即 $\hat{W}_l=W_l$ 时, 损失为 0, 因此此时为极小值点, 因此采用 OBS 的算法策略的前提条件得到满足, 因此可以用 OBS 算法进行权重剪枝, 然而正如前面所指出的, 每次裁剪一个权重分量都需要重新计算一次海塞矩阵的逆, 代价极大, 这里简单分析一下具体的计算量: 海塞矩阵的形状为 $(d=d_{in}*d_{out}, d=d_{in}*d_{out})$,

- 时间复杂度:
  - 计算海塞矩阵的计算量
  - 计算逆需要的时间复杂度为 $O(d^3)$, 剪枝可能要进行 $O(n)$ 次 (例如减去 20% 的权重), 因此总的计算量为 $O(d^4)$
- 空间复杂度: 存储海塞矩阵的逆需要 $(O(d^2))$ 空间
- 具体例子:
  - bert-base: hidden_size=768, intermediate_size=3072, 因此上面描述的 $d$ 约为 $2\times10^6$
  - llama 7B: hidden_size=4096, 最大的矩阵乘法发生在 feed-forward 处, intermediate_size=11008, 因此上面描述的 $d$ 约为 $4\times10^7$

因此, OBQ 论文提出了改进算法【TODO】


## GPTQ

本节正式介绍 GPTQ 的原理, 首先对问题定义做一个正式的描述, GPTQ 量化算法属于所谓的 post-train 的量化算法, 并且量化算法采用逐层的方式进行. 论文的亮点在于:

- 能对 100B 规模的模型例如 OPT, BLOOM, LLAMA 进行有效的量化, 且量化精度可以达到 int4 甚至 int3, 举例而言, 假设 170B 的模型使用 int4 进行存储, 则只需要 83GB (以1024为基数) 的存储空间, 如果使用 int3 进行存储则只需要 64GB 的存储空间, 那么在显存为 80GB 的 A100 上使用单张显卡就可以运行.
- 量化算法运行时间短, 论文声称能用大约 4 小时对 170B 参数量的模型进行量化.
- 注意: 该论文对 int4/int3 的模型的推理过程并没有太多的优化, 因此运行速度上并没有得到太多提升. 该论文的重点在于怎样得到量化权重, 代码中也实现了量化后模型的推理, 但没有做太多优化, 因为现阶段硬件层面, 不像 float16, bf16, int8 等数据类型那样, 英伟达显卡没有int3/int4运算的优化, 甚至于不支持 int3/int4 数据类型, 在 GPTQ 的代码实现中int3的存储时借用int32来实现的.

## vecquant3matmul

```c++
void vecquant3matmul_cuda(
  torch::Tensor vec, torch::Tensor mat, torch::Tensor mul,
  torch::Tensor scales, torch::Tensor zeros
);
```

- `vec`: shape (1, in_feat), dtype float32/float16
- `mat`: shape (in_feat/32*3, out_feat), dtype int32, 实际表示的是 3bit 整数
- `mul`: shape (out_feat,), dtype float32/float16, bias, **最终结果**累加到 `mul` 上
- `scales`: shape (out_feat,), dtype float32/float16, 量化的 scale 因子
- `zeros`: shape (out_feat,), dtype float32/float16, 量化零点的浮点数表示

这里对 `mat` 进行进一步的说明, 假设原始未量化的权重为 shape (in_feat, out_feat), dtype int32, 量化后(如果是 per-channel, 则表示对每一列分别进行 3bit int 量化)得到的权重矩阵为 shape (in_feat, out_feat), dtype int32, 实际取值为 `[0, 7]`, 之后按如下方式将其压缩存入 `mat` 中: (in_feat/32*3, out_feat)

```python
# 量化后的权重, in_feat=32, out_feat=1
weight = np.array([
    [1, 3, 5, 7, 0, 1, 6, 1, 1, 0,  # 001 010 101 111 000 001 110 001 001 000
     2, 1, 3, 4, 3, 5, 1, 0, 3, 5,  # 010 001 011 100 011 101 001 000 011 101
     1, 4, 5, 7, 0, 0, 4, 5, 1, 7,  # 001 100 101 111 000 000 100 101 001 111
     2, 5]                          # 010 101
    ], dtype=np.int32).T

# 10 000 001 001 110 001 000 111 101 010 001  # 第10个的最后2位放在最高位, 第9个放在次高位, ..., 第0个放在最低位
# 0 001 101 011 000 001 101 011 100 011 001 0  # 第10个的最高位放在最低位, 其余同理
# 101 010 111 001 101 100 000 000 111 101 10

# 按3bit进行压缩存储
mat = np.array([[
    0b10000001001110001000111101010001,
    0b00011010110000011010111000110010,
    0b10101011100110110000000011110110
]], dtype=np.int32).T  # 这里源码中是torch.int32存储的(torch没有uint32类型)
```

用法如下 `Quant3Linear.forward`


下面看具体的实现（此处源码理解后再进行一定的删减）

```c++
// quant_cuda_kernel.cu
#include <torch/all.h>
#include <torch/python.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>

const int BLOCKWIDTH  = 256;
const int BLOCKHEIGHT =  24;

void vecquant3matmul_cuda(
  torch::Tensor vec,
  torch::Tensor mat,
  torch::Tensor mul,
  torch::Tensor scales,
  torch::Tensor zeros
) {
  int height = mat.size(0);
  int width = mat.size(1);

  dim3 blocks(
    (height + BLOCKHEIGHT - 1) / BLOCKHEIGHT,
    (width + BLOCKWIDTH - 1) / BLOCKWIDTH
  );
  dim3 threads(BLOCKWIDTH);

  AT_DISPATCH_FLOATING_TYPES(
    vec.type(), "vecquant3matmul_cuda", ([&] {
      VecQuant3MatMulKernel<<<blocks, threads>>>(
        vec.data<scalar_t>(), mat.data<int>(), mul.data<scalar_t>(),
        scales.data<scalar_t>(), zeros.data<scalar_t>(),
        height, width
      );
    })
  );
}

template <typename scalar_t>
__global__ void VecQuant3MatMulKernel(
    const  scalar_t* __restrict__ vec,
    const       int* __restrict__ mat,
           scalar_t* __restrict__ mul,
    const  scalar_t* __restrict__ scales,
    const  scalar_t* __restrict__ zeros,
    int height,
    int width
) {
  int row = BLOCKHEIGHT * blockIdx.x;
  int col =  BLOCKWIDTH * blockIdx.y + threadIdx.x;

  __shared__ scalar_t blockvec[BLOCKWIDTH];
  blockvec[threadIdx.x] = vec[(row / BLOCKHEIGHT) * BLOCKWIDTH + threadIdx.x];
  __syncthreads();

  scalar_t scale = scales[col];
  scalar_t zero = zeros[col];

  scalar_t res = 0;
  int i = width * row + col;
  int k = 0;

  unsigned int tmp1;
  unsigned int tmp2;
  unsigned int tmp;

  while (k < BLOCKWIDTH) {
    tmp1 = as_unsigned(mat[i]);
    res += (scale * scalar_t((tmp1 >>  0) & 0x7) - zero) * blockvec[k + 0];
    res += (scale * scalar_t((tmp1 >>  3) & 0x7) - zero) * blockvec[k + 1];
    res += (scale * scalar_t((tmp1 >>  6) & 0x7) - zero) * blockvec[k + 2];
    res += (scale * scalar_t((tmp1 >>  9) & 0x7) - zero) * blockvec[k + 3];
    res += (scale * scalar_t((tmp1 >> 12) & 0x7) - zero) * blockvec[k + 4];
    res += (scale * scalar_t((tmp1 >> 15) & 0x7) - zero) * blockvec[k + 5];
    res += (scale * scalar_t((tmp1 >> 18) & 0x7) - zero) * blockvec[k + 6];
    res += (scale * scalar_t((tmp1 >> 21) & 0x7) - zero) * blockvec[k + 7];
    res += (scale * scalar_t((tmp1 >> 24) & 0x7) - zero) * blockvec[k + 8];
    res += (scale * scalar_t((tmp1 >> 27) & 0x7) - zero) * blockvec[k + 9];
    i += width;
    tmp2 = as_unsigned(mat[i]);
    tmp = (tmp1 >> 30) | ((tmp2 << 2) & 0x4);
    tmp2 >>= 1;
    res += (scale * scalar_t(tmp) - zero) * blockvec[k + 10];
    k += 11;
    res += (scale * scalar_t((tmp2 >>  0) & 0x7) - zero) * blockvec[k + 0];
    res += (scale * scalar_t((tmp2 >>  3) & 0x7) - zero) * blockvec[k + 1];
    res += (scale * scalar_t((tmp2 >>  6) & 0x7) - zero) * blockvec[k + 2];
    res += (scale * scalar_t((tmp2 >>  9) & 0x7) - zero) * blockvec[k + 3];
    res += (scale * scalar_t((tmp2 >> 12) & 0x7) - zero) * blockvec[k + 4];
    res += (scale * scalar_t((tmp2 >> 15) & 0x7) - zero) * blockvec[k + 5];
    res += (scale * scalar_t((tmp2 >> 18) & 0x7) - zero) * blockvec[k + 6];
    res += (scale * scalar_t((tmp2 >> 21) & 0x7) - zero) * blockvec[k + 7];
    res += (scale * scalar_t((tmp2 >> 24) & 0x7) - zero) * blockvec[k + 8];
    res += (scale * scalar_t((tmp2 >> 27) & 0x7) - zero) * blockvec[k + 9];
    i += width;
    tmp1 = as_unsigned(mat[i]);
    tmp = (tmp2 >> 30) | ((tmp1 << 1) & 0x6);
    tmp1 >>= 2;
    res += (scale * scalar_t(tmp) - zero) * blockvec[k + 10];
    k += 11;
    res += (scale * scalar_t((tmp1 >>  0) & 0x7) - zero) * blockvec[k + 0];
    res += (scale * scalar_t((tmp1 >>  3) & 0x7) - zero) * blockvec[k + 1];
    res += (scale * scalar_t((tmp1 >>  6) & 0x7) - zero) * blockvec[k + 2];
    res += (scale * scalar_t((tmp1 >>  9) & 0x7) - zero) * blockvec[k + 3];
    res += (scale * scalar_t((tmp1 >> 12) & 0x7) - zero) * blockvec[k + 4];
    res += (scale * scalar_t((tmp1 >> 15) & 0x7) - zero) * blockvec[k + 5];
    res += (scale * scalar_t((tmp1 >> 18) & 0x7) - zero) * blockvec[k + 6];
    res += (scale * scalar_t((tmp1 >> 21) & 0x7) - zero) * blockvec[k + 7];
    res += (scale * scalar_t((tmp1 >> 24) & 0x7) - zero) * blockvec[k + 8];
    res += (scale * scalar_t((tmp1 >> 27) & 0x7) - zero) * blockvec[k + 9];
    i += width;
    k += 10;
  }

  atomicAdd(&mul[col], res);
}
```