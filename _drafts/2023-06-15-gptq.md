---
layout: post
title: "(WIP) GPTQ 详解"
date: 2023-06-15 22:10:04 +0800
labels: [pytorch]
---

## 动机、参考资料、涉及内容

动机

- gptq 原理及源码解析: 理解一个具体的量化算法, 学习 torch cpp extension 的使用

参考资料

- 原始论文
- 原始代码仓库

涉及内容

- 原理解析及源码解析(涉及到关于torch的cpp的部分直接就地进行说明, 涉及到 CUDA C 的部分也直接就地说明)

不涉及内容

- 一般地量化方法介绍

## vecquant3matmul

```c++
void vecquant3matmul_cuda(
  torch::Tensor vec, torch::Tensor mat, torch::Tensor mul,
  torch::Tensor scales, torch::Tensor zeros
);
```

- `vec`: shape (1, in_feat), dtype float32/float16
- `mat`: shape (in_feat/32*3, out_feat), dtype int32, 实际表示的是 3bit 整数
- `mul`: shape (out_feat,), dtype float32/float16, bias, **最终结果**累加到 `mul` 上
- `scales`: shape (out_feat,), dtype float32/float16, 量化的 scale 因子
- `zeros`: shape (out_feat,), dtype float32/float16, 量化零点的浮点数表示

这里对 `mat` 进行进一步的说明, 假设原始未量化的权重为 shape (in_feat, out_feat), dtype int32, 量化后(如果是 per-channel, 则表示对每一列分别进行 3bit int 量化)得到的权重矩阵为 shape (in_feat, out_feat), dtype int32, 实际取值为 `[0, 7]`, 之后按如下方式将其压缩存入 `mat` 中: (in_feat/32*3, out_feat)

```python
# 量化后的权重, in_feat=32, out_feat=1
weight = np.array([
    [1, 3, 5, 7, 0, 1, 6, 1, 1, 0,  # 001 010 101 111 000 001 110 001 001 000
     2, 1, 3, 4, 3, 5, 1, 0, 3, 5,  # 010 001 011 100 011 101 001 000 011 101
     1, 4, 5, 7, 0, 0, 4, 5, 1, 7,  # 001 100 101 111 000 000 100 101 001 111
     2, 5]                          # 010 101
    ], dtype=np.int32).T

# 10 000 001 001 110 001 000 111 101 010 001  # 第10个的最后2位放在最高位, 第9个放在次高位, ..., 第0个放在最低位
# 0 001 101 011 000 001 101 011 100 011 001 0  # 第10个的最高位放在最低位, 其余同理
# 101 010 111 001 101 100 000 000 111 101 10

# 按3bit进行压缩存储
mat = np.array([[
    0b10000001001110001000111101010001,
    0b00011010110000011010111000110010,
    0b10101011100110110000000011110110
]], dtype=np.int32).T  # 这里源码中是torch.int32存储的(torch没有uint32类型)
```

用法如下 `Quant3Linear.forward`


下面看具体的实现（此处源码理解后再进行一定的删减）

```c++
// quant_cuda_kernel.cu
#include <torch/all.h>
#include <torch/python.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>

const int BLOCKWIDTH  = 256;
const int BLOCKHEIGHT =  24;

void vecquant3matmul_cuda(
  torch::Tensor vec,
  torch::Tensor mat,
  torch::Tensor mul,
  torch::Tensor scales,
  torch::Tensor zeros
) {
  int height = mat.size(0);
  int width = mat.size(1);

  dim3 blocks(
    (height + BLOCKHEIGHT - 1) / BLOCKHEIGHT,
    (width + BLOCKWIDTH - 1) / BLOCKWIDTH
  );
  dim3 threads(BLOCKWIDTH);

  AT_DISPATCH_FLOATING_TYPES(
    vec.type(), "vecquant3matmul_cuda", ([&] {
      VecQuant3MatMulKernel<<<blocks, threads>>>(
        vec.data<scalar_t>(), mat.data<int>(), mul.data<scalar_t>(),
        scales.data<scalar_t>(), zeros.data<scalar_t>(),
        height, width
      );
    })
  );
}

template <typename scalar_t>
__global__ void VecQuant3MatMulKernel(
    const  scalar_t* __restrict__ vec,
    const       int* __restrict__ mat,
           scalar_t* __restrict__ mul,
    const  scalar_t* __restrict__ scales,
    const  scalar_t* __restrict__ zeros,
    int height,
    int width
) {
  int row = BLOCKHEIGHT * blockIdx.x;
  int col =  BLOCKWIDTH * blockIdx.y + threadIdx.x;

  __shared__ scalar_t blockvec[BLOCKWIDTH];
  blockvec[threadIdx.x] = vec[(row / BLOCKHEIGHT) * BLOCKWIDTH + threadIdx.x];
  __syncthreads();

  scalar_t scale = scales[col];
  scalar_t zero = zeros[col];

  scalar_t res = 0;
  int i = width * row + col;
  int k = 0;

  unsigned int tmp1;
  unsigned int tmp2;
  unsigned int tmp;

  while (k < BLOCKWIDTH) {
    tmp1 = as_unsigned(mat[i]);
    res += (scale * scalar_t((tmp1 >>  0) & 0x7) - zero) * blockvec[k + 0];
    res += (scale * scalar_t((tmp1 >>  3) & 0x7) - zero) * blockvec[k + 1];
    res += (scale * scalar_t((tmp1 >>  6) & 0x7) - zero) * blockvec[k + 2];
    res += (scale * scalar_t((tmp1 >>  9) & 0x7) - zero) * blockvec[k + 3];
    res += (scale * scalar_t((tmp1 >> 12) & 0x7) - zero) * blockvec[k + 4];
    res += (scale * scalar_t((tmp1 >> 15) & 0x7) - zero) * blockvec[k + 5];
    res += (scale * scalar_t((tmp1 >> 18) & 0x7) - zero) * blockvec[k + 6];
    res += (scale * scalar_t((tmp1 >> 21) & 0x7) - zero) * blockvec[k + 7];
    res += (scale * scalar_t((tmp1 >> 24) & 0x7) - zero) * blockvec[k + 8];
    res += (scale * scalar_t((tmp1 >> 27) & 0x7) - zero) * blockvec[k + 9];
    i += width;
    tmp2 = as_unsigned(mat[i]);
    tmp = (tmp1 >> 30) | ((tmp2 << 2) & 0x4);
    tmp2 >>= 1;
    res += (scale * scalar_t(tmp) - zero) * blockvec[k + 10];
    k += 11;
    res += (scale * scalar_t((tmp2 >>  0) & 0x7) - zero) * blockvec[k + 0];
    res += (scale * scalar_t((tmp2 >>  3) & 0x7) - zero) * blockvec[k + 1];
    res += (scale * scalar_t((tmp2 >>  6) & 0x7) - zero) * blockvec[k + 2];
    res += (scale * scalar_t((tmp2 >>  9) & 0x7) - zero) * blockvec[k + 3];
    res += (scale * scalar_t((tmp2 >> 12) & 0x7) - zero) * blockvec[k + 4];
    res += (scale * scalar_t((tmp2 >> 15) & 0x7) - zero) * blockvec[k + 5];
    res += (scale * scalar_t((tmp2 >> 18) & 0x7) - zero) * blockvec[k + 6];
    res += (scale * scalar_t((tmp2 >> 21) & 0x7) - zero) * blockvec[k + 7];
    res += (scale * scalar_t((tmp2 >> 24) & 0x7) - zero) * blockvec[k + 8];
    res += (scale * scalar_t((tmp2 >> 27) & 0x7) - zero) * blockvec[k + 9];
    i += width;
    tmp1 = as_unsigned(mat[i]);
    tmp = (tmp2 >> 30) | ((tmp1 << 1) & 0x6);
    tmp1 >>= 2;
    res += (scale * scalar_t(tmp) - zero) * blockvec[k + 10];
    k += 11;
    res += (scale * scalar_t((tmp1 >>  0) & 0x7) - zero) * blockvec[k + 0];
    res += (scale * scalar_t((tmp1 >>  3) & 0x7) - zero) * blockvec[k + 1];
    res += (scale * scalar_t((tmp1 >>  6) & 0x7) - zero) * blockvec[k + 2];
    res += (scale * scalar_t((tmp1 >>  9) & 0x7) - zero) * blockvec[k + 3];
    res += (scale * scalar_t((tmp1 >> 12) & 0x7) - zero) * blockvec[k + 4];
    res += (scale * scalar_t((tmp1 >> 15) & 0x7) - zero) * blockvec[k + 5];
    res += (scale * scalar_t((tmp1 >> 18) & 0x7) - zero) * blockvec[k + 6];
    res += (scale * scalar_t((tmp1 >> 21) & 0x7) - zero) * blockvec[k + 7];
    res += (scale * scalar_t((tmp1 >> 24) & 0x7) - zero) * blockvec[k + 8];
    res += (scale * scalar_t((tmp1 >> 27) & 0x7) - zero) * blockvec[k + 9];
    i += width;
    k += 10;
  }

  atomicAdd(&mul[col], res);
}
```