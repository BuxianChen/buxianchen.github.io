---
layout: post
title: "(WIP)自动微分"
date: 2022-06-25 19:31:04 +0800
---

## 动机、参考资料、涉及内容

直接动机：

- CTC 是怎么计算梯度的

其他动机：

- 按CS231N中描述的，反向传播算法里存在由前向后与由后向前两种方式，与tape这个东西有关，tensorflow的API就明确有 `tf.GradientTape` 这个东西，因此有必要研究清楚
- [micrograd](https://github.com/karpathy/micrograd.git) 与 [tinygrad](https://github.com/geohot/tinygrad.git) 这两个 Github 项目有实现一套自动微分框架，需要研究一下
- 卷积操作的反向传播也是卷积操作？需要仔细推导一次

不涉及的内容

- 卷积操作的几十种快速计算办法以及`torch.backends.cudnn.benchmark=True`时具体怎么寻找最优的卷积算法

扩展内容（估计无法做到）
- Pytorch/Tensorflow/Oneflow在自动微分上的对比

参考资料【待整理阅读顺序】

- [pytorch/tutorials/beginner/autograd_tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
- 上一篇 tutorial 文末 further-readings 提及到的一个 colab notebook: [simple autograd](https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC#scrollTo=sAApdEb27KTA)
- 上一个 colab notebook 提及到的一个博客: [https://sidsite.com/posts/autodiff/](https://sidsite.com/posts/autodiff/)
- 陈天奇的 B 站课程: [深度学习系统](https://b23.tv/cmi9N66)
- 自定义算子相关【待定】

## 链式法则

$n$ 个输入, $m$ 个输出, 定义 Jacobean 矩阵为 (形状为 $m\times n$)

$$
J = \begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
\vdots & \ddots &\vdots \\
\frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}=
\begin{bmatrix}
\nabla^T {y_1} \\
\vdots \\
\nabla^T {y_m} \\
\end{bmatrix}
$$

链式法则可以写为

$$
\nabla \mathbf{x}=
\begin{bmatrix}
\frac{\partial L}{\partial x_1} \\
\vdots \\
\frac{\partial L}{\partial x_n} \\
\end{bmatrix}
= 
J^T
\begin{bmatrix}
\frac{\partial L}{\partial x_1} \\
\vdots \\
\frac{\partial L}{\partial x_n}
\end{bmatrix}
=J^T(\nabla \mathbf{y})
$$

## Simple Autograd 注解

本节是对 [simple autograd](https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC#scrollTo=sAApdEb27KTA) 这个 colab notebook 的一个注解

`torch/csrc/jit/runtime/autodiff.cpp`

## 陈天奇课程记录与实现