---
layout: post
title: "(WIP)自动微分"
date: 2022-06-25 19:31:04 +0800
---

## 动机、参考资料、涉及内容

直接动机：

- CTC 是怎么计算梯度的【大概最终不涉及】

其他动机：

- 按 CS231N 中描述的，反向传播算法里存在由前向后与由后向前两种方式，与 tape 这个东西有关，tensorflow的API就明确有 `tf.GradientTape` 这个东西，因此有必要研究清楚
- [micrograd](https://github.com/karpathy/micrograd.git) 与 [tinygrad](https://github.com/geohot/tinygrad.git) 这两个 Github 项目有实现一套自动微分框架，需要研究一下
- 卷积操作的反向传播也是卷积操作？需要仔细推导一次【这个在陈天奇的课程中有介绍】

参考资料

**<span style="color: red">本文的内容主要是对下面参考资料的“注解”</span>**

- pytorch 文档:
  - [pytorch/tutorials/beginner/autograd_tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
  - 上一篇 tutorial 文末 further-readings 提及到的一个 colab notebook: [simple autograd](https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC#scrollTo=sAApdEb27KTA)
- 上一个 colab notebook 提及到的一个 Autodiff 博客: [https://sidsite.com/posts/autodiff/](https://sidsite.com/posts/autodiff/)
- 陈天奇课程 (dlsyscourse):
  - B站视频(仅有一部分): [https://b23.tv/cmi9N66](https://b23.tv/cmi9N66), 完整视频及资料可参考[课程大纲](https://dlsyscourse.org/lectures/)
  - 课程主页: [http://dlsyscourse.org](http://dlsyscourse.org)
  - 课程配套 Github 项目: [https://github.com/dlsyscourse/](https://github.com/dlsyscourse/)
- micrograd 项目: [https://github.com/karpathy/micrograd.git](https://github.com/karpathy/micrograd.git)
- tinygrad 项目: [https://github.com/geohot/tinygrad.git](https://github.com/geohot/tinygrad.git)
- CS231N 作业中给出的自动微分框架代码 (作业中只要求写每个算子的局部导数计算, 自动微分框架是已经给出的)【待定】
- 自定义算子相关【待定】

## 链式法则【貌似没啥用】

$n$ 个输入, $m$ 个输出, 定义 Jacobean 矩阵为 (形状为 $m\times n$)

$$
J = \begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
\vdots & \ddots &\vdots \\
\frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}=
\begin{bmatrix}
\nabla^T {y_1} \\
\vdots \\
\nabla^T {y_m} \\
\end{bmatrix}
$$

假设: $\mathbf{z}=f(\mathbf{y}), y=g(\mathbf{x})$, 即: $\mathbf{z}=f(g(\mathbf{x}))$, 记 $\nabla\mathbf{x}=J(\mathbf{x}, \mathbf{z})$, 链式法则可以写为

$$
\nabla \mathbf{x}=
\begin{bmatrix}
\frac{\partial L}{\partial x_1} \\
\vdots \\
\frac{\partial L}{\partial x_n} \\
\end{bmatrix}
= 
J^T
\begin{bmatrix}
\frac{\partial L}{\partial x_1} \\
\vdots \\
\frac{\partial L}{\partial x_n}
\end{bmatrix}
=J^T(\nabla \mathbf{y})
$$

## Sum-Product 算法

<svg width="400" height="100">
    <circle cx="50" cy="50" r="20" style="stroke: black; fill: none" />
    <circle cx="150" cy="50" r="20" style="stroke: black; fill: none" />
    <line x1="70" y1="50" x2="130" y2="50" stroke="black" stroke-width="2" marker-end="url(#arrow)" />
</svg>

对于一个有向无环图, 我们把只有出边的节点称为叶子节点, 只有入边的节点称为根节点, 假设存在一条边 $v_i\to v_j$, 那么称 $v_j$ 是 $v_i$ 的父节点. 一般来说, 一个有向无环图中可以有多个叶子节点和多个根节点, 我们这里只讨论只有一个根节点的情形.

备注: 
- 注意一些概念上的区分, 【树】是一种特殊的【只有一个根节点的有向无环图】
- 【只有一个根节点的有向无环图】有一个性质是: 所有其他节点到根节点都至少存在一条路

我们在只有一个根节点的有向无环图上考虑一个这样的问题:

- 边的值: 假设每条边都有一个值, 例如边 $v_i\to v_j$ 上的值记作 $s_{i,j}$
- 根节点的值: 我们定义【根节点的值】为 $1$
- 到根节点的路的值: 【一个节点到根节点的一条路】的值被定义为其经过的所有边的值的乘积
- 其他节点的值: 【一个节点的值】定义为【其所有到根节点的路】的值之和

现在我们给定每条边的值, 需要按上面的定义计算每个节点的值. 后面我们将看到, 自动微分算法的本质就是这个过程.

## 原理

本节结合 [陈天奇课程](http://dlsyscourse.org) 的第 4 节课和 [Autodiff 博客](https://sidsite.com/posts/autodiff/) 做原理上的讲解


## 实现方案比较及阅读建议

- Autodiff: 支持高阶导数以及向量化(简易版), 可以说是最简实现(代码行数), 实现上没有显式使用拓扑排序, 而是采用了递归, 可能需要仔细揣摩.
- Micrograd: 不支持高阶导数, 支持向量化(简易版), 相当于 Autodiff 的简化版本, 实现上显式使用了拓扑排序, 便于理解.
- dlsyscourse: 完整框架, 支持高阶导数, 支持向量化(高效实现), 课程内容非常丰富, 适合认真学习, 获益很多.
- Simple Autograd: 支持高阶导数, 支持向量化, 还考虑了一些诸如自定义算子时的一些问题(初看时想了很久不得要领), 这部分内容不确定 dlsyscourse 里怎么涉及的
- Tinygrad: 待研究
- CS231N: 完成前面的内容后可以审视一下, 检验理解是否正确

## Python 高级特性【待定】

要完全理解具体的实现, 实际上需要对一些 Python 高级特性进行理解

- 函数内嵌套函数: 变量的作用域, 执行时变量的值
- 内置方法 `sum` 的执行逻辑
- 运算符重载: `__add__`, `__radd__`
- `__new__` 与 `__init__`: 目前感觉只是 dlsyscourse 里故弄玄虚?

## Autodiff

这里直接搬运了博客中的实现如下

<table style="width: 100%; table-layout: fixed;">
  <tr>
    <td style="width: 100%; word-wrap: break-word; padding=5px; border: 1px solid #ccc; vertical-align: top;"><div markdown="1">
```python
from collections import defaultdict

class Variable:
    def __init__(self, value, local_gradients=()):
        self.value = value
        self.local_gradients = local_gradients
    
def add(a, b):
    "Create the variable that results from adding two variables."
    value = a.value + b.value    
    local_gradients = (
        (a, 1),  # the local derivative with respect to a is 1
        (b, 1)   # the local derivative with respect to b is 1
    )
    return Variable(value, local_gradients)

def mul(a, b):
    "Create the variable that results from multiplying two variables."
    value = a.value * b.value
    local_gradients = (
        (a, b.value), # the local derivative with respect to a is b.value
        (b, a.value)  # the local derivative with respect to b is a.value
    )
    return Variable(value, local_gradients)

def get_gradients(variable):
    """ Compute the first derivatives of `variable` 
    with respect to child variables.
    """
    gradients = defaultdict(lambda: 0)
    
    def compute_gradients(variable, path_value):
        for child_variable, local_gradient in variable.local_gradients:
            # "Multiply the edges of a path":
            value_of_path_to_child = path_value * local_gradient
            # "Add together the different paths":
            gradients[child_variable] += value_of_path_to_child
            # recurse through graph:
            compute_gradients(child_variable, value_of_path_to_child)
    
    compute_gradients(variable, path_value=1)
    # (path_value=1 is from `variable` differentiated w.r.t. itself)
    return gradients
```
</div></td>
    <td style="width: 100%; word-wrap: break-word; padding=5px; border: 1px solid #ccc; vertical-align: top;"><div markdown="1">
使用:

```python
a = Variable(4)
b = Variable(3)
c = add(a, b) # = 4 + 3 = 7
d = mul(a, c) # = 4 * 7 = 28

gradients = get_gradients(d)

print('d.value =', d.value)
print("The partial derivative of d with respect to a =", gradients[a])
print('gradients[b] =', gradients[b])
print('gradients[c] =', gradients[c])
print('dict(d.local_gradients)[a] =', dict(d.local_gradients)[a])
print('dict(d.local_gradients)[c] =', dict(d.local_gradients)[c])
print('dict(c.local_gradients)[a] =', dict(c.local_gradients)[a])
print('dict(c.local_gradients)[b] =', dict(c.local_gradients)[b])
```
</div></td>
  </tr>
</table>


## Micrograd

## Simple Autograd 注解

本节是对 [simple autograd](https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC#scrollTo=sAApdEb27KTA) 这个 colab notebook 的一个注解

`torch/csrc/jit/runtime/autodiff.cpp`

对于这个 notebook 中作为示例的高阶导数, 这里先手动求出结果, 以便后续分析:

$$
\mathbf{a}=(a_1,\ldots,a_n), \mathbf{b}=(b_1,\ldots,b_n)\\
L_1=\sum_{i=1}^{n}(a_i+b_i)b_i\\
\frac{\partial L_1}{\partial \mathbf{a} }=\mathbf{b}, 
\frac{\partial L_1}{\partial \mathbf{b} }=\mathbf{a} + 2\mathbf{b}\\
L_2=\text{sum}[(\frac{\partial L_1}{\partial \mathbf{a} })^2+(\frac{\partial L_1}{\partial \mathbf{b} })^2]=\sum_{i=1}^{n}{[b_i^2+(a_i+2b_i)^2]} \\
\frac{\partial L_2}{\partial \mathbf{a} }=2\mathbf{a}+4\mathbf{b}, 
\frac{\partial L_2}{\partial \mathbf{b} }=4\mathbf{a}+10\mathbf{b}
$$

## tinygrad (早期版本)

在 tinygrad 最初的一些版本里, 例如: [17bf90db](https://github.com/tinygrad/tinygrad/commit/17bf90dbe42e0f44267ada6b5fe979a3bd539e90), tinygrad 宣称总代码行数将不超过 1000 行, 适合本文进行基本的原理研究

## tinygrad (近期版本)

近期 tinygrad 逐渐朝着一个真正的深度学习框架进行发展

```bash
git log -S"tinygrad will always be below 1000 lines" -- README.md

# 以下是输出内容
# commit 803587b8b42697fcc2f5ad920ea76d9cd716f666
# Author: George Hotz <geohot@gmail.com>
# Date:   Fri May 26 06:10:41 2023 +0000

#     update readme

# commit 2e7f16bf3f9b7b2934073acd3bae7f864c3b6ddf
# Author: George Hotz <geohot@gmail.com>
# Date:   Mon Nov 2 07:42:11 2020 -0800

#     the power of cheating

git show 803587b8b42697fcc2f5ad920ea76d9cd716f666
```

## 陈天奇课程记录与实现【待定】

## CS231N【待定】

留作 Code Review