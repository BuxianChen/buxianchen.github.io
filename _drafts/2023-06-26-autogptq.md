---
layout: post
title: "(WIP) Auto-GPTQ 浅析"
date: 2023-06-26 10:10:04 +0800
labels: [huggingface, repo]
---

## 动机、参考资料、涉及内容

动机

- Auto-GPTQ 源码浅析
  - 怎么写一个 python 包 (setup.py)
  - Github Action 做分发
  - torch cpp extension 的使用
  - OpenAI triton 的使用
  - 怎么基于 🤗 做二次开发的例子

参考资料

- 原始代码仓库: [https://github.com/PanQiWei/AutoGPTQ.git](https://github.com/PanQiWei/AutoGPTQ.git)

涉及内容

同动机

不涉及内容

torch cpp extension 及 OpenAI triton 的深度使用


## 项目目录结构

```
README.md
setup.py
docs/
examples/
.github/                            # 与 GitHub 与发布相关的
auto_gptq/                          # 安装的 python 包
    __init__.py
    eval_tasks/                     # 评估量化结果
    modeling/
        __init__.py
        _base.py                    # 核心类: BaseQuantizeConfig, BaseGPTQForCausalLM
        auto.py                     # 核心类: AutoGPTQForCausalLM
        llama.py                    # bloom.py, gptq2.py, ..., 继承自 BaseGPTQForCausalLM, 但只修改几个类属性
        ...
    nn_modules/                     # 待研究
        __init__.py
        _fused_base.py
        fused_gptj_attn.py
        fused_llama_attn.py
        fused_llama_mlp.py
        qlinear/
            __init__.py
            qlinear_cuda.py
            qlinear_cuda_old.py
            qlinear_triton.py
        triton_utils/
            __init__.py
            custom_autotune.py
            kernels.py
            mixin.py
    quantization/
        __init__.py
        gptq.py
        quantizer.py
    utils/
        ...
        peft_utils.py               # 🤗 Peft 的集成
autogptq_cuda/                      # Pytorch CUDAExtension 
    autogptq_cuda_256.cpp
    autogptq_cuda_64.cpp
    autogptq_cuda_kernel_256.cu
    autogptq_cuda_kernel_64.cu
```

## setup.py

根据官方文档的描述, 使用 `pip install` 的方式进行安装有如下几种选项

```bash
# 确认之前的安装被删除
pip uninstall autogptq_cuda -y

# 安装时使用 pytorch 编译 extension
pip install autogptq  # 等价与 BUILD_CUDA_EXT=1 pip install auto-gptq

# 安装时不编译 extension
BUILD_CUDA_EXT=0 pip install auto-gptq

# 额外安装 triton
pip install auto-gptq[triton]
```

上面的注释实际上有些“含糊”, 因此这里直接对 `setup.py` 进行分析, 以得到准确理解

```python
# 关键部分代码
common_setup_kwargs = {
    "python_requires": f">=3.8.0",                            # 强制检查, 否则安装报错
    # 以下只有在 BUILD_CUDA_EXT="1", 且 torch 存在时才有【待确认什么叫torch存在】
    "ext_modules": [
        cpp_extension.CUDAExtension(
            "autogptq_cuda_64",
            [
                "autogptq_cuda/autogptq_cuda_64.cpp",
                "autogptq_cuda/autogptq_cuda_kernel_64.cu"
            ]
        ),
        cpp_extension.CUDAExtension(
            "autogptq_cuda_256",
            [
                "autogptq_cuda/autogptq_cuda_256.cpp",
                "autogptq_cuda/autogptq_cuda_kernel_256.cu"
            ]
        )
    ],
    "cmdclass": {'build_ext': cpp_extension.BuildExtension}
}

include_dirs = [
    "autogptq_cuda"
    ""  # 【需要使用Pytorch extension时还会增加】
]

setup(
    packages=find_packages(),       # ["auto_gptq"]
    install_requires=requirements,  # 自动安装: ["accelerate>=0.19.0", "torch>=1.13.0", "transformers>=4.29.0", "peft", ...]
    extras_require=extras_require,  # 自动安装, 使用 pip install auto-gptq[triton] 才会触发 {"triton": ["triton>=2.0.0"]}
    include_dirs=include_dirs,      # 也放入 site-packages 目录内
    **common_setup_kwargs
)
```

关于 `extras_require` 与 `pip install xx[yy,zz]`: 【待确认并移入Notes中】

- 先判断 `"yy"` 是否是 `extras_require` 的键, 如果匹配上, 就安装 `extras_require["yy"]` 的包, 匹配不上就报一个警告, 然后进行下一步
- 然后判断 `yy` 是否为一个包名(PyPI), 如果匹配上, 就安装 `yy` 包
- 都匹配不上就不进行安装 `yy`

源码安装

```bash
pip install .[triton]
```


如果 Pytorch CUDA extension 被正确安装, 安装位置为
```
/path/to/site-packages/autogptq_cuda_256.cpython-38-x86_64-linux-gnu.so
/path/to/site-packages/autogptq_cuda_64.cpython-38-x86_64-linux-gnu.so
```


提示: 如果希望看出一些安装过程具体干了什么, 可以尝试使用 `pip install -v .`

通常不推荐使用: `python setup.py install`, 参考 [stackoverflow](https://stackoverflow.com/questions/15724093/difference-between-python-setup-py-install-and-pip-install)

```python
pip install -e -v .
# pip install . 相当于在 python setup.py install 之外还做了自动安装依赖包等工作
# -e 参数方便调试, 项目目录下的修改会自动生效
# -v 用于显示安装过程, 例如让 setup.py 文件中的 print 能正常生效
```

日志如下
```
# 以 # 号开头的是注释

Processing /abspath/to/AutoGPTQ
  Running command python setup.py egg_info
  ...  # 这一部分会打印 setup.py 中的 print 语句输出
  running egg_info
  # 创建一些临时目录, 并写文件
  writing /tmp/pip-pip-egg-info-eoiqxgek/auto_gptq.egg-info/PKG-INFO
  ...
  Prepareing metadata (setup.py) ... done
# 安装依赖包
Collecting accelerate>=0.19.0
...
Building wheels for collected packages: auto-gptq
  Running command python setup.py bdist_wheel
  running bdist_wheel
  running build
  running build_py
  # 拷贝文件到 build 目录
  create build/lib/auto_gptq
  copy auto_gptq/__init__.py -> build/lib/auto_gptq
  ...
  # 如果BUILD_CUDA_EXT=1(默认值), 且安装auto-gptq之前就已经安装了torch时会触发
  running build_ext
  ...
  running install
  running install_lib
  # 再拷贝一次
  copy build/lib/auto_gptq/__init__py -> build/bdist.linux-x86_64/wheel/auto_gptq
  ...
  running install_egg_info
  running egg_info
  # 疑似是写到 site-packages/auto_gptq.egg-info 中
  writing auto_gptq.egg-info/PKG-INFO
  ...
  running install_scripts
  creating build/bdist.linux-x86_64/wheel/auto_gptq-0.3.0.dev0.dist-info/WHEEL
  creating '/tmp/pip-wheel-nlkqzuc6/auto_gptq-0.3.0.dev0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
  adding 'auto_gptq/__init__.py'
  ...
  removing build/bdist.linux-x86_64/wheel
  Building wheel for auto-gptq (setup.py) ... done
  Created wheel for auto-gptq: filename=auto_gptq-0.3.0.dev0-py3-none-any.whl size=63666 sha256=12345163663
  Store in directory: /tmp/pip-ephem-wheel-cache-5yjlc9ij/wheels/36/9a/b2/12355772277
Successfully build auto-gptq
Installing collected packages: accelerate, auto-gptq
  # 上一行运行之后可能会卡住一段时间, 应该是把前面的临时文件夹进行拷贝
  # /tmp/pip-wheel-nlkqzuc6, /tmp/pip-ephem-wheel-cache-5yjlc9ij
  changing model of /path/to/python/bin/accelerate to 755
  changing model of /path/to/python/bin/accelerate-config to 755
  changing model of /path/to/python/bin/accelerate-launch to 755
Successfully installed accelerate-0.19.0 auto-gptq-0.3.0-dev0
```

备注: `/tmp` 目录的使用实际上还有更多, 但最终会被清理掉
```
pip-build-tracker-xxxx
pip-ephem-wheel-cahce-xxxx
pip-install-xxxx
pip-pip-egg-info-xxxx
pip-unpack-xxxx
pip-wheel-xxxx
```



### CUDA extension 安装可能报错的问题

```
error identifier "__hfma2" is undefined
```

nvcc 怎么确定类似这种参数 `-gencode=arch=compute_70,code=sm_70`, 参考:

- [博客](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/)
- [issue](https://github.com/PanQiWei/AutoGPTQ/issues/67#issuecomment-1577775096)
- [问答](https://stackoverflow.com/questions/68496906/pytorch-installation-for-different-cuda-architectures)

```
TORCH_CUDA_ARCH_LIST=7.0
/usr/local/cuda-11.7/bin/nvcc --list-gpu-arch
```


