---
layout: post
title: "(Alpha) Auto-GPTQ æµ…æ"
date: 2023-06-26 10:10:04 +0800
labels: [huggingface, repo]
---

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- Auto-GPTQ æºç æµ…æ
  - æ€ä¹ˆå†™ä¸€ä¸ª python åŒ… (setup.py)
  - Github Action åšåˆ†å‘
  - torch cpp extension çš„ä½¿ç”¨
  - OpenAI triton çš„ä½¿ç”¨
  - æ€ä¹ˆåŸºäº ğŸ¤— åšäºŒæ¬¡å¼€å‘çš„ä¾‹å­ã€å¾…å®šã€‘

å‚è€ƒèµ„æ–™

- åŸå§‹ä»£ç ä»“åº“: [https://github.com/PanQiWei/AutoGPTQ.git](https://github.com/PanQiWei/AutoGPTQ.git)

æ¶‰åŠå†…å®¹

- æœ¬åšå®¢åªåˆ†æ [commit id: 046c031](https://github.com/PanQiWei/AutoGPTQ/tree/046c0311396e5d2e06472b5e66b20dfd0e7c13fa) çš„ä»£ç 

ä¸æ¶‰åŠå†…å®¹

- GPTQ ç®—æ³• (float16 æ¨¡å‹é‡åŒ–ä¸ºæ•´æ•°æƒé‡çš„è¿‡ç¨‹)
- torch cpp extension åŠ OpenAI triton çš„æ·±å…¥ä»‹ç»


## é¡¹ç›®ç›®å½•ç»“æ„

```
README.md
setup.py
docs/
examples/
.github/                            # ä¸ GitHub ä¸å‘å¸ƒç›¸å…³çš„
auto_gptq/                          # å®‰è£…çš„ python åŒ…
    __init__.py
    eval_tasks/                     # è¯„ä¼°é‡åŒ–ç»“æœ
    modeling/
        __init__.py
        _base.py                    # æ ¸å¿ƒç±»: BaseQuantizeConfig, BaseGPTQForCausalLM
        auto.py                     # æ ¸å¿ƒç±»: AutoGPTQForCausalLM
        llama.py                    # bloom.py, gptq2.py, ..., ç»§æ‰¿è‡ª BaseGPTQForCausalLM, ä½†åªä¿®æ”¹å‡ ä¸ªç±»å±æ€§
        ...
    nn_modules/                     # å¾…ç ”ç©¶
        __init__.py
        _fused_base.py
        fused_gptj_attn.py
        fused_llama_attn.py
        fused_llama_mlp.py
        qlinear/
            __init__.py
            qlinear_cuda.py
            qlinear_cuda_old.py
            qlinear_triton.py
        triton_utils/
            __init__.py
            custom_autotune.py
            kernels.py
            mixin.py
    quantization/
        __init__.py
        gptq.py
        quantizer.py
    utils/
        ...
        peft_utils.py               # ğŸ¤— Peft çš„é›†æˆ
autogptq_cuda/                      # Pytorch CUDAExtension 
    autogptq_cuda_256.cpp
    autogptq_cuda_64.cpp
    autogptq_cuda_kernel_256.cu
    autogptq_cuda_kernel_64.cu
```

## å®‰è£…ä¸ setup.py

æ ¹æ®å®˜æ–¹æ–‡æ¡£çš„æè¿°, ä½¿ç”¨ `pip install` çš„æ–¹å¼è¿›è¡Œå®‰è£…æœ‰å¦‚ä¸‹å‡ ç§é€‰é¡¹

```bash
# ç¡®è®¤ä¹‹å‰çš„å®‰è£…è¢«åˆ é™¤
pip uninstall autogptq_cuda -y

# å®‰è£…æ—¶ä½¿ç”¨ pytorch ç¼–è¯‘ extension
pip install autogptq  # ç­‰ä»·ä¸ BUILD_CUDA_EXT=1 pip install auto-gptq

# å®‰è£…æ—¶ä¸ç¼–è¯‘ extension
BUILD_CUDA_EXT=0 pip install auto-gptq

# é¢å¤–å®‰è£… triton
pip install auto-gptq[triton]
```

ä¸Šé¢çš„æ³¨é‡Šå®é™…ä¸Šæœ‰äº›â€œå«ç³Šâ€, å› æ­¤è¿™é‡Œç›´æ¥å¯¹ `setup.py` è¿›è¡Œåˆ†æ, ä»¥å¾—åˆ°å‡†ç¡®ç†è§£

```python
# å…³é”®éƒ¨åˆ†ä»£ç 
common_setup_kwargs = {
    "python_requires": f">=3.8.0",                            # å¼ºåˆ¶æ£€æŸ¥, å¦åˆ™å®‰è£…æŠ¥é”™
    # ä»¥ä¸‹åªæœ‰åœ¨ BUILD_CUDA_EXT="1", ä¸” torch å­˜åœ¨æ—¶æ‰æœ‰ã€å¾…ç¡®è®¤ä»€ä¹ˆå«torchå­˜åœ¨ã€‘
    "ext_modules": [
        cpp_extension.CUDAExtension(
            "autogptq_cuda_64",
            [
                "autogptq_cuda/autogptq_cuda_64.cpp",
                "autogptq_cuda/autogptq_cuda_kernel_64.cu"
            ]
        ),
        cpp_extension.CUDAExtension(
            "autogptq_cuda_256",
            [
                "autogptq_cuda/autogptq_cuda_256.cpp",
                "autogptq_cuda/autogptq_cuda_kernel_256.cu"
            ]
        )
    ],
    "cmdclass": {'build_ext': cpp_extension.BuildExtension}
}

include_dirs = [
    "autogptq_cuda"
    ""  # ã€éœ€è¦ä½¿ç”¨Pytorch extensionæ—¶è¿˜ä¼šå¢åŠ ã€‘
]

setup(
    packages=find_packages(),       # ["auto_gptq"]
    install_requires=requirements,  # è‡ªåŠ¨å®‰è£…: ["accelerate>=0.19.0", "torch>=1.13.0", "transformers>=4.29.0", "peft", ...]
    extras_require=extras_require,  # è‡ªåŠ¨å®‰è£…, ä½¿ç”¨ pip install auto-gptq[triton] æ‰ä¼šè§¦å‘ {"triton": ["triton>=2.0.0"]}
    include_dirs=include_dirs,      # ä¹Ÿæ”¾å…¥ site-packages ç›®å½•å†…
    **common_setup_kwargs
)
```

å…³äº `extras_require` ä¸ `pip install xx[yy,zz]`: ã€å¾…ç¡®è®¤å¹¶ç§»å…¥Notesä¸­ã€‘

- å…ˆåˆ¤æ–­ `"yy"` æ˜¯å¦æ˜¯ `extras_require` çš„é”®, å¦‚æœåŒ¹é…ä¸Š, å°±å®‰è£… `extras_require["yy"]` çš„åŒ…, åŒ¹é…ä¸ä¸Šå°±æŠ¥ä¸€ä¸ªè­¦å‘Š, ç„¶åè¿›è¡Œä¸‹ä¸€æ­¥
- ç„¶ååˆ¤æ–­ `yy` æ˜¯å¦ä¸ºä¸€ä¸ªåŒ…å(PyPI), å¦‚æœåŒ¹é…ä¸Š, å°±å®‰è£… `yy` åŒ…
- éƒ½åŒ¹é…ä¸ä¸Šå°±ä¸è¿›è¡Œå®‰è£… `yy`

æºç å®‰è£…

```bash
pip install .[triton]
```


å¦‚æœ Pytorch CUDA extension è¢«æ­£ç¡®å®‰è£…, å®‰è£…ä½ç½®ä¸º
```
/path/to/site-packages/autogptq_cuda_256.cpython-38-x86_64-linux-gnu.so
/path/to/site-packages/autogptq_cuda_64.cpython-38-x86_64-linux-gnu.so
```


æç¤º: å¦‚æœå¸Œæœ›çœ‹å‡ºä¸€äº›å®‰è£…è¿‡ç¨‹å…·ä½“å¹²äº†ä»€ä¹ˆ, å¯ä»¥å°è¯•ä½¿ç”¨ `pip install -v .`

é€šå¸¸ä¸æ¨èä½¿ç”¨: `python setup.py install`, å‚è€ƒ [stackoverflow](https://stackoverflow.com/questions/15724093/difference-between-python-setup-py-install-and-pip-install)

```python
pip install -v -e .
# pip install . ç›¸å½“äºåœ¨ python setup.py install ä¹‹å¤–è¿˜åšäº†è‡ªåŠ¨å®‰è£…ä¾èµ–åŒ…ç­‰å·¥ä½œ
# -e å‚æ•°æ–¹ä¾¿è°ƒè¯•, é¡¹ç›®ç›®å½•ä¸‹çš„ä¿®æ”¹ä¼šè‡ªåŠ¨ç”Ÿæ•ˆ
# -v ç”¨äºæ˜¾ç¤ºå®‰è£…è¿‡ç¨‹, ä¾‹å¦‚è®© setup.py æ–‡ä»¶ä¸­çš„ print èƒ½æ­£å¸¸ç”Ÿæ•ˆ
```

æ—¥å¿—å¦‚ä¸‹
```
# ä»¥ # å·å¼€å¤´çš„æ˜¯æ³¨é‡Š

Processing /abspath/to/AutoGPTQ
  Running command python setup.py egg_info
  ...  # è¿™ä¸€éƒ¨åˆ†ä¼šæ‰“å° setup.py ä¸­çš„ print è¯­å¥è¾“å‡º
  running egg_info
  # åˆ›å»ºä¸€äº›ä¸´æ—¶ç›®å½•, å¹¶å†™æ–‡ä»¶
  writing /tmp/pip-pip-egg-info-eoiqxgek/auto_gptq.egg-info/PKG-INFO
  ...
  Prepareing metadata (setup.py) ... done
# å®‰è£…ä¾èµ–åŒ…
Collecting accelerate>=0.19.0
...
Building wheels for collected packages: auto-gptq
  Running command python setup.py bdist_wheel
  running bdist_wheel
  running build
  running build_py
  # æ‹·è´æ–‡ä»¶åˆ° build ç›®å½•
  create build/lib/auto_gptq
  copy auto_gptq/__init__.py -> build/lib/auto_gptq
  ...
  # å¦‚æœBUILD_CUDA_EXT=1(é»˜è®¤å€¼), ä¸”å®‰è£…auto-gptqä¹‹å‰å°±å·²ç»å®‰è£…äº†torchæ—¶ä¼šè§¦å‘
  running build_ext
  ...
  running install
  running install_lib
  # å†æ‹·è´ä¸€æ¬¡
  copy build/lib/auto_gptq/__init__py -> build/bdist.linux-x86_64/wheel/auto_gptq
  ...
  running install_egg_info
  running egg_info
  # ç–‘ä¼¼æ˜¯å†™åˆ° site-packages/auto_gptq.egg-info ä¸­
  writing auto_gptq.egg-info/PKG-INFO
  ...
  running install_scripts
  creating build/bdist.linux-x86_64/wheel/auto_gptq-0.3.0.dev0.dist-info/WHEEL
  creating '/tmp/pip-wheel-nlkqzuc6/auto_gptq-0.3.0.dev0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
  adding 'auto_gptq/__init__.py'
  ...
  removing build/bdist.linux-x86_64/wheel
  Building wheel for auto-gptq (setup.py) ... done
  Created wheel for auto-gptq: filename=auto_gptq-0.3.0.dev0-py3-none-any.whl size=63666 sha256=12345163663
  Store in directory: /tmp/pip-ephem-wheel-cache-5yjlc9ij/wheels/36/9a/b2/12355772277
Successfully build auto-gptq
Installing collected packages: accelerate, auto-gptq
  # ä¸Šä¸€è¡Œè¿è¡Œä¹‹åå¯èƒ½ä¼šå¡ä½ä¸€æ®µæ—¶é—´, åº”è¯¥æ˜¯æŠŠå‰é¢çš„ä¸´æ—¶æ–‡ä»¶å¤¹è¿›è¡Œæ‹·è´
  # /tmp/pip-wheel-nlkqzuc6, /tmp/pip-ephem-wheel-cache-5yjlc9ij
  changing model of /path/to/python/bin/accelerate to 755
  changing model of /path/to/python/bin/accelerate-config to 755
  changing model of /path/to/python/bin/accelerate-launch to 755
Successfully installed accelerate-0.19.0 auto-gptq-0.3.0-dev0
```

å¤‡æ³¨: `/tmp` ç›®å½•çš„ä½¿ç”¨å®é™…ä¸Šè¿˜æœ‰æ›´å¤š, ä½†æœ€ç»ˆä¼šè¢«æ¸…ç†æ‰
```
pip-build-tracker-xxxx
pip-ephem-wheel-cahce-xxxx
pip-install-xxxx
pip-pip-egg-info-xxxx
pip-unpack-xxxx
pip-wheel-xxxx
```



### CUDA extension å®‰è£…å¯èƒ½æŠ¥é”™çš„é—®é¢˜

**Case 1**

```
error identifier "__hfma2" is undefined
```

å‡ºç°æ­¤é—®é¢˜çš„ç¯å¢ƒæ˜¯: é©±åŠ¨ä¸ºæœ€é«˜æ”¯æŒ CUDA 11.5 ç‰ˆæœ¬, CUDA toolkit ç‰ˆæœ¬å·ä¸º 11.5, pytorch ç‰ˆæœ¬å·ä¸º 2.0.1 (å®‰è£…çš„æ˜¯ cuda11.7 çš„é¢„ç¼–è¯‘åŒ…, è‡ªåŠ¨å®‰è£…äº†ä¸€å † Nvidia çš„ CUDA 11.7 python åŒ…)

ç–‘ä¼¼æ˜¯æ˜¾å¡é©±åŠ¨/CUDA ç‰ˆæœ¬çš„é—®é¢˜, `__hfma2` å®šä¹‰åœ¨ [CUDA_MATH_API](https://docs.nvidia.com/cuda/pdf/CUDA_Math_API.pdf) ä¸­, è¿™ä¸ªå‡½æ•°è‡³å°‘åœ¨ CUDA 11.2 ç‰ˆæœ¬å°±å·²ç»æ”¯æŒ, æ•…å‡ºç°æ­¤é—®é¢˜çš„åŸå› å­˜ç–‘.

**Case 2**

```
identifier "AT_DISPATCH_CASE" is undefined
```

å‡ºç°æ­¤é—®é¢˜çš„ç¯å¢ƒæ˜¯é©±åŠ¨: ä¸ºæœ€é«˜æ”¯æŒ CUDA 11.5 ç‰ˆæœ¬, CUDA toolkit ç‰ˆæœ¬å·ä¸º 11.5, pytorch ç‰ˆæœ¬å·ä¸º 1.11.0 (å®‰è£…çš„æ˜¯ cuda11.5 çš„é¢„ç¼–è¯‘åŒ…), å¹¶æ‰‹åŠ¨ä¿®æ”¹äº† AutoGPTQ çš„ `setup.py` ä¸­ pytorch çš„æœ€ä½ç‰ˆæœ¬è¦æ±‚, å¹¶æ³¨é‡Šæ‰äº† `auto_gptq/__init__.py` ä¸­ PEFT çš„ `import` è¯­å¥.

è¿™ä¸ªé—®é¢˜åº”è¯¥æ˜¯ Pytorch ç‰ˆæœ¬å¤ªä½çš„é—®é¢˜, `AT_DISPATCH_CASE` è¿™ä¸ªå®å®šä¹‰åœ¨ pytorch ATEN ä¸­: `https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/Dispatch.h`, è€Œ Pytorch


**ä¸€äº›æ¢ç´¢ã€TODO: åç»­æ•´åˆåˆ°CUDAç¬”è®°ä¸­, å¹¶åˆ é™¤æ­¤å¤„çš„è®°å½•ã€‘**
nvcc æ€ä¹ˆç¡®å®šç±»ä¼¼è¿™ç§å‚æ•° `-gencode=arch=compute_70,code=sm_70`, å‚è€ƒ:

- [åšå®¢](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/)
- [issue](https://github.com/PanQiWei/AutoGPTQ/issues/67#issuecomment-1577775096)
- [é—®ç­”](https://stackoverflow.com/questions/68496906/pytorch-installation-for-different-cuda-architectures)

```
TORCH_CUDA_ARCH_LIST=7.0
/usr/local/cuda-11.7/bin/nvcc --list-gpu-arch
```


## åŸºæœ¬ä½¿ç”¨

åŸºæœ¬çš„ä½¿ç”¨æ–¹å¼å¯ä»¥å‚è€ƒå®˜æ–¹ [ç¤ºä¾‹ä»£ç ](https://github.com/PanQiWei/AutoGPTQ/blob/main/examples/quantization/basic_usage_wikitext2.py) æˆ–è€… [README](https://github.com/PanQiWei/AutoGPTQ/blob/main/README.md), ç²¾ç®€åå¦‚ä¸‹:

```python
import os

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import numpy as np
import torch
import torch.nn as nn

pretrained_model_dir = "/path/to/llama_7b"
quantized_model_dir = "llama_7b-4bit-128g"

def get_wikitext2(nsamples, seed, seqlen, model):
    from datasets import load_dataset
    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')
    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')

    from transformers import AutoTokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)
    except:
        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)
    trainenc = tokenizer("\n\n".join(traindata['text']), return_tensors='pt')
    testenc = tokenizer("\n\n".join(testdata['text']), return_tensors='pt')

    import random
    random.seed(seed)
    np.random.seed(0)
    torch.random.manual_seed(0)
    
    traindataset = []
    for _ in range(nsamples):
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        attention_mask = torch.ones_like(inp)
        traindataset.append({'input_ids':inp,'attention_mask': attention_mask})
    return traindataset, testenc

tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)
traindataset,testenc = get_wikitext2(128, 0, 2048, pretrained_model_dir)
quantize_config = BaseQuantizeConfig(
    bits=4,  # quantize model to 4-bit
    group_size=128,  # it is recommended to set the value to 128
    desc_act=False,  # desc_act and group size only works on triton
)
model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)
model.quantize(traindataset, use_triton=False)
model.save_quantized(quantized_model_dir)

model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))
```


å¦‚æœéœ€è¦å°† AutoGPTQ ç”¨äºæ›´å¤šçš„æ¨¡å‹, éœ€è¦ç†è§£å…¶æ ¸å¿ƒç±» `auto_gptq.modeling._base.BaseGPTQForCausalLM`

```python
class BaseGPTQForCausalLM(nn.Module, PushToHubMixin):
    layer_type: str = None
    layers_block_name: str = None
    outside_layer_modules: List[str] = None
    inside_layer_modules: List[List[str]] = None
    lm_head_name: str = "lm_head"

    fused_attn_module_type: Optional[FusedBaseAttentionModule] = None
    fused_mlp_module_type: Optional[FusedBaseMLPModule] = None

    def quantize(...):
        ...

    def from_quantized(...):
        ...
    
    def save_quantized(...):
        ...
    
    def from_pretrained(...):
        ...
    
    def save_pretrained(...):
        ...
```

è€Œæ¯ä¸ªå…·ä½“çš„æ¨¡å‹ä¸€èˆ¬åªéœ€è¦å®šä¹‰ç±»å±æ€§å³å¯, ä¾‹å¦‚: `auto_gptq.modeling.llama.LlamaGPTQForCausalLM`

```python
# LlamaGPTQForCausalLM ç±»çš„å®Œæ•´ä»£ç 
class LlamaGPTQForCausalLM(BaseGPTQForCausalLM):
    layer_type = "LlamaDecoderLayer"
    layers_block_name = "model.layers"
    outside_layer_modules = ["model.embed_tokens", "model.norm"]
    inside_layer_modules = [
        ["self_attn.k_proj", "self_attn.v_proj", "self_attn.q_proj"],
        ["self_attn.o_proj"],
        ["mlp.up_proj", "mlp.gate_proj"],
        ["mlp.down_proj"]
    ]
    # ä»¥ä¸‹ä¸¤ä¸ªæ˜¯éå¿…é¡»çš„
    fused_attn_module_type = FusedLlamaAttentionForQuantizedModel
    fused_mlp_module_type = FusedLlamaMLPForQuantizedModel
```

ä»¥ llama_7b ä¸ºä¾‹, æ¨¡å‹ç»“æ„å¯ä»¥ç”¨è¿™ä¸ªæ–¹å¼è¿›è¡Œæ‰“å°:

```python
from transformers.models.llama import LlamaForCausalLM, LlamaConfig
from accelerate import init_empty_weights
name = "./hf_download/llama_7B"
config = LlamaConfig.from_pretrained(name)
with init_empty_weights():
    model = LlamaForCausalLM(config)
print(model)
```

```
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
```

é¦–å…ˆå¯¹ç±»å±æ€§åšç®€å•ä»‹ç»:

**layer_type**

ç”¨é€”ä¸»è¦æ˜¯ä¼ é€’ç»™ `accelerate.utils.get_balanced_memory` çš„ `no_split_module_classes` å‚æ•°ç”¨

**layers_block_name**

è¿™ä¸ªæ˜¯ç”¨æ¥æŒ‡å®šéœ€è¦é‡åŒ–çš„å±‚, ä¾‹å¦‚åœ¨ llama-7b çš„ä¾‹å­é‡Œ, éœ€è¦é‡åŒ–çš„å‚æ•°éƒ½åœ¨ `model.layers` ä¸­

**outside_layer_modules**

è¿™ä¸ªå‚æ•°æ˜¯ç”¨äºæŒ‡å®šä¸éœ€è¦é‡åŒ–çš„å±‚, ä¾‹å¦‚ llama-7b çš„ä¾‹å­é‡Œ, å–å€¼ä¸º `["model.embed_tokens", "model.norm"]`, æŒ‡å®šè¿™ä¸ªä¸»è¦ç›®çš„æ˜¯ AutoGPTQ åœ¨è¿›è¡Œæ¨¡å‹é‡åŒ–çš„è¿‡ç¨‹é‡Œ, éœ€è¦ç”¨æ ¡å‡†æ•°æ®è·å– `layers_block_name` é‡Œç¬¬ä¸€å±‚çš„è¾“å…¥, å…·ä½“ç®—æ³•é€»è¾‘å¦‚ä¸‹:

- é¦–å…ˆåŠ è½½æ•´ä¸ªæ¨¡å‹åŸå§‹æµ®ç‚¹æ•°çš„æƒé‡è‡³ **CPU**, è¿™ä¸€è¿‡ç¨‹ä½¿ç”¨ `BaseGPTQForCausalLM.from_pretrained` è¿›è¡Œå®ç°
- ç„¶åå°† outside_layer_modules ä¸­çš„æ‰€æœ‰æƒé‡è½¬ç§»åˆ° **GPU** ä¸Š, ç„¶åå°† `layers_block_name` é‡Œç¬¬ä¸€å±‚æ›¿æ¢ä¸º `LayerHijacker`, è€Œè¢«æ›¿æ¢çš„è¿™ä¸ªå±‚åªä¼šè®°å½•è¾“å…¥å‚æ•°, ç„¶åè§¦å‘ `ValueError`, è¿™æ ·ä¸€æ¥ç¬¬ä¸€å±‚çš„è¾“å…¥å°±è¢«è®°å½•äº†ä¸‹æ¥
- ç„¶åä¸€å±‚ä¸€å±‚åšé‡åŒ–

**inside_layer_modules**

è¿™ä¸ªå‚æ•°ç”¨äºæŒ‡å®š `layers_block_name` ä¸­æ¯ä¸€å±‚éœ€è¦é‡åŒ–çš„å­æ¨¡å—, `inside_layer_modules` æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨, åˆ—è¡¨ä¸­è¶Šé å‰çš„ç»„è¶Šå…ˆè¢«ä¼˜åŒ–, è¿™ä¸ªé¡ºåºä¸æ¨¡å‹åœ¨å‰å‘è®¡ç®—æ—¶çš„è®¡ç®—æ¬¡åºä¸€è‡´.

**lm_head_name**

è¿™ä¸ªå‚æ•°å¯¹åº”æ¨¡å‹çš„ head éƒ¨åˆ†, ä½†æ„Ÿè§‰è¿™ä¸ªå‚æ•°åœ¨è®¾è®¡ä¸Šå¯èƒ½å¯ä»¥è¢«ä¼˜åŒ–æ‰.

**fused_attn_module_type**

è¿™ä¸ªå‚æ•°æ˜¯å¯é€‰çš„, ä¸»è¦æ˜¯åœ¨è°ƒç”¨ `from_quantized` æ—¶, å¯¹ä¸€äº›å±‚è¿›è¡Œæ›¿æ¢, èµ·åˆ° kernel fused çš„æ•ˆæœ. ç›®å‰ä¸»è¦æ˜¯ä½¿ç”¨ flashattention.

**fused_mlp_module_type**

è¿™ä¸ªå‚æ•°æ˜¯å¯é€‰çš„, ç±»ä¼¼ `fused_attn_module_type`

ç„¶åä»‹ç» `BaseGPTQForCausalLM` çš„å‡ ä¸ªä¸»è¦æ–¹æ³•ã€TODOå¾…è¡¥å……ã€‘

## é‡åŒ–å‚æ•°çš„ä¿å­˜æ ¼å¼

å¯¹äºä¸€ä¸ª `nn.Linear` å±‚, é‡åŒ–å‰åçš„å‚æ•°å¦‚ä¸‹(æ³¨æ„ä¸åŸå§‹ gptq å®ç°ä¸Šæœ‰ä¸€å®šçš„åŒºåˆ«):

```python
# é‡åŒ–å‰
weight.dtype, weight.shape = float16, (out_features, in_features)
bias.dtype, weights.shape = float16, (out_features,)
# é‡åŒ–å
qweight.dtype, qweight.shape = int32, (in_feature*bits/32, out_features)
qzeros.dtype, qzeros.shape = int32, (in_features/group_size, out_feature*bits/32)  # [0, 2**bits-2]. ä¿å­˜æ—¶å¾ˆå¥‡æ€ªåœ°å¯¹é‡åŒ–è¿‡ç¨‹ä¸­çš„ zeros éƒ½è¿›è¡Œäº†å‡ 1 æ“ä½œ
scales.dtype, scales.shape = float16, (in_features/group_size, out_feature)
g_idx.dtype, g_idx.shape = int32, (in_features,)
bias.dtype, bias.shape = float16, (out_features,)

# ä»¥ llama_7b çš„ layers.0.mlp.down_proj ä¸ºä¾‹:
group_size, bits = 128, 4  # é‡åŒ–è‡³ 4 ä½, ä»¥ 128 åˆ†ç»„æ‰§è¡Œé‡åŒ–
# é‡åŒ–å‰
weight.dtype, weight.shape = float16, (4096, 11008)
bias.dtype, weights.shape = float16, (4096,)
# é‡åŒ–å
qweight.dtype, qweight.shape = int32, (1376, 4096)
qzeros.dtype, qzeros.shape = int32, (86, 512)
scales.dtype, scales.shape = float16, (86, 4096)
g_idx.dtype, g_idx.shape = int32, (11008,)
bias.dtype, bias.shape = float16, (4096,)
```

è¿™é‡Œå¯¹ GPTQ ç®—æ³•çš„é‡åŒ–å‡½æ•°ç¨åŠ å›é¡¾: GPTQ ç®—æ³•ä»…å¯¹ `weight` è¿›è¡Œé‡åŒ–, é‡åŒ–çš„ç²’åº¦ä¸º `weight` æ¯ä¸€è¡Œçš„ group_size ä¸ªå‚æ•°ä¸ºä¸€ç»„, ä¹Ÿå°±æ˜¯æ¯ group_size ä¸ªå‚æ•°ç¡®å®šä¸€ä¸ªåªæœ‰ `2**bits` çš„ç½‘æ ¼, å°†æµ®ç‚¹å‚æ•°é‡åŒ–åˆ°è¿™ `2**bits` ä¸ªæµ®ç‚¹æ•°ä¸Šå», å› æ­¤ä¼šæœ‰ `(in_features/group_size)*out_feature` ä¸ªç¼©æ”¾ç³»æ•° `scale` å’Œæµ®ç‚¹æ•° 0.0 é‡åŒ–çš„å€¼ `nbit_zeros`. ç”±äº `nbit_zeros` çš„å–å€¼ä½äº `[0, 2**bits)`, æ‰€ä»¥æ¯ä¸ªæ•°å­—åªéœ€è¦ bits ä½å°±å¯ä»¥å­˜å‚¨, ä½†ä¸€èˆ¬çš„ç¡¬ä»¶å¯èƒ½ä¸æ”¯æŒ `bits=2,3,4` çš„æ•´æ•°å­˜å‚¨ç±»å‹, å› æ­¤å¯ä»¥å°†å¤šä¸ªæ•´æ•°æ‰“åŒ…åœ¨ä¸€èµ·, ç”¨ 32 ä½æ•´æ•°æ¥è¡¨ç¤º. åŒæ ·çš„é“ç†, `weight` è¢«é‡åŒ–åçš„å­˜å‚¨å½¢å¼ä¹Ÿä¼šåšç±»ä¼¼çš„æ‰“åŒ…. é‡åŒ–åçš„å‚æ•°ä¸é‡åŒ–å‰å‚æ•°çš„è½¬æ¢è§„åˆ™å¦‚ä¸‹:

$$
w_{dequant-float} = (w_{int} - zero_{int}) * scale
$$

è¿™é‡Œå…·ä½“å¯¹æ•´æ•°æ‰“åŒ…æ–¹å¼åšä¸ªè¯´æ˜, `qweight` å’Œ `qzeros` çš„å…·ä½“æ’å¸ƒæ–¹å¼è¿›è¡Œå¦‚ä¸‹è¯´æ˜:

```python
bits, group_size = 4, 4
unpack_qweight.shape = (in_features, out_features) = (8, 8)
unpack_qweight = [
    [0, 1, 2, 0, 1, 2, 0, 1],
    [1, 2, 3, 1, 2, 3, 1, 2],
    [2, 3, 4, 2, 3, 4, 2, 3],
    [3, 4, 5, 3, 4, 5, 3, 4],
    [4, 5, 6, 4, 5, 6, 4, 5],
    [5, 6, 7, 5, 6, 7, 5, 6],
    [7, 8, 9, 7, 8, 9, 7, 8],
    [15,0,14,15, 0,14,15, 0]
]
qweight.shape = (in_features * 32 / bits, out_features) = (1, 8)
qweight = [
    [
        0 + 1*16 + 2*(16**2) + 3*(16**3) + 4*(16**4) + 5*(16**5) + 7*(16**6) + 15*(16**7),
        1 + 2*16 + 3*(16**2) + 4*(16**3) + 5*(16**4) + 6*(16**5) + 8*(16**6) + 0*(16**7),
        2 + 3*16 + 4*(16**2) + 5*(16**3) + 6*(16**4) + 7*(16**5) + 9*(16**6) + 14*(16**7),
        0 + 1*16 + 2*(16**2) + 3*(16**3) + 4*(16**4) + 5*(16**5) + 7*(16**6) + 15*(16**7),
        1 + 2*16 + 3*(16**2) + 4*(16**3) + 5*(16**4) + 6*(16**5) + 8*(16**6) + 0*(16**7),
        2 + 3*16 + 4*(16**2) + 5*(16**3) + 6*(16**4) + 7*(16**5) + 9*(16**6) + 14*(16**7),
        0 + 1*16 + 2*(16**2) + 3*(16**3) + 4*(16**4) + 5*(16**5) + 7*(16**6) + 15*(16**7),
        1 + 2*16 + 3*(16**2) + 4*(16**3) + 5*(16**4) + 6*(16**5) + 8*(16**6) + 0*(16**7),
    ]
]

unpack_qzeros.shape = (in_features/group_size, out_features) = (2, 8)
# unpack_zeros[0][0] = 1 è¡¨ç¤ºçš„æ˜¯åœ¨è¿™ä¸ªgroupé‡Œ, æ•´æ•°1ä»£è¡¨æµ®ç‚¹æ•° 0.0
unpack_qzeros = [
    [1, 2, 3, 4, 15, 2, 3, 3],  # æ³¨æ„æŒ‰ç…§GPTQçš„é‡åŒ–å‡½æ•°, 0.0 å¯¹åº”çš„æ•´æ•°å€¼æ€»æ˜¯ä½äº [1, 15]
    [2, 3, 4, 5, 4, 15, 1, 2]
]

qzeros.shape = (in_features/group_size, out_features * 32 / bits) = (2, 1)
qzeros = [
    [(1-1) + (2-1)*16 + (3-1)*(16**2) + (4-1)*(16**3) + (15-1)*(16**4) + (2-1)*(16**5) + (3-1)*(16**6) + (3-1)*(16**7)],
    [(2-1) + (3-1)*16 + (4-1)*(16**2) + (5-1)*(16**3) + (4-1)*(16**4) + (15-1)*(16**5) + (1-1)*(16**6) + (2-1)*(16**7)],
]
```

äº†è§£è¿™ä¸€ç‚¹å, å®é™…ä¸Šå¯ä»¥ä» AutoGPTQ ä¿å­˜çš„é‡åŒ–åçš„æƒé‡å‚æ•°æ–‡ä»¶ `gptq_model-4bit-128g.bin` é‡æ–°è½¬ä¸º float16 ç‰ˆæœ¬çš„æƒé‡æ–‡ä»¶(åŸå§‹æƒé‡ç»è¿‡ AutoGPTQ è½¬æ¢ä¸ºé‡åŒ–åçš„æƒé‡ç›¸å½“äºä¸€æ¬¡æœ‰æŸå‹ç¼©, é€šè¿‡è¿™ç§æ–¹å¼æ¢å¤å›æ¥ç›¸å½“äºæ˜¯è§£å‹). è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„å®ç°æ–¹æ³•(è„šæœ¬æœ¬èº«ä¸å…·å¤‡é€šç”¨æ€§, ä½†å¯ä»¥ç”¨æ¥åšéªŒè¯)ã€å¾…è¡¥å……ã€‘

## é‡åŒ–æ¨¡å‹çš„æ¨ç†

AutoGPTQ ä¸­åŒ…å«äº†å‡ ç§ kernel å®ç° ([commit id: 046c031](https://github.com/PanQiWei/AutoGPTQ/tree/046c0311396e5d2e06472b5e66b20dfd0e7c13fa))

è¿™é‡Œçš„åˆ†å‘é€»è¾‘ç›¸å¯¹å¤æ‚, å› æ­¤åœ¨å…·ä½“ä»‹ç»æ¯ä¸€ç§å®ç°å‰éœ€è¦ç†æ¸…åˆ†å‘é€»è¾‘

- triton
  - triton: 2,4,8 bit é‡åŒ–
- cuda_old
  - cuda_faster_old: 2,3,4 bit é‡åŒ–, ä¸”å…¥å‚çš„ batch_size (reshape å) å°äºé˜ˆå€¼(é»˜è®¤ä¸º128) ä¸” `use_cuda_fp16=True`(é»˜è®¤å€¼)
  - cuda_old: 2,3,4,8 bit é‡åŒ–, ä¸”å…¥å‚çš„ batch_size (reshape å) å°äºé˜ˆå€¼(é»˜è®¤ä¸º128) ä¸” `use_cuda_fp16=False`
  - torch å®ç°(å…œåº•): 2,3,4,8 bit é‡åŒ–, ä¸”å…¥å‚çš„ batch_size (reshape å) å¤§äºç­‰äºé˜ˆå€¼(é»˜è®¤ä¸º128)
- cuda
  - cuda: 2,3,4,8 bit é‡åŒ–, ä¸”å…¥å‚çš„ batch_size (reshape å) å°äºé˜ˆå€¼(é»˜è®¤ä¸º128)
  - torch å®ç°(å…œåº•): 2,3,4,8 bit é‡åŒ–, ä¸”å…¥å‚ batch_size (reshape å) å¤§äºç­‰äºé˜ˆå€¼(é»˜è®¤ä¸º128)

å¤‡æ³¨: `cuda_old` å’Œ `cuda` çš„æƒ…å½¢ä¸‹, å¦‚æœå…¥å‚ç»´æ•°ä¸å‡ºå‚ç»´æ•°ä¸èƒ½è¢« 64 æ•´é™¤, ä¹Ÿä¼šç›´æ¥è°ƒç”¨ torch å®ç°å…œåº•

è°ƒç”¨ `from_quantized` æ–¹å¼æ—¶å¯ä»¥ä¼ å…¥å‚æ•° `use_triton=True` æ¥ä½¿ç”¨ triton çš„ kernel, å…¶ä½™æƒ…å†µä¸‹, å¦‚æœ `group_size!=-1` ä¸” `desc_act=True` æ—¶, ä½¿ç”¨ cuda, å¦åˆ™ä½¿ç”¨ cuda_old.


ä¸ºäº†æ¸…æ™°èµ·è§, è¿™é‡Œåˆ—å‡ºåˆ†å‘åˆ°æœ€åº•å±‚çš„â€œkernelâ€:

- cuda å®ç°å¯ç›´æ¥å‚è€ƒ `autogptq_cuda/autogptq_cuda_64.cpp` å’Œ `autogptq_cuda/autogptq_cuda_256.cpp` æ–‡ä»¶æœ«å°¾çš„ pybind11 ç›¸å…³ä»£ç , æ³¨æ„å‰è€…é€‚ç”¨äº in_features ä¸ out_features èƒ½è¢« 64 æ•´é™¤æ—¶çš„æƒ…å†µ, åè€…é€‚ç”¨äºèƒ½è¢« 256 æ•´é™¤çš„æƒ…å†µ (ä¼˜å…ˆä½¿ç”¨), å¦‚æœä¸æ»¡è¶³æ¡ä»¶, åˆ™ä½¿ç”¨ torch è¿›è¡Œè®¡ç®—
- triton å®ç°å¯å‚è€ƒ `auto_gptq/nn_modules/triton_utils/kernels.py` ä¸­çš„ `quant_matmul_248_kernel` ä¸ `transpose_quant_matmul_248_kernel`, å…¶ä¸­å‰è€…æ—¶å‰å‘è®¡ç®—çš„ kernel, åè€…æ˜¯åå‘æ±‚å¯¼çš„ kernel.
- torch å®ç°å¯å‚è€ƒ `auto_gptq/nn_modules/qlinear_cuda_old.py` ä¸­ `QuantLinear` çš„ `forward` å‡½æ•°çš„å®ç°

è€Œæœ€å¸¸è§çš„æƒ…å†µæ˜¯ 4 bit é‡åŒ–, æ¨ç†æ—¶æ˜¯ `batch_size=1`, è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€èˆ¬èƒ½è¢« 256 æ•´é™¤, ä¼šè§¦å‘ `cuda_faster_old` æˆ–è€… `triton`

## triton ç®—å­çš„å®ç°ã€å¾…å®šï¼Œæœ¬è´¨ä¸Šå¾ˆæœ´ç´ ã€‘

è¿™éƒ¨åˆ†å®ç°ä¸ triton å®˜æ–¹æ•™ç¨‹ä¸­çš„[çŸ©é˜µä¹˜æ³•](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html)ç±»ä¼¼, å…·ä½“é€»è¾‘æ˜¯å…ˆå°†æ•´æ•°ç±»å‹çš„æƒé‡é‡æ–°è½¬æ¢å› float16, ç„¶åä¸è¾“å…¥åšä¹˜ç§¯. æ³¨æ„ç´¯åŠ é¡¹é‡‡ç”¨çš„æ˜¯ float32 ç±»å‹ (åº”è¯¥æ˜¯ä¸ºäº†ç²¾åº¦), ç´¯åŠ å®Œæ¯•åå†å°†ç»“æœè½¬æ¢å› float16 ä½œä¸ºè¾“å‡º.

AutoGPTQ ä¸­å¯¹ `triton.autotune` åšäº†ä¸€äº› hackã€å¾…è¡¥å……ã€‘

## CUDA ç®—å­çš„å®ç°ã€å¾…å®šï¼Œæœ¬è´¨ä¸Šå¾ˆæœ´ç´ ã€‘

cuda_faster_old ä¸ cuda_old çš„å®ç°

## `.github` ç›®å½•

## ç›¸å…³é“¾æ¥

- 2023 å¹´ 6 æœˆä¸‹æ—¬è‡³ 7 æœˆä¸Šæ—¬, AutoGPTQ é¡¹ç›®ä¼¼ä¹é™·å…¥äº†åœæ» [issue](https://github.com/PanQiWei/AutoGPTQ/issues/187), ä¸æ­¤åŒæ—¶, huggingface å®˜æ–¹ä¼¼ä¹æœ‰è®¡åˆ’å°† GPTQ å¼•å…¥, ä½† huggingface çš„å¼€å‘äººå‘˜ä¼¼ä¹è®¡åˆ’ä½¿ç”¨ [exllama](https://github.com/turboderp/exllama) é¡¹ç›®æä¾›çš„ç®—å­ (exllama çš„æ¨ç†ç®—å­ä¼¼ä¹æ›´ä¸ºé«˜æ•ˆ). æˆªè‡³è‡³æœ¬æ–‡è®°å½•æ—¶é—´ 2023/07/07, è®¸å¤šå…¶ä»–é¡¹ç›®å¯¹ GPTQ çš„ä½¿ç”¨ (é‡åŒ–ä¸æ¨ç†) é‡‡ç”¨çš„æ˜¯ [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa). æ€»ä¹‹, å¯ä»¥æœŸå¾…ä¸€æ³¢ huggingface å¯¹ GPTQ çš„å®˜æ–¹æ”¯æŒ, ä¸ªäººé¢„è®¡ä¼šå‡ºç°åœ¨ huggingface optimum ä»“åº“ä¸­. æ³¨æ„ exllama ä»…åŒ…å«æ¨ç†éƒ¨åˆ†, é€‚ç”¨äº GPTQ-for-LLaMa å¯¼å‡ºçš„é‡åŒ–åçš„æ¨¡å‹ (åº”è¯¥ä¹ŸåŒæ ·é€‚ç”¨äº AutoGPTQ, å¾…éªŒè¯).