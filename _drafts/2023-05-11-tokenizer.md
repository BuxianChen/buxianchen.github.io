---
layout: post
title: "(WIP) Tokenizer è¯¦è§£"
date: 2023-05-11 10:01:04 +0800
labels: [huggingface]
---

<style>
h2:after {
  content: "# ";
  color: gray;
}
h3:after {
  content: "## ";
  color: gray;
}
h4:after {
  content: "### ";
  color: gray;
}
h5:after {
  content: "#### ";
  color: gray;
}
.alert-red {
    padding: 1em;
    border: 1px solid #f44336;
    background-color: #ffebee;
    color: #f44336;
    /* font-weight: bold; */
    margin-top: 1em;
    margin-bottom: 1em
}
</style>

## åŠ¨æœºã€å‚è€ƒèµ„æ–™ã€æ¶‰åŠå†…å®¹

åŠ¨æœº

- ç†Ÿæ‚‰ ğŸ¤— Tokenizers çš„ç›¸å…³ API ä¸æºç 
- ç†Ÿæ‚‰ ğŸ¤— Transformers slow/fast tokenizer çš„ç›¸å…³ API ä¸æºç 
- é€‚å½“è¡¥å……ç›¸å…³çŸ¥è¯†

å‚è€ƒèµ„æ–™

- ğŸ¤— Transformers 4.26.1 æºä»£ç 
- ğŸ¤— Transformers å®˜æ–¹æ–‡æ¡£
- ğŸ¤— Tokenizers å®˜æ–¹æ–‡æ¡£


## åŸç†è§£æï¼šTokenizer

å–å†³äºä¸åŒçš„ tokenizer å®ç°, ğŸ¤— Tokenizers ä¸­çš„ Tokenizer åœ¨encodeé˜¶æ®µé€šå¸¸ä¼šè¿›è¡Œå¦‚ä¸‹å‡ ä¸ªæ­¥éª¤ï¼Œå…·ä½“å®ç°ç»†èŠ‚è§æºç è§£æéƒ¨åˆ†

```
# ä»¥bert-base-uncasedçš„fastç‰ˆæœ¬ä¸ºä¾‹
How are U today?
# Normalization
how are u today?
# Pre-tokenization
[how, are, u, today, ?]
# tokenize
[how, are, u, to, ##day, ?]
# Postprocess
[CLS, how, are, u, to, ##day, ?, SEP]
```

<div class="alert-red">
æ³¨æ„: æœ¬èŠ‚å‰©ä½™éƒ¨åˆ†çš„ç®—æ³•æè¿°ä¸ä¿è¯ä¸ ğŸ¤— Tokenizers æˆ– ğŸ¤— Transformers ä¸­çš„ slow/fast ç‰ˆä¸­çš„å®ç°å®Œå…¨å»åˆã€‚åŸå› æ˜¯ï¼š
ï¼ˆ1ï¼‰ğŸ¤— Tokenizers çš„ç¡®å®ç°äº†ä»¥ä¸‹çš„å‡ ç§ç®—æ³•[å‚è€ƒå®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/tokenizers/api/models)ï¼Œä½†ç”±äºğŸ¤— Tokenizersé‡‡ç”¨äº† Rust è¿›è¡Œå®ç°ï¼Œç¬”è€…æš‚æ—¶æ— åŠ›ç†æ¸…å‡†ç¡®çš„æºç ï¼Œæ‰€ä»¥æ²¡æœ‰æ·±ç©¶
ï¼ˆ2ï¼‰ğŸ¤— Transformers ä¸­çš„ slow/fast ç‰ˆçš„ tokenizer æ˜¯ä¸ºäº†å¯¹é½ç›¸åº”æ¨¡å‹çš„åŸå§‹å®ç°ï¼Œå› æ­¤å¯¹äºä¸€ä¸ªä¸ªå…·ä½“çš„æ¨¡å‹çš„ Tokenizerï¼Œæœ‰å¯èƒ½ä¼šå¯¹æ ‡å‡†çš„ BPE/WordPiece/Unigramç®—æ³•åšäº›å°æ”¹åŠ¨ã€‚
</div>



### BPE

ä¸€ä¸ªå¸¦æœ‰å®Œæ•´å®ç°çš„æ•™ç¨‹ï¼š[ğŸ¤— NLP Course](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)

BPE (Byte Pairwise Encoding) ç®—æ³•çš„è®­ç»ƒæµç¨‹å¦‚ä¸‹ï¼š
```
è¾“å…¥ï¼šå¥å­åˆ—è¡¨ï¼Œè¯è¡¨æ•°é‡ä¸Šé™
ï¼ˆä¾‹å­ï¼‰ï¼š[" ".join(["hug"]*10), " ".join(["pug"]*5), " ".join(["pun"]*12)]
å‰å¤„ç†ï¼šå°†å¥å­åˆ—è¡¨è½¬æ¢ä¸ºè¯åˆ—è¡¨ï¼Œå¹¶ç»Ÿè®¡è¯é¢‘ã€‚åŒæ—¶è®°å½•æ‰€æœ‰å‡ºç°çš„å­—ç¬¦ä½œä¸ºbase_vocabï¼šæœ€ç»ˆå¾—åˆ°çš„ç»“æœä¸ºï¼š[(è¯è¯­1, è¯é¢‘1), ..., (è¯è¯­N, è¯é¢‘N)], base_vocab: [å­—ç¬¦1, ..., å­—ç¬¦K]
ï¼ˆä¾‹å­ï¼‰ï¼šä»¥ç©ºæ ¼ä½œä¸ºåˆ†éš”ç¬¦è¿›è¡Œåˆ‡è¯ï¼Œå¾—åˆ°[("hug": 10), ("pug", 5), ("pun", 12)], base_vocab: ["h", "u", "g", "p", "n"]
è®­ç»ƒæµç¨‹ï¼š
  é¦–å…ˆåˆå§‹åŒ–æ‰€æœ‰è¯è¯­çš„å½“å‰æ‹†è§£æ–¹å¼ï¼š{è¯è¯­1: ([å­—ç¬¦1,...,å­—ç¬¦k_1], è¯é¢‘1), ..., è¯è¯­N: ([å­—ç¬¦1,...,å­—ç¬¦k_N], è¯é¢‘N)}, å½“å‰mergeåˆ—è¡¨ä¸º: []
  ï¼ˆä¾‹å­ï¼‰ï¼š{hug: ([h, u, g], 10), pug: ([p, u, g], 5), pun: ([p, u, n], 12)}

  While True:
    æ ¹æ®å½“å‰è¯çš„æ‹†è§£æ–¹å¼è®¡ç®—å€™é€‰çš„mergeåˆ—è¡¨åŠå¯¹åº”çš„é¢‘æ•°, å€™é€‰çš„mergeåˆ—è¡¨æŒ‡çš„æ˜¯æ‰€æœ‰è¯è¯­å½“å‰æ‹†è§£æ–¹å¼
    ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå€™é€‰mergeåˆ—è¡¨ä¸ºï¼š[(h, u): 10, (u, g): 15, (p, u): 17, (u, n): 12]
    ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå€™é€‰mergeåˆ—è¡¨ä¸ºï¼š[(h, u): 10, (u, g): 10, (pu, g): 5, (pu, n): 12]
    é€‰å‡ºè¯é¢‘æœ€å¤§çš„mergeæ–¹å¼, åŠ å…¥è‡³mergeåˆ—è¡¨, å¹¶å¯¹æ‰€æœ‰è¯è¯­çš„æ‹†è§£æ–¹å¼åšæ›´æ–°
    ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå°†åŸå§‹çš„è¯è¯­æ‹†è§£æ–¹å¼ç”¨ [p, u] -> puæ›´æ–°ï¼Œå¾—åˆ°ï¼š{hug: ([h, u, g], 10), pug: ([pu, g], 5), pun: ([pu, n], 12)}, mergeåˆ—è¡¨ä¸º: [(p, u)]
    ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå°†åŸå§‹çš„è¯è¯­æ‹†è§£æ–¹å¼ç”¨ [pu, n] -> punæ›´æ–°ï¼Œå¾—åˆ°ï¼š{hug: ([h, u, g], 10), pug: ([pu, g], 5), pun: ([pun], 12)}, mergeåˆ—è¡¨ä¸º: [(p, u),(pu, n)]
    å¾ªç¯ç›´è‡³ï¼ˆmergeåˆ—è¡¨é•¿åº¦+base_vocabé•¿åº¦ï¼‰è¾¾åˆ°è¯è¡¨æ•°é‡ä¸Šé™
```

æ¨ç†æµç¨‹å¦‚ä¸‹
```
è¾“å…¥ï¼šå¥å­ï¼Œbase_vocabä¸åˆå¹¶è§„åˆ™
ï¼ˆä¾‹å­ï¼‰ï¼šbase_vocabä¸åˆå¹¶è§„åˆ™ï¼š[h, u, g, p, n, (p, u), (h, u), (hu, g)]
å‰å¤„ç†ï¼šå°†å¥å­æ‹†è§£ä¸ºè¯è¯­åˆ—è¡¨
æ¨ç†æµç¨‹ï¼š
  tokens = []
  for word in sentence:
    word_split = [å­—ç¬¦1, ..., å­—ç¬¦k]
    ï¼ˆä¾‹å­ï¼‰ï¼šword_split = [h,u,g,i,h,u]
    for merge in merges:
      å°è¯•å°†mergeåº”ç”¨äºwordä¸Š, å¹¶æ›´æ–°word_split
      ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå°è¯•ä½¿ç”¨(p, u)åˆå¹¶ï¼Œword_splitä¸å˜
      ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå°è¯•ä½¿ç”¨(h, u)åˆå¹¶ï¼Œword_splitå˜ä¸º[hu, g, g, i, hu]
      ï¼ˆä¾‹å­-ç¬¬3è½®ï¼‰ï¼šå°è¯•ä½¿ç”¨(hu, g)åˆå¹¶ï¼Œword_splitå˜ä¸º[hug, g, i, hu]
    tokens.extend(word_split)
```

### WordPiece

ä¸€ä¸ªå¸¦æœ‰å®Œæ•´å®ç°çš„æ•™ç¨‹ï¼š[ğŸ¤— NLP Course](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)

WordPiece ç®—æ³•æ˜¯ Bert æ‰€ç”¨çš„ tokenize ç®—æ³•

<div class="alert-red">
æ­£å¦‚[ğŸ¤— NLP Course](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)æŒ‡å‡ºçš„é‚£æ ·, Google å¹¶æœªå°† WordPiece çš„è®­ç»ƒç®—æ³•è¿›è¡Œå¼€æºï¼Œä½†æ¨ç†ç®—æ³•æ˜¯å¼€æºçš„ï¼Œæ¨ç†ç®—æ³•å¯å‚è€ƒ[bertæºç ](https://github.com/google-research/bert/blob/master/tokenization.py)ã€‚å› æ­¤ä¸¥æ ¼åœ°è¯´ï¼ŒWordPiece çš„è®­ç»ƒç®—æ³•åªæ˜¯çŒœæµ‹ã€‚
</div>

ä¸ BPE ç®—æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼š
- ä¸­é—´å­—ç¬¦é‡‡ç”¨ "##" å¼€å¤´è¡¨ç¤º
- è®­ç»ƒé˜¶æ®µ
  - é€‰å–mergeæ—¶ï¼Œåˆ¤æ–­æœ€å¤§å€¼çš„æ ‡å‡†å˜ä¸º: åˆå¹¶å‡ºç°çš„æ¬¡æ•°/(piece1çš„æ¬¡æ•°*piece2çš„æ¬¡æ•°)
  - ä¸ä¿ç•™mergeçš„äºŒå…ƒç»„, åªä¿ç•™æœ€ç»ˆç»“æœ
- æ¨ç†é˜¶æ®µ
  - è´ªå¿ƒç®—æ³•åŒ¹é…æ¯ä¸ªè¯çš„å‰©ä½™å­—ç¬¦

WordPiece ç®—æ³•çš„è®­ç»ƒæµç¨‹å¦‚ä¸‹ï¼š

```
è¾“å…¥ï¼šå¥å­åˆ—è¡¨ï¼Œè¯è¡¨æ•°é‡ä¸Šé™
ï¼ˆä¾‹å­ï¼‰ï¼š[" ".join(["hug"]*10), " ".join(["pug"]*5), " ".join(["pun"]*12)]
å‰å¤„ç†ï¼šå°†å¥å­åˆ—è¡¨è½¬æ¢ä¸ºè¯åˆ—è¡¨ï¼Œå¹¶ç»Ÿè®¡è¯é¢‘ã€‚åŒæ—¶è®°å½•æ‰€æœ‰å‡ºç°çš„å­—ç¬¦ä½œä¸ºvocabï¼ŒåŒ…æ‹¬å‡ºç°åœ¨å¼€å¤´çš„å­—ç¬¦ä¸å‡ºç°åœ¨ä¸­é—´çš„å­—ç¬¦ï¼šæœ€ç»ˆå¾—åˆ°çš„ç»“æœä¸ºï¼š[(è¯è¯­1, è¯é¢‘1), ..., (è¯è¯­N, è¯é¢‘N)], vocab: [å­—ç¬¦1, ..., å­—ç¬¦K]
ï¼ˆä¾‹å­ï¼‰ï¼šä»¥ç©ºæ ¼ä½œä¸ºåˆ†éš”ç¬¦è¿›è¡Œåˆ‡è¯ï¼Œå¾—åˆ°[("hug": 10), ("pug", 5), ("pun", 12)], vocab: ["h", "##u", "##g", "p", "##n"]
è®­ç»ƒæµç¨‹ï¼š
  é¦–å…ˆåˆå§‹åŒ–æ‰€æœ‰è¯è¯­çš„å½“å‰æ‹†è§£æ–¹å¼ï¼š{è¯è¯­1: ([å­—ç¬¦1,##å­—ç¬¦k_2...,##å­—ç¬¦k_1], è¯é¢‘1), ..., è¯è¯­N: ([å­—ç¬¦1,##å­—ç¬¦2...,##å­—ç¬¦k_N], è¯é¢‘N)}
  ï¼ˆä¾‹å­ï¼‰ï¼š{hug: ([h, ##u, ##g], 10), pug: ([p, ##u, ##g], 5), pun: ([p, ##u, ##n], 12)}
  While True:
    æ ¹æ®å½“å‰è¯çš„æ‹†è§£æ–¹å¼è®¡ç®—å€™é€‰çš„mergeåˆ—è¡¨åŠå¯¹åº”çš„åˆ†æ•°(åˆå¹¶åå‡ºç°çš„é¢‘æ•°/åˆå¹¶å‰çš„é¢‘æ•°ä¹‹ç§¯), å€™é€‰çš„mergeåˆ—è¡¨æŒ‡çš„æ˜¯æ‰€æœ‰è¯è¯­å½“å‰æ‹†è§£æ–¹å¼
    ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå€™é€‰mergeåˆ—è¡¨ä¸ºï¼š[(h, ##u): 10/(10*27), (##u, ##g): 15/(27*15), (p, ##u): 17/(17*27), (##u, ##n): 12/(27*12)]ï¼Œè¿™ä¸ªä¾‹å­æ¯”è¾ƒç‰¹åˆ«ï¼Œåˆ†æ•°å…¨éƒ¨ç›¸åŒ
    ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå€™é€‰mergeåˆ—è¡¨ä¸ºï¼š[(hu, ##g): 10/(10*15) , (p, ##u): 17/(17*17), (##u, ##g): 5/(17*15), (##u, ##n): 17/(17*12)]ï¼Œæœ€å¤§åˆ†æ•°çš„åˆå¹¶æ–¹å¼ä¸º(##u, ##n)
    é€‰å‡ºè¯é¢‘æœ€å¤§çš„mergeæ–¹å¼, vocabåˆ—è¡¨, å¹¶å¯¹æ‰€æœ‰è¯è¯­çš„æ‹†è§£æ–¹å¼åšæ›´æ–°
    ï¼ˆä¾‹å­-ç¬¬1è½®ï¼‰ï¼šå°†åŸå§‹çš„è¯è¯­æ‹†è§£æ–¹å¼ç”¨ [h, ##u] -> huæ›´æ–°ï¼Œå¾—åˆ°ï¼š{hug: ([hu, ##g], 10), pug: ([p, ##u, ##g], 5), pun: ([p, ##u, ##n], 12)}ã€‚vocab.append("hu")
    ï¼ˆä¾‹å­-ç¬¬2è½®ï¼‰ï¼šå°†åŸå§‹çš„è¯è¯­æ‹†è§£æ–¹å¼ç”¨ [##u, ##n] -> ##unæ›´æ–°ï¼Œå¾—åˆ°ï¼š{hug: ([hu, ##g], 10), pug: ([p, ##u, ##g], 5), pun: ([p, ##un], 12)}ã€‚vocab.append("##un")
    å¾ªç¯ç›´è‡³vocabè¾¾åˆ°è¯è¡¨æ•°é‡ä¸Šé™
```

æ¨ç†æµç¨‹å¦‚ä¸‹ï¼ˆå…¶å®æ˜¯ç®€å•çš„è´ªå¿ƒç­–ç•¥ï¼Œå°½é‡åŒ¹é…è¯è¡¨é‡Œæœ€é•¿çš„å­—ä¸²ï¼Œå¦‚æœæŸä¸€æ­¥ç¢°åˆ°OOVï¼Œåˆ™è¿™ä¸ªè¯çš„å‰©ä½™éƒ¨åˆ†è¢«æ ‡è®°ä¸ºUNKï¼‰ï¼Œå‡†ç¡®ä»£ç å¯ç›´æ¥å‚è€ƒ[BertåŸå§‹ä»£ç ](https://github.com/google-research/bert/blob/master/tokenization.py)
```
è¾“å…¥ï¼šå¥å­ï¼Œvocab
å‰å¤„ç†ï¼šå°†å¥å­æ‹†è§£ä¸ºè¯è¯­åˆ—è¡¨
æ¨ç†æµç¨‹ï¼š
  tokens = []
  for word in sentence:
    start=0, end=len(word)
    while start < len(word):
      while end > start:
        if word[start:end] in vocab:
          tokens.append(word[start:end])
          end -= 1
          start = end
          break
        if end == start:
          è¿™ç§æƒ…å†µä¸‹æŠŠæ•´ä¸ªåç»­tokenéƒ½ä½œä¸º[unk]ï¼Œä¸å†è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†è¯
```

### Unigram

è¿™é‡ŒæŒ‰ç…§ [ğŸ¤— nlp course](https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt) ä¸­çš„æè¿°å¯¹ç®—æ³•è¿›è¡Œç®€è¦ä»‹ç»ã€‚

è®­ç»ƒæµç¨‹ï¼š

- å‰å¤„ç†ï¼šå°†å¥å­åˆ†å‰²ä¸ºè¯
- é¦–å…ˆå°†æ‰€æœ‰å‡ºç°çš„å•ä¸ªå­—ç¬¦ä½œä¸ºbase-vocabï¼Œç„¶åä½¿ç”¨ä¸€äº›æ–¹æ³•è·å–åˆ°ä¸€ä¸ªç›¸å¯¹æ¯”è¾ƒå¤§çš„è¯è¡¨vocabï¼ˆæ•™ç¨‹çš„ä»£ç é‡Œé‡‡ç”¨çš„æ˜¯æ‰€æœ‰å‡ºç°çš„è¯çš„å­åºåˆ—ï¼Œå¹¶æŒ‡å‡ºå®é™…ä½¿ç”¨æ—¶å¯ä»¥é‡‡ç”¨BPEç®—æ³•ï¼‰ï¼Œå…¶ä¸­vocabåŒ…å«base-vocabã€‚å¹¶ä¸”è®¡ç®—vocabä¸­æ¯ä¸ªtokenå‡ºç°çš„é¢‘ç‡ï¼Œä¾›åç»­è®¡ç®—æŸå¤±æ—¶ä½¿ç”¨ã€‚
- å¯¹äºvocabä¸­çš„æ¯ä¸ªébase-vocabä¸­çš„è¯ï¼Œè®¡ç®—è¿™ä¸ªè¯ä»è¯è¡¨ä¸­æ’é™¤åï¼Œæ•´ä½“æŸå¤±çš„å¢é•¿é‡ï¼Œä¸¢å¼ƒå¢é•¿é‡æœ€å¤§çš„å‰20%çš„vocabã€‚é‡å¤æ­¤æ­¥éª¤ç›´è‡³è¯è¡¨å¤§å°æ»¡è¶³è¦æ±‚

ç»™å®šä¸€ä¸ªè¯è¡¨ï¼Œè¿™ä¸ªè¯è¡¨åœ¨æ•°æ®é›†ä¸Šçš„æŸå¤±å®šä¹‰ä¸ºæ•°æ®é›†ä¸­æ‰€æœ‰è¯çš„æŸå¤±æŒ‰è¯é¢‘åŠ æƒå¹³å‡ï¼Œè€Œæ¯ä¸ªè¯çš„æŸå¤±ä¸ºï¼š

$$
L(word)=\max_{\bold{x}\in S(word)}[-\sum_{i}log(p(x_i))]
$$

è¿™é‡Œ $S(word)$ è¡¨ç¤ºçš„æ˜¯æŒ‰ç…§ vocabï¼Œæ‰€æœ‰èƒ½æ‹¼å‡‘æˆ $word$ çš„ subword åºåˆ—ã€‚

æ¨ç†æµç¨‹ï¼š

(1) one-best-decoding: å³è®¡ç®—æ¯ä¸ªè¯çš„æŸå¤±æ—¶æ‰¾åˆ°çš„æœ€ä¼˜ subword åºåˆ—ï¼Œè¿™å¯ä»¥ç”¨åŠ¨æ€è§„åˆ’ï¼ˆç»´ç‰¹æ¯”ç®—æ³•ï¼‰æ¥è§£å†³ï¼Œå…·ä½“è¿‡ç¨‹ä»ç•¥ã€‚
(2) k-best-decoding: [ğŸ¤— nlp course](https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt) æ²¡æœ‰æ¶‰åŠåˆ°ï¼Œä½†åŸå§‹è®ºæ–‡ä¸­æŒ‡å‡ºå¯ä»¥ä½¿ç”¨ Forward-DP Backward-A* ç®—æ³•å¾—åˆ°æœ€ä¼˜çš„ k ç§subword åºåˆ—, ä½¿ç”¨ Forward-Filtering and Backward-Sampling algorithm(FFBS) å¯ä»¥æŒ‰æ¦‚ç‡é‡‡æ ·åˆ° k ç§ subword åºåˆ—


åŸå§‹è®ºæ–‡[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959)ä¸­å¯¹ Unigram ç®—æ³•çš„æè¿°ä¸ä¸Šè¿°åŸºæœ¬ä¸€è‡´ï¼Œç¨æœ‰ä¸åŒçš„æ˜¯åœ¨ $p(x_i)$ çš„è®¡ç®—ä¸Šï¼Œè®ºæ–‡ä¸­æè¿°ç”¨ EM ç®—æ³•å¾—åˆ°ï¼Œè€Œä¸Šè¿°æè¿°é‡Œç›´æ¥ä½¿ç”¨é¢‘ç‡å¾—åˆ°ã€‚

å¯¹åŸå§‹è®ºæ–‡çš„ç†è§£ä»¥åŠä¸€äº›å®ç°ç»†èŠ‚å¯ä»¥å‚è€ƒè¿™ç¯‡[åšå®¢](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)

### SentencePiece

<div class="alert-red">
æœ¬å°èŠ‚çš„æè¿°å¯èƒ½ä¸å‡†ç¡®ï¼Œéœ€è¿›ä¸€æ­¥åˆ†è¾¨
</div>

åœ¨ ğŸ¤— Transformers ä¸­,  æŒ‰[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/tokenizer_summary)æè¿°ï¼šSentencePiece ç®—æ³•æ€»æ˜¯å’Œ Unigram é…åˆä½¿ç”¨, å› æ­¤å¯ä»¥è®¤ä¸ºåœ¨ ğŸ¤— Transformers ä¸­, è¿™ä¸¤è€…åŸºæœ¬ä¸Šå¯ä»¥åˆ’ç­‰å·ã€‚ï¼ˆğŸ¤— Transformers ä¸­çš„ SentencePiece = ä¸€äº›é¢„å¤„ç† + Unigramï¼‰

åœ¨å®ç°ç»†èŠ‚ä¸Šï¼ŒğŸ¤— Transformers ä¸­ fast tokenizer ä¾èµ–äº ğŸ¤— Tokenizersï¼Œè€Œ ğŸ¤— Tokenizers ä¸­å¯¹ sentencepiece çš„å¤„ç†æ–¹å¼æ˜¯ä½¿ç”¨ protobuf è§£æ sentencepiece çš„è¯è¡¨å­˜å‚¨æ ¼å¼, ç„¶åå†ç»„åˆä¸Š ğŸ¤— Tokenizers è‡ªèº«å®ç°çš„ Unigram, è¯¦ç»†å†…å®¹å¯ä»¥å‚è€ƒ[tokenizers/implementations/sentencepiece_unigram.py](https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py)ã€‚å³ç›¸å½“äº ğŸ¤— Tokenizers é‡æ–°å®ç°äº† sentencepieceã€‚ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼š ğŸ¤— Tokenizers ä¹Ÿå®ç°äº† SentencePieceBPETokenizer ï¼Œä½†å¹¶æœªåœ¨ ğŸ¤— Transformers è¢«ä½¿ç”¨åˆ°ã€‚

ğŸ¤— Transformers ä¸­ slow tokenizer åˆ™ä¸€èˆ¬ä¾èµ–äº sentencepiece åŒ…


### T5 ä½¿ç”¨çš„ tokenizer

T5 ä½¿ç”¨ SentencePiece ä½œä¸º tokenizerï¼Œç»†èŠ‚å‚è€ƒå®ç°éƒ¨åˆ†

## æºç è§£æ: ğŸ¤— Tokenizers

æœ¬èŠ‚åªä»‹ç» ğŸ¤— Tokenizers æœ¬èº«çš„ä½¿ç”¨ï¼Œä¸æ¶‰åŠ ğŸ¤— Transformers ä¸­ fast tokenizer å¯¹ ğŸ¤— Tokenizers çš„è¿›ä¸€æ­¥å°è£…

ğŸ¤— Tokenizers çš„[å®˜æ–¹æ–‡æ¡£-Getting Started](https://huggingface.co/docs/tokenizers/index)å¯¹ä½¿ç”¨çš„ä»‹ç»å·²ç»è¶³å¤Ÿå……åˆ†ï¼Œæ­¤å¤„ä»…èµ·ä¸€ä¸ªæµ“ç¼©çš„ä½œç”¨ã€‚

ğŸ¤— Tokenizers ä»£ç åº“çš„æ ¸å¿ƒç±»ä¸º `tokenizers.Tokenizer`ã€‚

### ç»„æˆ

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
# (component-3: model): å°†è¯tokenizeä¸ºtokenåˆ—è¡¨: List(str) -> List(Token)
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

# (component-1: normalizer): å¯¹åŸå§‹å¥å­è¿›è¡Œé¢„å¤„ç†: str -> str
from tokenizers.normalizers import NFD, StripAccents
tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents()])

# (component-2: pre_tokenizer): å°†å¥å­æ‹†åˆ†ä¸ºè¯åˆ—è¡¨: str -> List(str)
from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

# (component-4: post-processor): å¯¹tokenåˆ—è¡¨è¿›è¡Œåå¤„ç†, ä¾‹å¦‚å¢åŠ EOS: List(Token) -> List(Token)
from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",  # è¿™é‡Œçš„:1æŒ‡çš„æ˜¯å°†è¿™éƒ¨åˆ†çš„token_type_idæ ‡è®°ä¸º1
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

# (component-5: decoder): å°†tokenåˆ—è¡¨è½¬æ¢ä¸ºå¥å­: List(str) -> str
from tokenizers import decoders
tokenizer.decoder = decoders.WordPiece()
tokenizer.decode(output.ids)

# trainer
from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)

# tokenizer çš„ä¿å­˜æ ¼å¼ä¸ºä¸€ä¸ªå•ä¸€çš„ json æ–‡ä»¶
tokenizer.save("data/tokenizer-wiki.json")
tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")
tokenizer = Tokenizer.from_pretrained("bert-base-uncased")  # ä¸ ğŸ¤— Transformers ä¸­ fast tokenizer çš„ä½¿ç”¨ç±»ä¼¼

# è¿™ç§ç”¨æ³•å¯èƒ½ä¸å¸¸ç”¨? BertWordPieceTokenizer çš„åŸºç±»æ˜¯BaseTokenizer, è€ŒBaseTokenizerä¸Tokenizerç±»æ— å…³
from tokenizers import BertWordPieceTokenizer
tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)
```

ä»¥ gpt2 ä¸ºä¾‹ç®€è¦çœ‹ä¸€ä¸‹å„ä¸ªç»„æˆéƒ¨åˆ†æ€ä¹ˆå•ç‹¬è¢«è°ƒç”¨

<div class="alert-red">
æ³¨æ„: ä¸€èˆ¬æƒ…å†µä¸‹, ä¸è¦å•ç‹¬ä½¿ç”¨å„ä¸ªç»„æˆéƒ¨åˆ†
</div>

```python
from tokenizers import Tokenizer
tokenizer = Tokenizer.from_pretrained("gpt2")

# (component-1: normalizer): å¯¹åŸå§‹å¥å­è¿›è¡Œé¢„å¤„ç†: str -> str
tokenizer.normalizer                              # None
# ä¸å·§çš„æ˜¯, gpt2å¹¶æ²¡æœ‰normalizer, æ‰€ä»¥è¿™é‡Œåªå¥½å¦å¤–é€ ä¸€ä¸ªä¾‹å­
from tokenizers.normalizers import StripAccents, NFD, NFC, Sequence
normalizer = Sequence([NFD(), StripAccents()])    # StripAccents éœ€è¦ä¸ NFD é…åˆä½¿ç”¨
normalizer.normalize_str("Ã©")                     # è¾“å‡º: 'e'

text = "ä¸­å›½"

# (component-2: pre_tokenizer): å°†å¥å­æ‹†åˆ†ä¸ºè¯åˆ—è¡¨: str -> List(str)
tokenizer.pre_tokenizer                           # tokenizers.pre_tokenizers.ByteLevel
word_with_pos = tokenizer.pre_tokenizer.pre_tokenize_str("ä¸­å›½")
# word_with_pos: [('Ã¤Â¸ÅƒÃ¥Ä½Â½', (0, 2))], åˆ‡è¯çš„ç»“æœ, è¿™ä¸ªçœ‹èµ·æ¥ä¹±ç çš„ä¸œè¥¿å®é™…ä¸Šé•¿åº¦ä¸º6(åœ¨utf-8ç¼–ç ä¸­æ±‰å­—ä¸€èˆ¬ç”±3ä¸ªå­—èŠ‚æ„æˆ)
print([ord(x) for x in word_with_pos[0][0]])      # [228, 184, 323, 229, 317, 189]
print(list(text.encode()))                        # [228, 184, 323, 229, 317, 189]

# (component-3: model): å°†è¯tokenizeä¸ºtokenåˆ—è¡¨: List(str) -> List(Token)
tokenizer.model                                   # tokenizers.models.ByteLevel
all_tokens = []
for word, (start, end) in word_with_pos:
    tokens = tokenizer.model.tokenize(word)       # tokens: List[tokenizers.Token]
    all_tokens.append(tokens)

# tokenizers.Token ä¸»è¦æ–¹æ³•ä¸º as_tuple(), ä¸»è¦å±æ€§æ˜¯ value, id
print([token.as_tuple() for token in all_tokens[0]])
# è¾“å‡ºä¸º: [(40792, 'Ã¤Â¸Åƒ', (0, 6)), (32368, 'Ã¥Ä½', (6, 10)), (121, 'Â½', (10, 12))]
# ä¸ºä»€ä¹ˆæ˜¯(0, 6), (6, 10), (10, 12)è€Œä¸æ˜¯(0, 3), (3, 5), (5, 6)ï¼Ÿ

# (component-4: post-processor): å¯¹tokenåˆ—è¡¨è¿›è¡Œåå¤„ç†, ä¾‹å¦‚å¢åŠ EOS: List(Token) -> List(Token)
tokenizer.post_processor                          # tokenizers.processors.ByteLevel

# ä¸å·§çš„æ˜¯, gpt2çš„post_processoræ²¡æœ‰è¿½åŠ ä»»ä½•token
# tokenizer = Tokenizer.from_pretrained("hfl/chinese-roberta-wwm-ext")
encoding = tokenizer.encode(text, add_special_tokens=False)
print(encoding.tokens)                            # ['ä¸­', 'å›½']
encoding = tokenizer.post_processor.process(encoding)
print(encoding.tokens)                            # ['[CLS]', 'ä¸­', 'å›½', '[SEP]']

# (component-5: decoder): å°†tokenåˆ—è¡¨è½¬æ¢ä¸ºå¥å­: List(str) -> str
tokenizer.decoder                                 # tokenizers.decoders.ByteLevel
token_strs = [token.value for tokens in all_tokens for token in tokens]  # ['Ã¤Â¸Åƒ', 'Ã¥Ä½', 'Â½']
tokenizer.decoder.decode(token_strs)              # "ä¸­å›½"
```

ä¸‹é¢çš„å†…å®¹æœ¬è´¨ä¸Šæ˜¯APIæ–‡æ¡£ä»‹ç»çš„æµ“ç¼©

### å®ä¾‹åŒ–ä¸åºåˆ—åŒ–

```python
# æ„å»ºæ–¹æ³•1
from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
# tokenizer.normalizer = ...
# tokenizer.pre_tokenizer = ...
# tokenizer.post_processor = ...
# tokenizer.decoder = ...

# æ„å»ºæ–¹æ³•2: åœ¨ ğŸ¤— hub ä¸­ä¿å­˜çš„ tokenizer.json æ–‡ä»¶
tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")

# æ„å»ºæ–¹æ³•3: ç±»ä¼¼äº ğŸ¤— Transformers çš„ä½¿ç”¨
tokenizer = Tokenizer.from_pretrained("bert-base-uncased")

# ä¿å­˜
save_path = "tokenizer.json"
tokenizer.save(save_path)

# è¡¥å……: ğŸ¤— Transformers ä¸­ä½¿ç”¨ tokenizers.Tokenizer æ„å»º fast tokenizer
# æœ¬è´¨ä¸Š: (1) PreTrainedTokenizerFast çš„è¡Œä¸ºå®Œå…¨ç”± tokenizer_object å†³å®š
# (2) PreTrainedTokenizerFast.save_pretrained å®é™…ä¸Šè°ƒç”¨äº† Tokenizer.save
from transformers import PreTrainedTokenizerFast
fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=save_path)
```

### encode and decode

```python
tokenizer.token_to_id  # <unk> -> unk_id(int), sdhdhud -> None
tokenizer.id_to_token  # 1273773 -> None
# è¿™é‡Œçš„add_special_tokensæ— è®ºTrue/Falseï¼Œç”¨æˆ·è‡ªå®šä¹‰å¢åŠ çš„tokenéƒ½ä¼šè¢«encode
# add_special_tokens=Trueè¡¨ç¤ºè¿›è¡Œpost-processingè¿‡ç¨‹(å³å¢åŠ CLSç­‰)
# is_pretokenized ç”¨é»˜è®¤å€¼å³å¯
tokenizer.encode(sequece: str, pair: Optional[str]=None, is_pretokenized=False, add_special_tokens=True)  # -> tokenizers.Encoding
tokenizer.encode_batch(sequeces: List[str], pair: Optional[str]=None, is_pretokenized=False, add_special_tokens=True)  # -> tokenizers.Encoding

# skip_special_tokensçš„é»˜è®¤å€¼ä¸ºTrue, è‹¥è®¾å®šä¸ºTrue, åˆ™è§£ç æ—¶æ»¤æ‰ç‰¹æ®Štoken
# ç‰¹æ®ŠtokenæŒ‡çš„ï¼štokenizeræœ¬èº«ç‰¹æ®Štokenä¾‹å¦‚CLS, BOSï¼Œä»¥åŠé€šè¿‡add_special_tokenæ·»åŠ çš„token
tokenizer.decode(ids: List[int], skip_special_tokens=True)
tokenizer.decode_batch(sequences: List[List[int]], skip_special_tokens=True)
```

### padding and truncate

```python
# ä»…ä»…æ˜¯ä¾‹å­, è€Œéå®Œæ•´çš„API
tokenizer.enable_padding(pad_id=3, pad_token="[PAD]", ...)
tokenizer.enable_truncation(max_lenghth=128, strategy="longest_first", direction="right")

tokenizer.no_padding()
tokenizer.no_truncation()
```

### add tokens, vocab

`add_special_tokens` ä¸ `add_tokens`ã€‚è§åæ–‡å¯¹ ğŸ¤— Transformers ä¸­ tokenizer çš„ç›¸å…³æ–¹æ³•ã€å¾…è¡¥å……ã€‘

```python
# æ€»æ˜¯åŒ…å«ç‰¹æ®Štoken, ä½†å¯ä»¥æ§åˆ¶æ˜¯å¦åŠ å…¥add_tokenæ—¶å¢åŠ çš„token
tokenizer.get_vocab(with_added_tokens=True)
tokenizer.get_vocab_size(with_added_tokens=True)
```

### train

```python
tokenizer.train(files: List[str], trainer: tokenizers.trainer.Trainer)
# next(iterator) è¿”å› str æˆ– List[str], æ¨èList[str]
tokenizer.train_from_iterator(iterator, trainer: tokenizers.trainer.Trainer, length=None)
```

### å¯è§†åŒ–

```python
from tokenizers.tools.visualizer import EncodingVisualizer, Tokenizer

tokenizer = Tokenizer.from_pretrained("t5-small/tokenizer.json")
viz = EncodingVisualizer(tokenizer) # Change here
text = "I am a boy, using sentencepiece tokenizer ä¸­å›½"
viz(text=text)
```

## æºç è§£æ: sentencepieceã€TODOï¼šæºç è§£æå¾…åç»­å¦èµ·ä¸€ç¯‡åšå®¢è¿›è¡Œä»‹ç»ã€‘

ğŸ¤— Transformers ä¸­æ¯ä¸ªå…·ä½“çš„ slow tokenizer çš„å®ç°é‡Œ, å¦‚æœ tokenizer çš„ç±»å‹ä¸º BPE æˆ–è€…æ˜¯ WordPiece, é‚£ä¹ˆä¸€èˆ¬æ˜¯åœ¨ç›¸åº”çš„ `xxx_tokenizer.py` ä¸­ä½¿ç”¨ python å®ç° BPE å’Œ WordPieceã€‚å› æ­¤ä¼šå‘ç°ä¸€äº›é‡å¤çš„ä»£ç ï¼Œä¾‹å¦‚[tokenization_bert.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py)ä¸[tokenization_distilbert.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/tokenization_distilbert.py)ï¼Œè¿™ç¬¦åˆ ğŸ¤— Transformers ä»£ç åº“çš„[å“²å­¦](https://huggingface.co/docs/transformers/philosophy)ã€‚è€Œ tokenizer çš„ç±»å‹ä¸º SentencePiece æ—¶ï¼Œç›¸åº”çš„ slow tokenizer çš„å®ç°ä¼šå€ŸåŠ© [sentencepiece](https://pypi.org/project/sentencepiece/) åŒ…ã€‚

sentencepieceåŒ…çš„ä½¿ç”¨æ–¹æ³•è¯·ç›´æ¥å‚è€ƒ: [å®˜æ–¹ç¤ºä¾‹](https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb)


## æºç è§£æ: ğŸ¤— Transformers ä¸­çš„ tokenizer

é¦–å…ˆè¯´æ˜ä¸€ä¸‹ ğŸ¤— Transformers ä¸ ğŸ¤— Tokenizers ä¹‹é—´çš„å…³ç³»ï¼šğŸ¤— Transformers 4.x ç‰ˆæœ¬ä¸­æ¯ä¸ªæ¨¡å‹éƒ½ä¼šå°½é‡æ”¯æŒä¸¤ç§ tokenizer çš„å®ç°, slowç‰ˆæœ¬çš„å®ç°ä¸fastç‰ˆæœ¬çš„å®ç°, åè€…ä¾èµ–äº ğŸ¤— Tokenzers åŒ…, è€Œå‰è€…ä¸ä¾èµ–, ä¸”ä¸ºçº¯ python å®ç°ï¼Œæ‰€ä»¥ slow ç‰ˆæœ¬çš„ tokenizer æ›´æ–¹ä¾¿é˜…è¯»ã€‚

å…·ä½“æ¥è¯´ï¼ŒğŸ¤— Transformers ä¸­ fast tokenizer åœ¨å®ä¾‹åˆå§‹åŒ–æ—¶æœ‰å¦‚ä¸‹ä»£ç æ®µï¼š
```python
from .convert_slow_tokenizer import convert_slow_tokenizer
from tokenizers import Tokenizer as TokenizerFast

class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
  vocab_files_names = VOCAB_FILES_NAMES
  slow_tokenizer_class: PreTrainedTokenizer = None
  can_save_slow_tokenizer: bool = True
  
  # èŠ‚é€‰äº†ä¸€éƒ¨åˆ†
  def __init__(self, *args, **kwargs):
    tokenizer_object = kwargs.pop("tokenizer_object", None)
    slow_tokenizer = kwargs.pop("__slow_tokenizer", None)
    fast_tokenizer_file = kwargs.pop("tokenizer_file", None)
    from_slow = kwargs.pop("from_slow", False)
    if from_slow and slow_tokenizer is None and self.slow_tokenizer_class is None:
        raise ValueError("...")
    if tokenizer_object is not None:
        fast_tokenizer = copy.deepcopy(tokenizer_object)
    elif fast_tokenizer_file is not None and not from_slow:
        # We have a serialization from tokenizers which let us directly build the backend
        fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
    elif slow_tokenizer is not None:
        # We need to convert a slow tokenizer to build the backend
        fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
    elif self.slow_tokenizer_class is not None:
        # We need to create and convert a slow tokenizer to build the backend
        slow_tokenizer = self.slow_tokenizer_class(*args, **kwargs)
        fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
    else:
        raise ValueError("...")
    self._tokenizer = fast_tokenizer
    # ...
```
fast tokenizer çš„å„ç±»æ–¹æ³•ä¾‹å¦‚ï¼š`tokenize`ã€`convert_tokens_to_ids`ã€`get_vocab`ã€`decode` æœ€ç»ˆéƒ½ä¼šç›´æ¥è½¬æ¢ä¸ºå¯¹ `self._tokenizer` çš„ç›¸åº”æ–¹æ³•çš„è°ƒç”¨ã€‚ä»å‰é¢å¯¹äº ğŸ¤— Tokenizers çš„ä»‹ç»å¯ä»¥çŸ¥é“ï¼Œ`self._tokenizer` å°è£…äº†è¿™äº›ç»„æˆéƒ¨åˆ†ï¼š`normalizer`ã€`pre_tokenizer`ã€`tokenizer`ã€`post_processor`ã€`decoder`ã€‚

ğŸ¤— Transformers ä¸­çš„æ¯ä¸ª slow tokenizer éœ€è¦é€ä¸€ç”¨pythonå®ç° `normalizer`ã€`pre_tokenizer`ã€`tokenizer`ã€`post_processor`ã€`decoder` è¿™äº›ç»„æˆéƒ¨åˆ†ï¼Œå…¶ä¸­ `tokenizer` æ˜¯ BPE æˆ–æ˜¯ WordPiece æ—¶ï¼Œåˆ™éœ€æ‰‹åŠ¨å®ç° encode çš„è¿‡ç¨‹ï¼Œå¦‚æœæ˜¯ SentencePiece æ—¶ï¼Œåˆ™ä¸€èˆ¬å€ŸåŠ© sentencepiece åŒ…æ¥å®ç°ä¸»è¦é€»è¾‘ã€‚


ğŸ¤— Transformers ä¸­, æ¯ä¸ªæ¨¡å‹éƒ½ä¼šå¯¹åº”äºå…¶ç‰¹æœ‰çš„ tokenizer, ä¾‹å¦‚: t5 æ¨¡å‹çš„ tokenizer ä¸º `T5Tokenizer` å’Œ `T5TokenizerFast`ã€‚ç»§æ‰¿å…³ç³»å¦‚ä¸‹ï¼š

![](../assets/figures/t5/tokenizer.png)

<div class="alert-red">
æ³¨æ„: slow ç‰ˆæœ¬çš„ tokenizer ä¸ fast ç‰ˆæœ¬çš„ tokenizer çš„è¡Œä¸ºæœªå¿…èƒ½å®Œå…¨ä¸€è‡´
</div>

åç»­ç« èŠ‚ä½¿ç”¨å¦‚ä¸‹æœ¯è¯­ï¼š

- base tokenizer: transformers.PretrainedTokenizerBase
- slow tokenizer: transformers.PretrainedTokenizer
- fast tokenizer: transformers.PretrainedTokenizerFast
- specific slow tokenizer: transformers.PretrainedTokenizer çš„å­ç±», ä¾‹å¦‚: T5Tokenizer
- specific fast tokenizer: transformers.PretrainedTokenizerFast çš„å­ç±», ä¾‹å¦‚: T5TokenizerFast
- specific tokenizer: specific slow tokenizer å’Œ specific fast tokenizer
- tokenizers.Tokenizer: ğŸ¤— Tokenizer ä¸­çš„ tokenizers.Tokenizer

### ä½¿ç”¨ã€TODO: éœ€è°ƒæ•´ã€‘

ä»ä¸€ä¸ªç–‘æƒ‘å¼•å…¥ï¼š[issue](https://github.com/huggingface/transformers/issues/5087)

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer
pretrained_name_or_path = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(pretrained_name_or_path)

print(tokenizer.special_tokens_map_extended)
# {"eos_token": "</s>", "unk_token": "<unk>", "pad_token": "<pad>", "additional_special_tokens": ['<extra_id_0>', ..., '<extra_id_99>']}
print(tokenizer.eos_token, tokenizer.unk_token, tokenizer.pad_token)
# eos_token: 1, unk_token: 2, pad_token: 0

text = "abc __"
tokens = tokenizer.tokenize(text)  # ["__ab", "c", "__", "_", "_"]
ids = tokenizer.convert_tokens_to_ids(tokens)  # [703, 75, 3, 834, 834]

ids = tokenizer.encode(text)  # [703, 75, 3, 834, 834, 1]
```

ç”±æ­¤å¯è§ï¼Œåœ¨ `T5Tokenizer` çš„å®ç°é‡Œï¼Œæ²¡æœ‰ `bos_token` è¿™ä¸ªå±æ€§ï¼Œå¹¶ä¸”æ¯ä¸ª word èµ·å§‹çš„ subword ä¼šåŠ ä¸Š `__` çš„å‰ç¼€ã€‚æ³¨æ„è¯è¡¨ä¸­æ—¢æœ‰ä»¥ `__` å¼€å¤´çš„ tokenï¼Œä¾‹å¦‚ `__ab`ï¼Œè€Œ `__` æœ¬èº«ä¹Ÿåœ¨è¯è¡¨ä¸­ã€‚è¿™ç§å¤„ç†æ–¹å¼æ˜¯å› ä¸º `T5Tokenizer` ä½¿ç”¨äº† SentencePiece Tokenizerã€‚

tokenizer çš„å¸¸ç”¨æ–¹æ³•å¦‚ä¸‹å‚è€ƒ[ç¬”è®°](https://buxianchen.gitbook.io/notes/note/dl/huggingface#pretrainedtokenizerbase)ã€åç»­è€ƒè™‘æ€ä¹ˆåˆå¹¶/åˆ å‡ã€‘


### add tokens

å¦å¤–ï¼Œ`PretrainedTokenizerBase` çš„ `add_tokens` ä¸ `add_special_tokens` çš„è¿™ä¸¤ä¸ªæ–¹æ³•ä¹Ÿè®©äººå›°æƒ‘ã€‚å› æ­¤æœ‰å¿…è¦ç†æ¸…æ¥šã€‚

é¦–å…ˆï¼Œè¿™é‡Œå¼•ç”¨ [SentencePiece æ•™ç¨‹](https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb) ä¸­çš„å‡ ä¸ªæœ¯è¯­å¯¹ token è¿›è¡Œåˆ†ç±»ï¼ˆåœ¨ğŸ¤— Transformersçš„æ–‡æ¡£ä¸­ï¼Œç¬”è€…æ²¡æœ‰è§åˆ°ç±»ä¼¼çš„æœ¯è¯­ï¼‰ï¼š

- normal symbols: æ™®é€šçš„ token, sentencepiece tokenizer å¯èƒ½ä¼šå°†è¿™ç§ token åˆ‡åˆ†å¼€
- user defined symbols: ç”¨æˆ·å¢åŠ çš„ç‰¹æ®Š token, å¯ä»¥å‡ºç°åœ¨åŸå§‹æ–‡æœ¬ä¸­, sentencepiece tokenizer ä¿è¯ä¸ä¼šå¯¹è¿™ç§ token è¿›è¡Œåˆ‡åˆ†
- control symbols: å¯¹tokenizerçš„ç»“æœè¿›è¡Œåå¤„ç†æ—¶ä½¿ç”¨çš„ token, ä¾‹å¦‚ï¼šsentencepiece tokenizer å°†å¥å­ tokenize å, åå¤„ç†åŠ ä¸Š `"[CLS]"` å’Œ `"[SEP]"`, å¦‚æœåœ¨è¾“å…¥çš„å¥å­ä¸­å«æœ‰ `"[CLS]"`, sentencepiece tokenizer æœ‰å¯èƒ½ä¼šå°†è¿™ç§ token åˆ‡åˆ†å¼€

ä»ä¸Šé¢çš„ä¾‹å­æ¨å¹¿å¼€æ¥, å¯¹ tokenizer å¢åŠ  token åº”è¯¥è¦åŒ…å«è¿™å‡ ç§æƒ…å½¢:

- æ™®é€štoken,å‡ºç°åœ¨åŸå§‹æ–‡æœ¬, ä¸ä¿è¯å®ƒä¸è¢«åˆ‡åˆ†å¼€: ä¾‹å¦‚: å‡è®¾è¯è¡¨ä¸­å·²ç»æœ‰äº† `"ä¸­å›½"` å’Œ `"äºº"` è¿™ä¸¤ä¸ªtokenï¼Œç°åœ¨å¢åŠ  `"ä¸­å›½äºº"` åˆ°è¯è¡¨é‡Œ, ç›®çš„æ˜¯å¸Œæœ› tokenizer æœ‰å¯èƒ½ä¼šå°† `"ä¸­å›½äºº"` å½“ä½œä¸€ä¸ªæ•´ä½“, å½“ç„¶ä¹Ÿä¸æ’é™¤ tokenizer ä»ç„¶ä¼šè¢«åˆ‡åˆ†ä¸º `"ä¸­å›½"` å’Œ `"äºº"`ã€‚ç„¶è€Œåœ¨ BPEã€WordPieceã€Unigram è¿™ä¸‰ç±»ç®—æ³•ä¸­ï¼Œä¸ºäº†å¢åŠ è¿™ç§ç±»å‹çš„tokenï¼Œ
  - BPE éœ€è¦å¢åŠ çš„æ˜¯ merge è§„åˆ™, å³ `("ä¸­å›½", "äºº")`, ç”šè‡³äºéœ€è¦è°ƒæ•´è¿™ä¸ª merge çš„è§„åˆ™åˆ°åˆé€‚çš„ä½ç½®(ä¼˜å…ˆçº§)
  - WordPiece åªéœ€è¦å°† `"ä¸­å›½äºº"` åŠ å…¥åˆ°è¯è¡¨ä¸­å³å¯
  - Unigram éœ€è¦å°† `"ä¸­å›½äºº"` ä»¥åŠç›¸åº”çš„æ¦‚ç‡å€¼åŠ å…¥è‡³è¯è¡¨é‡Œ, ç”šè‡³äºéœ€è¦è°ƒæ•´å·²æœ‰è¯çš„æ¦‚ç‡å€¼
  å› æ­¤ ğŸ¤— Transformers ä¸­ä¸æ”¯æŒè¿™ç§æ·»åŠ æ–¹å¼(slow tokenizerä¸æ”¯æŒ, ä¸ç¡®å®š fast tokenizer çš„æƒ…å†µ)
- å‡ºç°åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„token, ä¿è¯å®ƒä¸ä¼šè¢«åˆ‡åˆ†å¼€ï¼ˆğŸ¤— Transformers æ”¯æŒï¼‰
- åå¤„ç†token, ä¸»è¦ç”¨é€”ç”¨äºåå¤„ç†æ—¶è¿½åŠ ã€‚å¹¶ä¸”å³ä½¿å®ƒå‡ºç°åœ¨åŸå§‹æ–‡æœ¬ä¸­, ä¹Ÿä¸ä¼šåˆ‡åˆ†å¼€ï¼ˆğŸ¤— Transformers çš„EOSç­‰éƒ½æœ‰æ­¤æ€§è´¨ï¼‰
- åå¤„ç†token, ä¸»è¦ç”¨é€”ç”¨äºåå¤„ç†æ—¶è¿½åŠ ã€‚ä½†å¦‚æœå®ƒå‡ºç°åœ¨åŸå§‹æ–‡æœ¬ä¸­, æœ‰å¯èƒ½ä¼šè¢«åˆ‡åˆ†å¼€ï¼ˆğŸ¤— Transformers ä¸æ”¯æŒï¼‰


æœ‰äº†ä¸Šè¿°è®¤çŸ¥ï¼Œä¸‹é¢å…·ä½“åˆ†ææºä»£ç 

`PrtrainedTokenizerBase` ä¸ `add_tokens` å’Œ `add_special_tokens` ä¸­æœ‰å…³çš„ä»£ç ç‰‡æ®µå¦‚ä¸‹
```python
class SpecialTokensMixin:
    SPECIAL_TOKENS_ATTRIBUTES = [
        "bos_token",
        "eos_token",
        "unk_token",
        "sep_token",
        "pad_token",
        "cls_token",
        "mask_token",
        "additional_special_tokens",
    ]
    def add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, AddedToken]], replace_additional_special_tokens=True) -> int:
        if not special_tokens_dict:
            return 0
        added_tokens = 0
        for key, value in special_tokens_dict.items():
            assert key in self.SPECIAL_TOKENS_ATTRIBUTES, f"Key {key} is not a special token"
            if key == "additional_special_tokens":
                assert isinstance(value, (list, tuple)) and all(isinstance(t, (str, AddedToken)) for t in value)
                if replace_additional_special_tokens:
                    setattr(self, key, value)
                else:
                    # This is a copy of `self._additional_special_tokens`
                    additional_special_tokens = getattr(self, key)
                    additional_special_tokens_set = set(additional_special_tokens)
                    to_add = []
                    for token in value:
                        if str(token) not in additional_special_tokens_set and str(token) not in to_add:
                            to_add.append(token)
                    # update the property
                    additional_special_tokens.extend(to_add)
                    self.additional_special_tokens = additional_special_tokens
                added_tokens += self.add_tokens(value, special_tokens=True)
            else:
                assert isinstance(value, (str, AddedToken))
                setattr(self, key, value)
                added_tokens += self.add_tokens([value], special_tokens=True)
        return added_tokens
    def add_tokens(self, new_tokens: Union[str, AddedToken, List[Union[str, AddedToken]]], special_tokens: bool = False) -> int:
        if not new_tokens:
            return 0
        if not isinstance(new_tokens, (list, tuple)):
            new_tokens = [new_tokens]
        return self._add_tokens(new_tokens, special_tokens=special_tokens)
    def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:
        raise NotImplementedError
```

ç”±æ­¤å¯è§:
- `add_special_tokens` å®é™…ä¸Šåªæ˜¯ç”¨æ¥æ“ä½œ `[bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token]` ä»¥åŠ `additional_special_tokens` è¿™å‡ ä¸ªå±æ€§çš„, ä»ã€å…¶ä»–ã€‘åˆ†æå¯ä»¥çŸ¥é“ `additional_special_tokens` è·Ÿå‰é¢ 6 ç§ token å¹¶æ²¡æœ‰æœ¬è´¨åŒºåˆ«ã€‚è€Œ `add_special_tokens` çš„è¡Œä¸ºæ˜¯ç»™è¿™ 7 ä¸ªå®ä¾‹å˜é‡èµ‹æ–°å€¼, ç„¶åå†è°ƒç”¨ `added_tokens`
- `add_tokens` çš„è¡Œä¸ºå®Œå…¨ç”± `_add_tokens` å†³å®š, ç”±å­ç±» `PrtrainedTokenizer` å’Œ `PrtrainedTokenizerFast` å®ç°

**slow tokenizer**

å¯¹äº slow tokenizer, `_add_tokens` çš„å®ç°å¦‚ä¸‹

```python
def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:
    new_tokens = [str(tok) for tok in new_tokens]
    tokens_to_add = []
    for token in new_tokens:
        if not isinstance(token, str):
            raise TypeError(f"Token {token} is not a string but a {type(token)}.")
        if not special_tokens and hasattr(self, "do_lower_case") and self.do_lower_case:
            token = token.lower()
        if (
            token != self.unk_token
            and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token)
            and token not in tokens_to_add
        ):
            tokens_to_add.append(token)
    added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(tokens_to_add))
    added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}
    self.added_tokens_encoder.update(added_tok_encoder)
    self.added_tokens_decoder.update(added_tok_decoder)

    # Make sure we don't split on any special tokens (even they were already in the vocab before e.g. for Albert)
    if special_tokens:
        if len(new_tokens) == 1:
            _insert_one_token_to_ordered_list(self.unique_no_split_tokens, new_tokens[0])
        else:
            self.unique_no_split_tokens = sorted(set(self.unique_no_split_tokens).union(set(new_tokens)))
    else:
        # Or on the newly added tokens
        if len(tokens_to_add) == 1:
            _insert_one_token_to_ordered_list(self.unique_no_split_tokens, tokens_to_add[0])
        else:
            self.unique_no_split_tokens = sorted(set(self.unique_no_split_tokens).union(set(tokens_to_add)))
    self._create_trie(self.unique_no_split_tokens)
    return len(tokens_to_add)
```

å› æ­¤ slow tokenizer `add_tokens` æ–¹æ³•çš„æµç¨‹å¦‚ä¸‹:

- é€šè¿‡ `convert_tokens_to_ids(token)==convert_tokens_to_ids(self.unk_token)` åˆ¤æ–­æ˜¯å¦ä¸ºæ–°å¢è¯, å¦‚æœæ˜¯, åˆ™åœ¨ `self.added_tokens_encoder` ä»¥åŠ `self.added_tokens_decoder` ä¸­è®°å½• token to idx å’Œ idx to token çš„æ˜ å°„å…³ç³», æ³¨æ„è¿™ä¸¤ä¸ªå®ä¾‹å˜é‡æ˜¯ slow tokenizer ç‹¬æœ‰çš„, fast tokenizer æ— æ­¤å®ä¾‹å˜é‡ã€è¿˜éœ€è¦åœ¨å…¶ä»–åœ°æ–¹ä»‹ç»è¿™ä¸¤ä¸ªå®ä¾‹å˜é‡ã€‘
- å…¥å‚ `special_tokens=True`, é‚£ä¹ˆå°±ä¸è¿›è¡Œå‰ä¸€æ­¥ç­›é€‰, ç›´æ¥å°†å…¥å‚ `new_tokens` ä½œä¸ºä¸å¯åˆ†å‰²çš„ token åŠ å…¥åˆ°è¯è¡¨ä¸­ã€è¿˜éœ€è¦åœ¨å…¶ä»–åœ°æ–¹ä»‹ç»self.unique_no_split_tokensã€‘ã€‚å…¥å‚ `special_tokens=False`, é‚£ä¹ˆå°±éœ€è¦ç»è¿‡å‰ä¸€æ­¥ç­›é€‰å†ä½œä¸ºä¸å¯åˆ†å‰²çš„ token åŠ å…¥åˆ°è¯è¡¨ä¸­
- è°ƒç”¨ `self._create_trie`, ä¾¿äºtokenizeçš„æ—¶å€™å…ˆä¿è¯ä¸å¯åˆ†å‰²çš„è¯ä¸è¢«åˆ‡å¼€

å› æ­¤, `add_tokens(tokens, special_tokens=False)`çš„è¡Œä¸ºæ˜¯:

- å¦‚æœè¢«åŠ å…¥çš„tokenä¸åœ¨è¯è¡¨å†…, åˆ™ä¸ºå…¶å¢åŠ å¯¹åº”çš„token_id, å¹¶ä¸”å°†è¢«åŠ å…¥çš„tokenä¸å¯åˆ†å‰²
- å¦‚æœè¢«åŠ å…¥çš„tokenåœ¨è¯è¡¨å†…, åˆ™ä»€ä¹ˆéƒ½ä¸åš

`add_tokens(tokens, special_tokens=True)` çš„è¡Œä¸ºæ˜¯:

- å¦‚æœè¢«åŠ å…¥çš„tokenä¸åœ¨è¯è¡¨å†…, åˆ™ä¸ºå…¶å¢åŠ å¯¹åº”çš„token_id, å¹¶ä¸”å°†è¢«åŠ å…¥çš„tokenä¸å¯åˆ†å‰²
- å¦‚æœè¢«åŠ å…¥çš„tokenåœ¨è¯è¡¨å†…, åˆ™å°†å…¶ä½œä¸ºä¸å¯åˆ†å‰²çš„token


**fast tokenizer**

å¯¹äº fast tokenizer, `_add_tokens` çš„å®ç°å¦‚ä¸‹
```python
def _add_tokens(self, new_tokens: List[Union[str, AddedToken]], special_tokens=False) -> int:
    if special_tokens:
        return self._tokenizer.add_special_tokens(new_tokens)
    return self._tokenizer.add_tokens(new_tokens)
```
æ‰€ä»¥æœ¬è´¨ä¸Šå›åˆ°äº† ğŸ¤— Tokenizers ä¸­ `tokenizers.Tokenizer` çš„ä¸¤ä¸ªæ–¹æ³•: `add_special_tokens`, `add_tokens`

æ€»ç»“å¦‚ä¸‹ï¼š

- ğŸ¤— Tokenizers ä¸­ `tokenizers.Tokenizer` ç›¸å…³çš„æ–¹æ³•æ€»ç»“å¦‚ä¸‹:
  - `Tokenizer.add_tokens(tokens: List[Union[AddedToken, str]])`: å¦‚æœåŠ å…¥çš„tokenä¸åœ¨åŸæœ¬çš„è¯è¡¨å†…, åˆ™ä¸ºå…¶å¢åŠ token_id, å¹¶ä¿è¯æ–°å¢çš„tokenä¸ä¼šè¢«åˆ‡åˆ†å¼€ï¼›å¦‚æœåŠ å…¥çš„tokenåœ¨åŸæœ¬çš„è¯è¡¨å†…, åˆ™ä»€ä¹ˆéƒ½ä¸åš(å³å®ƒåœ¨tokenizeé˜¶æ®µä»æœ‰å¯èƒ½è¢«åˆ‡åˆ†å¼€)
  - `Tokenizer.add_special_tokens(tokens: List[Union[AddedToken, str]])`: å¦‚æœåŠ å…¥çš„tokenä¸åœ¨åŸæœ¬çš„è¯è¡¨å†…, åˆ™ä¸ºå…¶å¢åŠ token_id, å¹¶ä¿è¯æ–°å¢çš„tokenä¸ä¼šè¢«åˆ‡åˆ†å¼€ï¼›å¦‚æœåŠ å…¥çš„tokenåœ¨åŸæœ¬çš„è¯è¡¨å†…, åˆ™ä¸ä¸ºå…¶å¢åŠ token_id, ä½†ä¿è¯å®ƒä¸ä¼šè¢«åˆ‡åˆ†å¼€
- ğŸ¤— Transformers ä¸­çš„ slow/fast tokenizer çš„ç›¸å…³æ–¹æ³•æ€»ç»“å¦‚ä¸‹:
  - `PretrainedTokenizerBase.add_tokens(tokens, special_tokens=False)`çš„è¡Œä¸ºæ˜¯:
    - å¦‚æœè¢«åŠ å…¥çš„tokenä¸åœ¨è¯è¡¨å†…, åˆ™ä¸ºå…¶å¢åŠ å¯¹åº”çš„token_id, å¹¶ä¸”å°†è¢«åŠ å…¥çš„tokenä¸å¯åˆ†å‰²
    - å¦‚æœè¢«åŠ å…¥çš„tokenåœ¨è¯è¡¨å†…, åˆ™ä»€ä¹ˆéƒ½ä¸åš
  - `PretrainedTokenizerBase.add_tokens(tokens, special_tokens=True)`çš„è¡Œä¸ºæ˜¯:
    - å¦‚æœè¢«åŠ å…¥çš„tokenä¸åœ¨è¯è¡¨å†…, åˆ™ä¸ºå…¶å¢åŠ å¯¹åº”çš„token_id, å¹¶ä¸”å°†è¢«åŠ å…¥çš„tokenä¸å¯åˆ†å‰²
    - å¦‚æœè¢«åŠ å…¥çš„tokenåœ¨è¯è¡¨å†…, åˆ™å°†å…¶ä½œä¸ºä¸å¯åˆ†å‰²çš„token
  - `PretrainedTokenizerBase.add_special_tokens(special_tokens_dict)`: åªèƒ½å¢åŠ  8 ç§ç‰¹æ®Š token, é¦–å…ˆè®¾ç½®ç›¸å…³çš„å±æ€§, ä¾‹å¦‚: `self.cls_token`, ç„¶åè°ƒç”¨ `PretrainedTokenizerBase.add_tokens(tokens, special_tokens=True)`, è¿™ä¹ˆä¸€æ¥ä¿è¯åŠ å…¥çš„tokenæ€»æ˜¯ä¸ä¼šè¢«åˆ‡åˆ†å¼€


### vocabulary

å¯¹äºä¸€ä¸ªç‰¹å®šçš„ tokenizer, æˆ‘ä»¬ç°åœ¨å·²ç»çŸ¥é“ï¼Œå®ƒè¯è¡¨é‡Œçš„tokenä¸€èˆ¬åˆ†ä¸ºå‡ ç±»

- (1) æ™®é€š token
- (2) SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES
- (3) é€šè¿‡`add_tokens`æ·»åŠ çš„ token

é‚£ä¹ˆåœ¨è·å–è¯è¡¨æ—¶æˆ–è®¡ç®—è¯è¡¨é•¿åº¦æ—¶ï¼Œå°±ä¼šå‡ºç°å‡ ç§ä¸åŒçš„è®¡ç®—æ–¹å¼ï¼Œæœ‰å¿…è¦ç†æ¸…ä¸€ä¸‹ï¼š

**base tokenizer**

åœ¨åŸºç±» `PretrainedTokenizerBase` çš„çˆ¶ç±» `SpecialTokensMixin` ä¸­

```python
class SpecialTokensMixin:
    @property
    def special_tokens_map(self) -> Dict[str, Union[str, List[str]]]:
        set_attr = {}
        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:
            attr_value = getattr(self, "_" + attr)
            if attr_value:
                set_attr[attr] = (
                    type(attr_value)(str(attr_value_sub) for attr_value_sub in attr_value)
                    if isinstance(attr_value, (list, tuple))
                    else str(attr_value)
                )
        return set_attr

    @property
    def special_tokens_map_extended(self) -> Dict[str, Union[str, AddedToken, List[Union[str, AddedToken]]]]:
        set_attr = {}
        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:
            attr_value = getattr(self, "_" + attr)
            if attr_value:
                set_attr[attr] = attr_value
        return set_attr

    @property
    def all_special_tokens(self) -> List[str]:
        all_toks = [str(s) for s in self.all_special_tokens_extended]
        return all_toks

    @property
    def all_special_tokens_extended(self) -> List[Union[str, AddedToken]]:
        all_toks = []
        set_attr = self.special_tokens_map_extended
        for attr_value in set_attr.values():
            all_toks = all_toks + (list(attr_value) if isinstance(attr_value, (list, tuple)) else [attr_value])
        all_toks = list(OrderedDict.fromkeys(all_toks))
        return all_toks

    @property
    def all_special_ids(self) -> List[int]:
        all_toks = self.all_special_tokens
        all_ids = self.convert_tokens_to_ids(all_toks)
        return all_ids
```

è¿™äº›å±æ€§éƒ½ä»…æ˜¯ç¬¬(2)ç±»tokençš„é›†åˆ

- `special_tokens_map_extended`: `bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens` å…«è€…çš„æ€»å’Œ, valueå¯ä»¥æ˜¯ğŸ¤— Tokenizers ä¸­ `tokenizers.AddedToken` ç±»å‹
- `special_tokens_map`: ä¸ä¸Šè€…ç›¸åŒ, å”¯ä¸€çš„åŒºåˆ«æ˜¯valueä¸€å®šæ˜¯str
- `all_special_tokens_extended`: `special_tokens_map_extended` çš„valueåˆ—è¡¨
- `all_special_tokens`: ä¸ä¸Šè€…ç›¸åŒ, å”¯ä¸€çš„åŒºåˆ«æ˜¯valueä¸€å®šæ˜¯str
- `all_special_ids`: ä½¿ç”¨ `convert_tokens_to_ids` è½¬æ¢ç¬¬(2)ç±»tokençš„token_idåˆ—è¡¨

**slow tokenizer**

- `added_tokens_encoder: Dict[str, int]` å±æ€§: åªåœ¨è°ƒç”¨ `add_tokens` æ—¶è¢«æ›´æ–°, åœ¨å°† token è½¬åŒ–ä¸º id æ—¶è¢«ä½¿ç”¨, å³åªåŒ…å«ç¬¬(3)éƒ¨åˆ†çš„token
- `added_tokens_decoder: Dict[int, str]` å±æ€§: åªåœ¨è°ƒç”¨ `add_tokens` æ—¶è¢«æ›´æ–°, åœ¨å°† id è½¬åŒ–ä¸º token æ—¶è¢«ä½¿ç”¨, å³åªåŒ…å«ç¬¬(3)éƒ¨åˆ†çš„token
- `__len__()`: `self.vocab_size+len(self.added_tokens_encoder)`, å³åŒ…å«(1)(2)(3)å…¨éƒ¨çš„token
- `vocab_size`: ç”±å…·ä½“çš„ tokenizer å®ç°, åŒ…å«çš„æ˜¯ç¬¬(1)å’Œç¬¬(2)éƒ¨åˆ†çš„tokenæ•°é‡
- `get_vocab`: ç”±å…·ä½“çš„ tokenizer å®ç°, è¿”å› `Dict[str, int]`, å³åŒ…å«(1)(2)(3)å…¨éƒ¨çš„token
- `get_added_vocab()`: è¿”å› `self.added_tokens_encoder`, å³ç¬¬(3)éƒ¨åˆ†token
- `unique_no_split_tokens`: ä¸€èˆ¬æƒ…å†µä¸‹ï¼ˆä½¿ç”¨`from_pretrained`æ–¹æ³•åˆå§‹åŒ–slow tokenizeræ—¶ï¼‰åŒ…å«ç¬¬(2)å’Œç¬¬(3)éƒ¨åˆ†çš„token
- `tokens_trie`: ç”±`unique_no_split_tokens`æ„æˆçš„`Trie`æ•°æ®ç»“æ„
- `save_vocabulary`: ç”±å…·ä½“çš„ tokenizer å®ç°, å®ç°çš„é€»è¾‘æ˜¯ä¿å­˜ç¬¬(1)å’Œç¬¬(2)éƒ¨åˆ†tokenã€‚`save_pretrained` æ–¹æ³•ä¼šé¢å¤–å¤„ç†ä»¥ä¸‹ä¸¤ä»¶äº‹ï¼šå°†ç¬¬(3)éƒ¨åˆ†tokenä¿å­˜åœ¨`added_token.json` æ–‡ä»¶å†…, ç¬¬(2)éƒ¨åˆ†tokenè¿˜ä¼šåŒæ—¶å†åº¦è¢«ä¿å­˜åœ¨`special_tokens_map.json`æ–‡ä»¶ä¸­

**fast tokenizer**

fast tokenizer ä¸ slow tokenizer åœ¨ç›¸åŒå‘½åçš„å±æ€§/æ–¹æ³•ä¸Šçš„å«ä¹‰æ˜¯ç›¸åŒçš„ 

- `vocab_size`: åŒ…å«çš„æ˜¯ç¬¬(1)å’Œç¬¬(2)éƒ¨åˆ†çš„tokenæ•°é‡
  ```python
  @property
  def vocab_size(self) -> int:
      # `int`: Size of the base vocabulary (without the added tokens).
      return self._tokenizer.get_vocab_size(with_added_tokens=False)
  ```
- `get_vocab`æ–¹æ³•ä¸`vocab`å±æ€§ä¸€è‡´: åŒ…å«(1)(2)(3)ä¸‰éƒ¨åˆ†token
  ```python
  def get_vocab(self) -> Dict[str, int]:
      return self._tokenizer.get_vocab(with_added_tokens=True)
  ```
- `get_added_vocab`: `Dict[str, int]`, ç¬¬(3)éƒ¨åˆ†çš„token
  ```python
  def get_added_vocab(self) -> Dict[str, int]:
      base_vocab = self._tokenizer.get_vocab(with_added_tokens=False)
      full_vocab = self._tokenizer.get_vocab(with_added_tokens=True)
      added_vocab = dict((tok, index) for tok, index in full_vocab.items() if tok not in base_vocab)
      return added_vocab
  ```
- `__len__()`: åŒ…å«(1)(2)(3)ä¸‰éƒ¨åˆ†token
  ```python
  def __len__(self) -> int:
      return self._tokenizer.get_vocab_size(with_added_tokens=True)
  ```


### å®ä¾‹åŒ–ä¸åºåˆ—åŒ–ã€TODOï¼šä»£ç å·²æŠ„å®Œ, ä½†è”ç³»æ²¡ææ¸…æ¥šã€‘

æœ¬èŠ‚ä¸»è¦æ¶‰åŠå¦‚ä¸‹æ–¹æ³• `__init__`ã€`from_pretrained`ã€`save_pretrained`ï¼Œä»¥åŠä¸€äº› 3 ä¸ªåŸºç±»å®šä¹‰çš„ä¸€äº›å±æ€§

#### ç±»å±æ€§ä¸`__init__`

```python
class SpecialTokensMixin:
    def __init__(self, verbose=True, **kwargs):
        self._bos_token, self._eos_token, self._unk_token self._sep_token = ...  # kwargs
        self._pad_token, self._cls_token, self._mask_token = ...  # kwargs
        self._pad_token_type_id = 0
        self._additional_special_tokens = ...  # kwargs
        self.verbose = verbose

class PretrainedTokenizerBase(SpecialTokensMixin, PushToHubMixin):
    vocab_files_names: Dict[str, str] = {}                           # specific slow tokenizer éœ€æŒ‡å®š, fast tokenizer ä¸ºå›ºå®šå€¼ {"tokenizer_file": "tokenizer.json"}, ç”¨äº __init__, from_pretrained, save_pretrained
    pretrained_vocab_files_map: Dict[str, Dict[str, str]] = {}       # specific tokenizer éœ€æŒ‡å®š, "å®˜æ–¹"æ¨¡å‹çš„vocab_files_names
    pretrained_init_configuration: Dict[str, Dict[str, Any]] = {}    # ??? specific tokenizer éœ€æŒ‡å®š, "å®˜æ–¹"æ¨¡å‹çš„init_kwargs
    max_model_input_sizes: Dict[str, Optional[int]] = {}             # specific tokenizer éœ€æŒ‡å®š, "å®˜æ–¹"æ¨¡å‹çš„max_model_input_sizes
    _auto_class: Optional[str] = None                                # ???
    model_input_names: List[str] = ["input_ids", "token_type_ids", "attention_mask"]  # ??? specific tokenizer éœ€æŒ‡å®š
    padding_side: str = "right"
    truncation_side: str = "right"
    slow_tokenizer_class = None                                      # specific fast tokenizer éœ€è®¾å®š
    def __init__(self, **kwargs):
        # inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)
        self.init_inputs = ()
        self.init_kwargs = copy.deepcopy(kwargs)
        self.name_or_path = ...                   # kwargs
        self._processor_class = ...               # kwargs, ???
        self.model_max_length = ...               # kwargs, default: int(1e30)
        self.padding_side = ...                   # kwargs, default: cls.padding_side
        self.truncation_side = ...                # kwargs, default: cls.padding_side
        self.model_input_names = ...              # kwargs, default: cls.model_input_names
        self.clean_up_tokenization_spaces = ...   # default True
        self.deprecation_warnings = ({})
        self._in_target_context_manager = False   # ???


class PretrainedTokenizer(PreTrainedTokenizerBase):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # Added tokens - We store this for both slow and fast tokenizers
        # until the serialization of Fast tokenizers is updated
        self.added_tokens_encoder: Dict[str, int] = {}
        self.added_tokens_decoder: Dict[int, str] = {}
        self.unique_no_split_tokens: List[str] = []
        self.tokens_trie = Trie()

        self._decode_use_source_tokenizer = False  # ???

class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
    vocab_files_names = {"tokenizer_file": "tokenizer.json"} # æ‰€æœ‰çš„ specific fast tokenizer éƒ½æ˜¯è¿™ä¸ª
    slow_tokenizer_class: PreTrainedTokenizer = None         # specific fast tokenizer éœ€æŒ‡å®š, ä¾‹å¦‚: T5TokenizerFast ä¸­è®¾ä¸º T5Tokenizer
    can_save_slow_tokenizer: bool = True                     # æ§åˆ¶ save_pretrained çš„è¡Œä¸º
    def __init__(self, *args, **kwargs):
        self._tokenizer: tokenizers.Tokenizer                # construct directly / convert slow tokenizer to fast tokenizer
        self._decode_use_source_tokenizer = False
        # We call this after having initialized the backend tokenizer because we update it.
        super().__init__(**kwargs)
```


#### from_pretrained

**base tokenizer**

```python
@classmethod
def from_pretrained(cls, pretrained_model_name_or_path, ...):
    # ä¸€å †ä¸‹è½½ä¸ç¼“å­˜å¿…è¦çš„æ–‡ä»¶çš„æ“ä½œ
    # resolved_vocab_files ä¸€èˆ¬ä¼šåŒ…å«å¦‚ä¸‹key, è‹¥å¯¹åº”çš„valueçš„æ–‡ä»¶å­˜åœ¨
    # {
    #    **cls.vocab_files_names,
    #    "added_tokens_file": "added_tokens.json",
    #    "special_tokens_map_file": "special_tokens_map.json",
    #    "tokenizer_config_file": "tokenizer_config.json"
    #  }
    return cls._from_pretrained(resolved_vocab_files, pretrained_model_name_or_path, ...)
@classmethod
def _from_pretrained(cls, pretrained_model_name_or_path, ...):
    ... # ä¸€å †æ“ä½œ
    tokenizer = cls(*args, **kwargs)
    tokenizer.add_tokens(...)
```

#### save_pretrained

**base tokenizer**
```python
def save_pretrained(self, save_directory, legacy_format: Optional[bool] = None, filename_prefix=None, push_to_hub=False, **kwargs):
    ...  # ä¿å­˜ä¿¡æ¯è‡³ tokenizer_config.json ä¸­å»
    ...  # å°†self.special_tokens_map_extendedä¿å­˜è‡³special_tokens_map.json ä¸­å»
    file_names = (tokenizer_config_file, special_tokens_map_file)
    save_files = self._save_pretrained(save_directory, file_names, legacy_format, filename_prefix)
    return save_files

def _save_pretrained(save_directory, file_names, legacy_format, filename_prefix):
    ...  # å°†self.get_added_vocab()ä¿å­˜è‡³added_tokens.jsonä¸­å»
    vocab_files = self.save_vocabulary(save_directory, filename_prefix)  # å­ç±»å®ç°
    return file_names + vocab_files + (added_tokens_file,)
```

**fast tokenizer**
```python
# è¦†ç›–çˆ¶ç±»æ–¹æ³•
def _save_pretrained(save_directory, file_names, legacy_format, filename_prefix):
    # legacy_format ä¸º None, åˆ™å°½é‡åˆ†åˆ«ä¿å­˜ slow tokenizer çš„æ–‡ä»¶ä»¥åŠ fast tokenizer çš„æ–‡ä»¶
    # legacy_format ä¸º False, åˆ™åªä¿å­˜ fast tokenizer çš„æ–‡ä»¶
    # legacy_format ä¸º True, åˆ™åªä¿å­˜ slow tokenizer çš„æ–‡ä»¶
    if save_slow:
        ...  # å°†self.get_added_vocab()ä¿å­˜è‡³added_tokens.jsonä¸­å»
        vocab_files = self.save_vocabulary(save_directory, filename_prefix)  # å­ç±»å®ç°
        file_names = file_names + vocab_files + (added_tokens_file,)
    if save_fast:
        self.backend_tokenizer.save(tokenizer_file)  # fast tokenizer åªéœ€è¦ä¿å­˜tokenizer.json
        file_names = file_names + (tokenizer_file,)
    return filenames
```

å› æ­¤ specific slow tokenizer å¿…é¡»å®ç° `save_vocabulary` æ–¹æ³•, ç”¨æ¥ä¿å­˜ç¬¬(1)(2)ç±»token???ã€å¾…ææ¸…æ¥šã€‘, è€Œ specific fast tokenizer ä¹Ÿå°½é‡å®ç° `save_vocabulary` æ–¹æ³•, ä»¥æ”¯æŒå¯¹åº”çš„ slow tokenizer çš„ä¿å­˜ã€‚å…·ä½“å¯å‚è€ƒ T5Tokenizer ä¸ T5TokenizerFast çš„å®ç°ã€‚


### `BatchEncoding`

åœ¨ä»‹ç» `__call__` æ–¹æ³•çš„é€»è¾‘ä¹‹å‰, å…ˆå¯¹å®ƒçš„è¿”å›å€¼åšç®€å•ä»‹ç»ã€‚

æç¤º: ä¸‹é¢ä»‹ç»çš„ fast tokenizer ä¸“å±çš„å±æ€§å®é™…æ¥æºäº backend_tokenizer å¤„ç†åçš„è¿”å›ç±»å‹ `Tokenizer.Encoding`ï¼Œ`PretrainedTokenizerFast` ä¸­çš„ `_convert_encoding` æ–¹æ³•ç”¨äºå°† `Tokenizer.Encoding` è½¬æ¢ä¸ºå­—å…¸å½¢å¼, æœ€ç»ˆå†è½¬æ¢ä¸º `transformers.BatchEncoding` ä½œä¸º `__call__` æ–¹æ³•çš„è¿”å›å€¼


é¦–å…ˆçœ‹ä¸€ä¸ªä½¿ç”¨ç¤ºä¾‹ï¼š

```python
from transformers import AutoTokenizer  # AutoTokenizeræ€»æ˜¯å°è¯•åŠ è½½fastç‰ˆæœ¬çš„tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")
encodings = tokenizer(["This is a sentence"])
```

æ­¤å¤„çš„ `encodings` çš„ç±»å‹ä¸º `BatchEncoding` ç±»å‹, å®ƒç»§æ‰¿è‡ª `UserDict`ï¼Œå³ç»§æ‰¿è‡ªå­—å…¸ã€‚é™¤äº†å­—å…¸çš„æ–¹æ³•å¤–ï¼Œå®ƒè¿˜å…·å¤‡ä»¥ä¸‹æ–¹æ³•

```python
inputs = encoding.convert_to_tensors("pt")  # inplaceæ“ä½œ, å°†å†…éƒ¨çš„valueä¾‹å¦‚input_idsç­‰è½¬æ¢ä¸ºtensor
inputs = encoding.to("cuda:0")  # inplaceæ“ä½œ, æ”¹å˜è®¾å¤‡
# æ³¨: ä»¥ä¸‹æ–¹æ³•ä»…ä½¿ç”¨äº fast ç‰ˆæœ¬çš„ tokenizer çš„æƒ…å½¢
# æ³¨: ä»¥ä¸‹æ–¹æ³•å¯¹äºç‰¹æ®Štokenä¾‹å¦‚[CLS], è¿”å›ç»“æœä¼šå¤„ç†æˆNone
encoding.tokens()                     # List[str], æ‰€æœ‰çš„tokenå­—ç¬¦ä¸², å‡è®¾é•¿åº¦ä¸ºN
# tokenizeä¸€ä¸ªbatchçš„æ•°æ®æ—¶, æ³¨æ„éœ€è¦è°ƒæ•´å…¥å‚, encoding.tokens(i), iä¸ºç¬¬å‡ ä¸ªæ ·æœ¬, ä¸‹åŒ
encoding.word_ids()                   # List[int], æ¯ä¸ªtokenæ‰€åœ¨çš„word_idx, æ³¨æ„è¿™é‡Œçš„wordçš„æ¦‚å¿µé€šå¸¸å–å†³äºpre-tokenizerçš„å®šä¹‰
encoding.sequence_ids()               # List[int], æ¯ä¸ªtokenæ‰€åœ¨çš„sequence_idx, è¿™é‡Œçš„sequenceçš„æ¦‚å¿µå–å†³äºpre-tokenizerçš„å®šä¹‰
encoding.token_to_word(token_idx)     # ç¬¬token_idxä¸ªtokenæ‰€åœ¨çš„word_idx
encoding.token_to_sequence(token_idx) # ç¬¬token_idxä¸ªtokenæ‰€åœ¨çš„sequence_idx
start, end = encoding.word_to_chars(word_idx)      # ç¬¬word_idxä¸ªwordå¯¹åº”çš„åŸå§‹stringçš„èµ·å§‹/ç»“æŸä½ç½®
start, end = encoding.word_to_tokens(word_idx)     # ç¬¬word_idxä¸ªwordå¯¹åº”çš„èµ·å§‹ä¸ç»“æŸçš„token_idx
start, end = encoding.token_to_chars(token_idx)    # ç¬¬token_idxä¸ªtokenå¯¹åº”çš„åŸå§‹stringçš„èµ·å§‹/ç»“æŸä½ç½®
word_idx = encoding.char_to_word(i)                # åŸå§‹stringä¸­ç¬¬iä¸ªå­—ç¬¦å¯¹åº”çš„word_idx
token_idx = encoding.char_to_token(i)              # åŸå§‹stringä¸­ç¬¬iä¸ªå­—ç¬¦å¯¹åº”çš„token_idx
```

ç®€å•æ¥è¯´, fast ç‰ˆæœ¬çš„ tokenizer çš„ encode è¿‡ç¨‹ä¿å­˜äº†åŸå§‹å­—ç¬¦ä¸²ä¸­æ¯ä¸ªå­—ç¬¦ä¸token, word, sequenceçš„å¯¹åº”å…³ç³», è€Œ slow ç‰ˆæœ¬ä¸å…·å¤‡


### encode: `PretrainedTokenizerBase.__call__`ã€TODOã€‘

å®˜æ–¹å»ºè®®ä¸è¦ç›´æ¥è°ƒç”¨ `batch_encode_plus`ï¼Œ`encode_plus` æ–¹æ³•ï¼Œè€Œæ˜¯é€šè¿‡ `__call__` æ–¹æ³•æ¥è°ƒç”¨ã€‚è¿™ä¸€è¿‡ç¨‹å®é™…èµ·ä½œç”¨çš„â€œç»„ä»¶â€å‡½æ•°ä¸ºï¼š`encode`ã€`convert_tokens_to_ids`ã€`prepare_for_model`ã€‚

ä¸åšè¯´æ˜çš„æƒ…å†µä¸‹ï¼Œé»˜è®¤æŒ‡çš„æ˜¯`PretrainedTokenizerBase`çš„æ–¹æ³•ï¼Œé¦–å…ˆå¯¹ `__call__` æ–¹æ³•çš„é‡è¦çš„è¾“å…¥å‚æ•°åšä»‹ç»

```python
def __call__(
    self,
    text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,
    text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,
    text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,
    text_pair_target: Optional[
        Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]
    ] = None,
    add_special_tokens: bool = True,  # æ˜¯å¦éœ€è¦åœ¨tokenizeä¹‹åæ·»åŠ ä¸€äº›ç‰¹æ®Štoken(ä¾‹å¦‚èµ·å§‹ç»“æŸtoken), éœ€å…·ä½“çš„tokenizerå®ç°ï¼Œå…·ä½“è§åé¢è¯´æ˜
    padding: Union[bool, str, PaddingStrategy] = False,  # padding, truncation, max_lengthè§åé¢è¯´æ˜
    truncation: Union[bool, str, TruncationStrategy] = None,
    max_length: Optional[int] = None,
    is_split_into_words: bool = False,  # è§åé¢è¯´æ˜
    return_overflowing_tokens: bool = False,  # è¿”å›è¢«æˆªæ–­çš„éƒ¨åˆ†
    return_offsets_mapping: bool = False,  # è¿”å›æ¯ä¸ªtoken_idå¯¹åº”äºåŸå§‹æ–‡æœ¬çš„èµ·å§‹ä½ç½®(slow tokenizerä¸æ”¯æŒæ­¤ç‰¹æ€§)
    ...
) -> BatchEncoding:
    # ä»¥ä¸‹ä¸ºå¤§ä½“é€»è¾‘, æœ‰åˆ æ”¹
    if text is not None:
        if not self._in_target_context_manager:
            self._switch_to_input_mode()
        encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
    if text_target is not None:
        self._switch_to_target_mode()
        target_encodings = self._call_one(text=text_target, text_pair=text_pair_target, **all_kwargs)
    self._switch_to_input_mode()
    if text_target is None:
        return encodings
    elif text is None:
        return target_encodings
    else:  # sourceå’Œtargetéƒ½ç»™çš„æ—¶å€™, åªæŠŠtarget encodingsç»“æœä¸­çš„input_idsä½œä¸ºlabelsæ·»åŠ åˆ°sourceçš„encodingç»“æœé‡Œ
        encodings["labels"] = target_encodings["input_ids"]
    return encodings
# æ³¨æ„ text_pair æŒ‡çš„æ˜¯ç¬¬2ä¸ªå¥å­, è€Œéå¥å­å¯¹

# text/text_pair/text_target/text_pair_targetçš„æ•°æ®ç±»å‹ä¸ºä»¥ä¸‹4ç§æƒ…å†µ:
TextInput = str  # å³æ•´å¥è¯, è½¬ä¸ºtokenåºåˆ—
List[TextInput] = List[str] # batchç‰ˆæœ¬, å¤šå¥è¯åˆ†åˆ«è½¬ä¸ºtokenåºåˆ—
PreTokenizedInput = List[str]  # å·²ç»é¢„å…ˆåˆ‡å¥½"è¯"çš„åºåˆ—, è¿™ä¸ªæ—¶å€™ä¼šå¯¹æ¯ä¸ªå°æ®µè¿›è¡ŒtokenåŒ–, æœ€åæ‹¼æ¥åœ¨ä¸€èµ·
List[PreTokenizedInput] = List[List[str]]  # å¤šä¸ªå·²ç»ä¸”ä¸ºå°æ®µçš„å¥å­
# ä¸€ä¸ª PreTokenizedInput çš„ä½¿ç”¨ä¾‹å­æ˜¯: ["ç´«ç¦åŸ", "æ˜¯xxx,åè½äº", "åŒ—äº¬"]
# å¾—åˆ°çš„åºåˆ—ä¼šæ˜¯ List[int] = tokenize("ç´«ç¦åŸ") + tokenize(æ˜¯xxx,åè½äº) + tokenize("åŒ—äº¬")
# ä¿è¯å‘½åå®ä½“æœ¬èº«ä¸ä¼šè¢«åˆ‡åˆ†å¼€æ¥
```

å¯¹è¾“å…¥å‚æ•°åšç®€è¦è§£é‡Šå¦‚ä¸‹

**`text`, `text_pair`, `text_target`, `text_pair_target`**

éœ€è¦è¢«åºåˆ—åŒ–ä¸ºæ•´æ•°çš„â€œä¸œè¥¿â€ï¼ŒæŸäº› tokenizer å¯¹ source(è¾“å…¥) å’Œ target(è¾“å‡º) çš„åºåˆ—åŒ–æ–¹å¼å¯èƒ½æœ‰æ‰€ä¸åŒ, æ‰€ä»¥ç•™äº†

**`add_special_tokens`**

è¡¨ç¤ºæ˜¯å¦éœ€è¦åœ¨tokenizeä¹‹åæ·»åŠ ä¸€äº›ç‰¹æ®Štoken(ä¾‹å¦‚èµ·å§‹ç»“æŸtoken), é»˜è®¤å€¼ä¸ºTrueï¼Œè¿™ä¸ªå‚æ•°åœ¨ `prepare_for_model` ä¸­ç”¨åˆ°ï¼Œä¸åŒçš„ tokenizer éœ€è¦é€šè¿‡é‡è½½å¦‚ä¸‹å‡ ä¸ªæ–¹æ³•è¿›è¡Œå®ç°ï¼š
  ```python
  def prepare_for_model(self, ...):
      # å‰åºå¤„ç†çœç•¥, ä¸»è¦åŒ…æ‹¬truncate

      # Add special tokens
      if add_special_tokens:
          sequence = self.build_inputs_with_special_tokens(ids, pair_ids)  # è¿½åŠ ç‰¹æ®Štoken
          token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)  # é€šå¸¸ç¬¬1å¥è¯çš„ä½ç½®ä¸º0ï¼Œç¬¬2å¥çš„ä½ç½®ä¸º1
      else:
          sequence = ids + pair_ids if pair else ids
          token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])

      # Build output dictionary
      encoded_inputs["input_ids"] = sequence
      if return_token_type_ids:
          encoded_inputs["token_type_ids"] = token_type_ids
      if return_special_tokens_mask:
          if add_special_tokens:
              encoded_inputs["special_tokens_mask"] = self.get_special_tokens_mask(ids, pair_ids)
          else:
              encoded_inputs["special_tokens_mask"] = [0] * len(sequence)
      # åç»­å¤„ç†ä¸»è¦æ˜¯pad
  
  # ä¸Šé¢å‡ ä¸ªæ–¹æ³•çš„é»˜è®¤å®ç°å¦‚ä¸‹:
  def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1):
      if token_ids_1 is None:
          return token_ids_0
      return token_ids_0 + token_ids_1
  def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1):
      if token_ids_1 is None:
          return len(token_ids_0) * [0]
      return [0] * len(token_ids_0) + [1] * len(token_ids_1)
  def get_special_tokens_mask(self, token_ids_0, token_ids_1, already_has_special_tokens=False):
      # 1 ä»£è¡¨ special token, 0 ä»£è¡¨æ™®é€šçš„ token
      all_special_ids = self.all_special_ids  # cache the property
      special_tokens_mask = [1 if token in all_special_ids else 0 for token in token_ids_0]
      return special_tokens_mask

  ```
  ä¾‹å­: `BertTokenizer`ï¼Œ`T5Tokenizer` å‡å¯¹ `build_inputs_with_special_tokens`ã€`create_token_type_ids_from_sequences`ã€`get_special_tokens_mask`åšé‡è½½ã€‚å¹¶ä¸”ä¹Ÿæ˜¯è¿™ä¸¤ä¸ª tokenizer é™¤äº†å¿…é¡»å®ç°çš„5ä¸ªæ–¹æ³• `save_vocabulary`ã€`get_vocab`ã€`_tokenize`ã€`_convert_token_to_id`ã€`convert_id_to_token` ä»¥å¤–çš„å…¨éƒ¨é‡è½½æ–¹æ³•ã€‚ï¼ˆå¯¹äºdecodeè¿‡ç¨‹ï¼Œ`T5Tokenizer`è¿˜é‡è½½äº†`convert_tokens_to_string`ï¼‰

**`padding`ã€`truncate`ã€`max_length`ã€`is_split_into_words`**

is_split_into_words ä¸è°ƒç”¨ `_batch_encode_plus` è¿˜æ˜¯è°ƒç”¨ `_encode_plus` æ˜¯ç›¸å…³çš„ã€å¾…è¡¥å……ã€‘

å¤‡æ³¨ï¼š

- `truncation_side` å–å€¼ä¸º `"left"` è¡¨ç¤ºæˆªæ–­æ—¶å»æ‰å·¦è¾¹çš„å­—ç¬¦ï¼Œå–å€¼ä¸º `"right"` è¡¨ç¤ºæˆªæ–­æ—¶å»æ‰å³è¾¹çš„å­—ç¬¦

**`__call__`æ–¹æ³•çš„è°ƒç”¨æµç¨‹**

`__call__` æ–¹æ³•çš„å…·ä½“æµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆå°†éœ€è¦è½¬æ¢ä¸º token åºåˆ—çš„è¾“å…¥åˆ†ä¸ºä¸¤ç»„ `text, text_pair` å’Œ `text_target, text_pair_target`ï¼Œåˆ†åˆ«è°ƒç”¨ `_call_one` æ–¹æ³•ï¼Œç„¶åå°†ä¸¤éƒ¨åˆ†è¿›è¡Œåˆå¹¶ã€‚è€Œ `_call_one` æ–¹æ³•æ ¹æ® `text` æˆ– `text_target` çš„å˜é‡ç±»å‹ä»¥åŠ `is_split_into_words` å‚æ•°çš„å–å€¼ç¡®å®šè¿›ä¸€æ­¥è°ƒç”¨ä¸¤è€…ä¹‹ä¸€: `batch_encode_plus` æˆ–æ˜¯ `encode_plus`ï¼Œæ­¤æ—¶æ³¨æ„è¿™ä¸¤ä¸ªå‡½æ•°çš„å‡½æ•°ç­¾åå¦‚ä¸‹:

```python
EncodedInput=List[int]
EncodedInputPair=Tuple[List[int], List[int]]

def batch_encode_plus(
    self,
    batch_text_or_text_pairs: Union[
        List[TextInput],  # List[str]
        List[TextInputPair],  # List[Tuple[str, str]]
        List[PreTokenizedInput],  # List[List[str]]
        List[PreTokenizedInputPair],  # List[Tuple[List[str], List[str]]]
        List[EncodedInput],  # å¦‚æœåªçœ‹ __call__ æ–¹æ³•docstring, åœ¨è°ƒç”¨__call__æ–¹æ³•æ—¶, ä¸å¯èƒ½ä»¥è¿™ç§å˜é‡ç±»å‹è§¦å‘batch_encode_plusæ–¹æ³•
        List[EncodedInputPair],  # å¦‚æœåªçœ‹ __call__ æ–¹æ³•docstring, åœ¨è°ƒç”¨__call__æ–¹æ³•æ—¶, ä¸å¯èƒ½ä»¥è¿™ç§å˜é‡ç±»å‹è§¦å‘batch_encode_plusæ–¹æ³•
    ],
    ...
) -> BatchEncoding:
    ...

def encode_plus(
    self,
    text: Union[TextInput, PreTokenizedInput, EncodedInput],  # åŒç†EncodeInputè¿™ç§ç±»å‹æŒ‰ç†ä¹Ÿä¸ä¼šè§¦å‘
    text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,
    ...
) -> BatchEncoding:
    ...
```

è€Œè¿™ä¸¤ä¸ªæ–¹æ³•é¦–å…ˆæ ¹æ®è¾“å…¥å‚æ•° `padding`ã€`truncate`ã€`max_length` å¤„ç†å¥½ï¼ˆè½¬æ¢æˆç›¸åº”çš„æšä¸¾ç±»å‹ï¼Œç”¨æˆ·å¦‚æœä¸ä¼  max_lengthï¼Œè¿™ä¸€æ­¥ä¹Ÿä¼šå°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªç¡®ç¡®å®å®çš„æ•´æ•°ï¼‰ï¼Œç„¶åç»§ç»­è°ƒç”¨ `_batch_encode_plus` æˆ– `_encode_plus` (è¿™ä¸¤ä¸ªæ–¹æ³•åœ¨å­ç±» `PretrainedTokenizer` å’Œ `PretrainedTokenizerFast` ä¸­åˆ†åˆ«å®ç°) ï¼Œä»¥ slow ç‰ˆæœ¬çš„ä¸ºä¾‹ï¼Œå®ƒä»¬å®é™…åšçš„äº‹æƒ…å¯ä»¥å‚è€ƒå¦‚ä¸‹ç®€åŒ–ç‰ˆæœ¬çš„æºä»£ç å®ç°ï¼š

```python
# ç®€åŒ–ç‰ˆæœ¬(åªè€ƒè™‘textä¸ºstrç±»å‹, ä¸è€ƒè™‘List[str]ç±»å‹)
def _encode_plus(self, text, text_pair):
    # tokenizeæ–¹æ³•å†…éƒ¨ä¾æ¬¡è°ƒç”¨: ä¸€æ¬¡ prepare_for_tokenization å’Œå¤šæ¬¡ _tokenize å®Œæˆ
    # _tokenizeæ–¹æ³•å¿…é¡»åœ¨å…·ä½“çš„tokenizerä¸­å®ç°
    # tokenizeçš„å¤§ä½“é€»è¾‘ä¸º: æŠŠä¸å¯æ‹†åˆ†çš„tokenæŠ½å‡ºæ¥, å…¶ä½™çš„è°ƒç”¨_tokenizeæ¥å®Œæˆ
    # ä¾‹å¦‚: "æˆ‘åœ¨<extra_001>é©¬è·¯è¾¹" => ["æˆ‘åœ¨", "<extra_001>", "é©¬è·¯è¾¹"]
    # => [_tokenize("æˆ‘åœ¨"), 32001, _tokenize("é©¬è·¯è¾¹")] = [34, 567, 32001, 76, 98]
    first_tokens = self.tokenize(text, **kwargs)
    first_ids = self.convert_tokens_to_ids(first_tokens)
    second_tokens = self.tokenize(text_pair, **kwargs)
    second_ids = self.convert_tokens_to_ids(second_tokens)
    # prepare_for_model éœ€è¦åšåå¤„ç†: é¦–å°¾åŠ ç‰¹æ®Štoken, è·å–
    return self.prepare_for_model(first_ids, second_ids, **kwargs)  # è¿™ä¸ªæ–¹æ³•å®šä¹‰åœ¨çˆ¶ç±»æ–¹æ³•ä¸­, è§å‰é¢å…³äº__call__çš„å…¥å‚è§£é‡Šéƒ¨åˆ†

# ç®€åŒ–ç‰ˆæœ¬(åªè€ƒè™‘batch_text_or_text_pairsä¸ºList[tuple[str, str]]çš„æƒ…å†µ)
def _batch_encode_plus(self, batch_text_or_text_pairs):
    input_ids = []
    for text_or_text_pair in batch_text_or_text_pairs:
        text, text_pair = text_or_text_pair
        first_tokens: List[str] = self.tokenize(text, **kwargs)
        first_ids: List[int] = self.convert_tokens_to_ids(first_tokens)
        second_tokens = self.tokenize(text_pair, **kwargs)
        second_ids = self.convert_tokens_to_ids(second_tokens)
        input_ids.append((first_ids, second_ids))
    return self._batch_prepare_for_model(input_ids, **kwargs)

def _batch_prepare_for_model(input_ids):
    batch_out = defaultdict(list)
    for first_ids, second_ids in input_ids:
        outputs: BatchEncoding = self.prepare_for_model(first_ids, second_ids)
        for key, value in outputs.items():
            batch_out[key].append(value)
    return BatchEncoding(self.pad(batch_out))
```

ä¸Šé¢çš„æºç ä¸­, æ¶‰åŠåˆ° `PretrainedTokenizer` çš„ `tokenize`ã€`convert_tokens_to_ids`ã€`prepare_for_model` æ–¹æ³•ï¼Œæ­¤å¤„å†åšä¸€äº›å±•å¼€è¯´æ˜ï¼š
```python

```


### decodeã€TODOã€‘


### è®­ç»ƒä¸€ä¸ª Tokenizer

åœ¨ ğŸ¤— Transformers åº“ä¸­, fast ç‰ˆæœ¬çš„ tokenizer å®é™…ä¸Šåˆ©ç”¨äº† ğŸ¤— Tokenizers çš„ä¸€äº›å†…å®¹, å› æ­¤ `PretrainedTokenizerFast` æ˜¯å¯ä»¥è®­ç»ƒçš„ï¼Œè€Œ slow ç‰ˆæœ¬çš„ tokenizer ä¸æ”¯æŒè®­ç»ƒã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```python
from transformers import AutoTokenizer
old_tokenizer = AutoTokenizer.from_pretrained("t5-small")
training_corpus: List[str] = ["sentence one", "sentence one"]
training_corpus = ([training_corpus[i*32: (i+1)*32]] for i in range(100))  # è¿­ä»£å™¨å³å¯
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
tokenizer.save_pretrained("save-dir")
```

æ³¨æ„ï¼šè¿™ç§åšæ³•é€‚ç”¨äºä¸ç°æœ‰çš„ä¸€ä¸ª tokenizer ä¸€è‡´çš„è®¾å®š, ä¾‹å¦‚ï¼šBOS tokenç­‰, ç»å¤§å¤šæ•°æƒ…å†µä¸‹, å·²ç»è¶³å¤Ÿä½¿ç”¨ã€‚å¦‚æœç¡®å®éœ€è¦åšæ¯”è¾ƒå¤§çš„è°ƒæ•´ï¼Œåˆ™éœ€è¦å€ŸåŠ© ğŸ¤— Tokenizers åŒ…ï¼ˆè§å‰æ–‡ä»‹ç»ï¼‰ã€‚

### è‡ªå®šä¹‰ specific slow tokenizerã€TODOã€‘

æœ¬èŠ‚ä»¥ `T5Tokenizer` ä¸ºä¾‹, ä»‹ç»å¦‚ä½•å†™ä¸€ä¸ª slow tokenizer

### è‡ªå®šä¹‰ specific fast tokenizerã€TODOã€‘

æœ¬èŠ‚ä»¥ `T5TokenizerFast` ä¸ºä¾‹, ä»‹ç»å¦‚ä½•å†™ä¸€ä¸ª fast tokenizer

### Converterã€TODOã€‘

### æ‚é¡¹ã€TODO: éœ€è°ƒæ•´ã€‘

fast ç‰ˆæœ¬çš„ tokenizer å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼æŸ¥çœ‹å…¶èƒŒåçš„ tokenizer ç±»å‹ï¼š

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("chinese-roberta-wwm-ext", use_fast=False)
json.loads(tokenizer._tokenizer.to_str())["model"]["type"]
```
