---
layout: post
title: "(P1) RL-101"
date: 2025-03-05 08:00:04 +0800
labels: [rl]
---

采用如下记号: $t=0,1,...$, 当前状态为 $s_t$, 采取某个动作 $a_t$ 后, 跳转到状态 $s_{t+1}$, 获得的奖励为 $r_{t+1}$.

严格地说, 环境由四元组所描述: $(\mathcal{S},\mathcal{A},P,R)$, 其中 $\mathcal{S}$ 是所有可能地状态, $\mathcal{A}$ 是所有可能地动作集合, $P$ 是转移概率: $P(s_{t+1}|s_{t},a_{t})$, $R$ 是奖励函数: $R(s_{t},a_{t},s_{t+1})$.


## 散点记录(待整合)

**马尔可夫**

马尔可夫过程(MP): 系统自带状态转移规则

马尔可夫奖励过程(MRP): 系统自带状态转移规则, 并且自带状态转移的即时奖励和折扣因子

马尔可夫决策过程(MDP): 动作由智能体决定, 系统提供状态转移规则(根据当前状态和动作)和奖励函数(奖励函数的完整格式是基于当前状态,动作,以及转移后的动作), 奖励折扣因子

**贝尔曼方程**

对于特定的随机策略 $\pi$, 也就是它由 $P_{\pi}=\pi(a|s)$ 表示基于当前状态, 采取下一个动作 $a$ 的概率来刻画. 价值函数有如下关系:

$$
V_{\pi}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)[R(s, a)+\gamma \sum_{s'\in\mathcal{S}}P(s'|s,a)V_{\pi}(s')]
$$

这里的 $R(s,a)$ 表示当步的期望奖励, 而 $P(s'|s,a)$ 是由环境决定的转移概率.

针对特定的确定性策略 $\pi$, 也就是它由函数 $a=\pi(s)$ 来刻画, 价值函数有如下关系:

$$
V_{\pi}(s)=R(s,a)+\gamma \sum_{s'\in\mathcal{S}}P(s'|s,a)V_{\pi}(s')
$$

同样这里的 $R(s,a)$ 表示当步的期望奖励, 而 $P(s'|s,a)$ 是由环境决定的转移概率.

**值函数**

基于值函数的策略改进方式: 给定当前策略 $\pi$, 执行如下步骤来更新这个策略, 首先利用当前策略与环境进行交互估计出 状态-动作价值函数 $Q_{\pi}(s,a)$. 动作价值函数表示的是在状态 $s$ 下, 强制此步骤采用动作 $a$, 接下来再采用策略 $\pi$, 所得到的累计回报.

之后采用 $\pi'(s) = \argmax_{a\in\mathcal{A}}{Q(s, a)}$ 来得到更新后的策略. 有定理能保证这么做得到的更新后的策略好于之前的策略. 这也就是 Q-learning 算法.


对于随机性策略:

状态-动作价值函数 与 状态价值函数之间可以互相推算(但都需要有一些额外信息):

$$
V_{\pi}(s)=\sum_{a\in\mathcal{A}}{\pi(a|s)}Q_{\pi}(s,a)
$$

推算前提是知道当前策略, 这一般是已知的. 但这里涉及到一个求和, 可能不一定是可计算的.

$$
Q_{\pi}(s, a)=R(s, a)+\gamma\sum_{s'\in\mathcal{S}}{P(s'|s, a)V_{\pi}(s')}
$$

推算前提是知道奖励函数 $R(s,a)$ 以及转移概率 $P(s'|s,a)$. 这些信息是由环境决定的, 不一定可以获取.

**贝尔曼方程**可由上面的式子进一步推导出来:

$$
V_{\pi}(s)=\sum_{a\in\mathcal{A}}{\pi(a|s)}{[R(s, a)+\gamma\sum_{s'\in\mathcal{S}}{P(s'|s, a)V_{\pi}(s')}]}
$$

以及

$$
Q_{\pi}(s,a)=R(s, a)+\gamma\sum_{s'\in\mathcal{S}}{P(s'|s, a)[\sum_{a'\in\mathcal{A}}{\pi(a'|s')}Q_{\pi}(s',a')]}
$$

注意, 这个两个递推都需要知道环境信息: 奖励函数 $R(s,a)$ 以及转移概率 $P(s'|s,a)$. 以及策略本身的信息 $\pi(a|s)$

**actor-critic 方法**
