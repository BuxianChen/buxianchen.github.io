---
layout: post
title: "(LTS) TODO LIST"
date: 2023-11-18 11:10:04 +0800
---

## 动机、参考资料、涉及内容

计划,资源,随笔

## 计划

未归类

- Rust 入门 + text-generation-inference/huggingface Tokenizer 库
- 《网络是怎样连接的》
- lmdeploy 的组装 batch 的一些细节
- 自动微分 + 陈天奇 dlsyscourse 课程
- Tensor Parallel + Pipeline Parallel (lmdeploy 等框架)
- qwen-agent
- React + OpenDevin(目前好像改名为 OpenHands 了)
- 强化学习相关, PPO, DPO, ... (huggingface course)
- Stable Diffusion (huggingface diffusers 以及课程)
- GGML + GGUF + llama.cpp
- 预测性解码 (transformers 和 vllm)
- 结构化解码: outlines
- guidance, Dify, SGLang
- SSE(stream/socket/FastAPI/js)
- FastAPI(backgroud task)

Python 相关

- CPython Internel (微信公众号 + RealPython 书)
- Python asyncio
- Python ast, cst 模块 (Modular Transformers, Pydantic V1 -> V2)

huggingface 相关

- ChatTemplate: 模板字符串使用 Jinja2 语法
- Modular Transformers

pytorch 相关
- torch.fx + torchscript + torch.compile
- torch 原生支持的量化 + QAT + AWQ
- torchao (pytorch的新仓库, 不确定和 torch.ao 的区别): 看起来 C 代码并不算太多, 可以学一学, 似乎包括了 GPTQ 等实现
- DDP 与 FSDP, Pytorch 2.4 引入 FSDP2
- torch.compile (depyf)

博客

- 重组如下内容显得重复的 web 相关内容
  - `2021-12-15-html-css-javascript-tutorial.md`: 拆掉, 往 notes 仓库里放
  - `2023-08-15-streamlit-tutorial.md`: 调整内容结构
  - `2023-08-31-flask-tutorial.md`: 调整内容结构
  - `2024-01-18-socket-http.md`: 拆掉, 改为 requests 库的介绍
  - `2024-02-25-wsgi-asgi.md`: 保留, Python 的 WSGI 与 ASGI
  - `2024-06-03-event.md`: 计划删除, 但还不确定放哪
  - `2024-06-03-fastapi.md`: 保留, 内容完善
  - `2024-08-28-socket.md`: 计算机网络中 socket 层相关的概念, python socket 包的 API, 以及利用 socket 来实现各种高级任务

## 资源

**Pytorch 与分布式训练相关**

- Pytorch tutorial: [https://pytorch.org/tutorials](https://pytorch.org/tutorials)
- 博客园(罗西的思考), 包含了一些关于分布式机器学习的博客(最大的优点是注明了原文的出处), 博主还出了本书: [https://www.cnblogs.com/rossiXYZ/](https://www.cnblogs.com/rossiXYZ/)
- [deepspeed](https://github.com/microsoft/DeepSpeed)
- [ColossalAI](https://github.com/hpcaitech/ColossalAI)

**大模型部署相关**

- [text-generation-inference](https://huggingface.co/docs/text-generation-inference/quicktour): huggingface 出品
- [vLLM](https://github.com/vllm-project/vllm)
- [deepspeed](https://www.deepspeed.ai/tutorials/inference-tutorial/): Microsoft 出品
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII): Microsoft 出品, 也许是目前最快的?
- [lmdeploy](https://github.com/InternLM/lmdeploy): mmlab 出品
- [FastTransformer](https://github.com/NVIDIA/FasterTransformer): Nvidia 出品
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM): Nvidia 出品, 似乎是 fastertransformer 的替代, 也许是目前最快的?
- [triton-inference-server](https://developer.nvidia.com/triton-inference-server): Nvidia 出品

**量化**

`W8A8` 指模型权重和激活值都量化到 8 bit int; `W4A16` 指模型权重量化到 4 bit int, 激活值保持为 FP 16

- [GPTQ](https://github.com/IST-DASLab/gptq), [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
- [exllama](https://github.com/turboderp/exllama): GPTQ 量化后模型推理的算子优化
- [AWQ](https://github.com/mit-han-lab/llm-awq), [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)

pytorch 原生量化

pytorch 中的[量化公式](https://pytorch.org/docs/stable/quantization-support.html#quantized-dtypes-and-quantization-schemes)

- blog (2020/3/26, pytorch 1.4): [https://pytorch.org/blog/introduction-to-quantization-on-pytorch/](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)
- docs:
  - [https://pytorch.org/docs/stable/quantization.html](https://pytorch.org/docs/stable/quantization.html)
  - [pytorch wiki](https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor)
- API: [https://pytorch.org/docs/stable/quantization-support.html](https://pytorch.org/docs/stable/quantization-support.html)
- tutorial:
  - [https://pytorch.org/tutorials/recipes/quantization.html](https://pytorch.org/tutorials/recipes/quantization.html)
  - [https://leimao.github.io/article/Neural-Networks-Quantization/](https://leimao.github.io/article/Neural-Networks-Quantization/)
  - [https://leimao.github.io/blog/PyTorch-Dynamic-Quantization/](https://leimao.github.io/blog/PyTorch-Dynamic-Quantization/)

**rapidapi**

[rapidapi](https://docs.rapidapi.com/docs/what-is-rapidapi): [ToolLLM paper](https://github.com/OpenBMB/ToolBench#data)


**pytorch compiler 相关**

- [torch.jit 文档](https://pytorch.org/docs/1.9.0/jit.html)
- [torch.fx 论文](https://arxiv.org/pdf/2112.08429.pdf)
- [Pytorch LazyTensor 论文](https://arxiv.org/pdf/2102.13267.pdf)
- [博客: Pytorch 的 2000+ 算子](https://dev-discuss.pytorch.org/t/where-do-the-2000-pytorch-operators-come-from-more-than-you-wanted-to-know/373/9)

**新闻**

- (2024/01/25) OpenAI 模型更新: [https://openai.com/blog/new-embedding-models-and-api-updates](https://openai.com/blog/new-embedding-models-and-api-updates): 文本嵌入模型 `text-embedding-3-large` 和 `text-embedding-3-small`, gpt 系列: `gpt-3.5-turbo-0125` 和 `gpt-4-0125-preview`, 合规检测模型(一个多分类模型, 免费使用): `text-moderation-007`
- (2024/02/13) OpenAI ChatGPT 聊天界面增加记忆管理等新功能: [https://openai.com/blog/memory-and-new-controls-for-chatgpt](https://openai.com/blog/memory-and-new-controls-for-chatgpt)
- (2024/02/09) Gemini Ultra: [https://deepmind.google/technologies/gemini/#gemini-1.0](https://deepmind.google/technologies/gemini/#gemini-1.0)
- (2024/02/14) Gemini 1.5: [https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)
- (2024/02/16) OpenAI 文生视频: [https://openai.com/sora](https://openai.com/sora)

**LLM 工具**

- ChatGPT: [https://chat.openai.com/chat](https://chat.openai.com/chat)
- Claude: [https://claude.ai/chat](https://claude.ai/chat)
- Mistral (Le Chat): [https://chat.mistral.ai/chat](https://chat.mistral.ai/chat)
- Perplexity (可联网): [https://www.perplexity.ai](https://www.perplexity.ai)
- Popai (基于 gpt-3.5/gpt-4, dalle 的文献阅读应用): [https://www.popai.pro/](https://www.popai.pro/)

**博客**

- Let's build a [compiler | web server] from scratch: [https://ruslanspivak.com/](https://ruslanspivak.com/)

**未归类**

- Langchain-Chatchat: [https://github.com/chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat), 主要可以关注前端写法, 中文文本切分, 以及对 Langchain 的使用, asyncio 的用法等
- 一门深度学习系统课程: [https://github.com/chenzomi12/DeepLearningSystem](https://github.com/chenzomi12/DeepLearningSystem), B站/youtube 上还有视频课程


## 随笔

---
链接:

- Release 信息: [https://github.com/huggingface/transformers/releases/tag/v4.45.0](https://github.com/huggingface/transformers/releases/tag/v4.45.0)
- 官方文档(附带有一个 Roberta 继承 Bert 的例子): [https://huggingface.co/docs/transformers/en/modular_transformers](https://huggingface.co/docs/transformers/en/modular_transformers)

2024/09/25 的 transformers 4.45.0 版本 Release 信息中提到, transformers 库引入了一种新的写法: `modular_xxx.py`, 用来替换以前的 `modeling_xxx.py`, `configuration_xxx.py` 的写法. 简要的工作方式如下: 

在之前, 由于 transformers 代码库采用了 Repeat Yourself 的设计哲学, 避免了太深的继承关系(例如所有的 `XXXModel` 都不会有继承关系, 而是有统一的基类), 因此各个模型的代码会有大量的重复, 为了让读代码的人注意到这些复制代码的存在, 会使用 `# Copied from ...` 这种注释来表明引用关系, 并且 CI 时会使用一些工具例如 `utils/check_copies.py` 来确保这些复制的代码没有发生篡改, 以免误导读者. 但有个弊端是, 读者需要看太多的文件: `modeling_xxx.py`, `configuration_xxx.py`.

目前引入的解决方案是: 增加一个 `modular_xxx.py` 文件, 允许它使用继承, 然后使用自动化转化工具 `utils/modular_model_converter.py` 将它转化为: `modeling_xxx.py`, `configuration_xxx.py` 这种文件, 这么一来, 读者就可以有两个选项, 一是可以照旧看 `modeling_xxx.py`, `configuration_xxx.py` 这些文件, 好处是它们没有复杂的继承关系, 没有依赖; 二是只看 `modular_xxx.py` 文件, 好处是代码量少, 尤其是如果读者对它继承的模型相对熟悉时, 方便阅读, 并且只需要看这一个文件.

样例: `models/gemma`

```bash
# 使用 modular_gemma.py 生成 modeling_gemma.py 和 configuration_gemma.py 文件
python utils/modular_model_converter.py --files_to_parse /path/to/transformers/src/transformers/models/gemma/modular_gemma.py
```

原理: python 文件的转换使用了 `libcst` 库

---